{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Fiddler Evals SDK - Custom Judge Evaluators\n",
    "\n",
    "This quickstart shows how to create custom LLM-as-a-Judge evaluators using `CustomJudge`.\n",
    "\n",
    "The `CustomJudge` evaluator lets you define arbitrary evaluation criteria by specifying a **prompt template** with `{{ placeholder }}` syntax and **output fields** that define the structured response.\n",
    "\n",
    "**Prerequisites:**\n",
    "- A Fiddler account with API access\n",
    "- An LLM credential configured in **Settings > LLM Gateway**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q fiddler-evals pandas\n",
    "\n",
    "import pandas as pd\n",
    "from fiddler_evals import init\n",
    "from fiddler_evals.evaluators import CustomJudge\n",
    "\n",
    "URL = ''  # e.g., 'https://your-org.fiddler.ai'\n",
    "TOKEN = ''  # From Settings > Credentials\n",
    "LLM_CREDENTIAL_NAME = ''  # From Settings > LLM Gateway\n",
    "LLM_MODEL_NAME = ''\n",
    "\n",
    "init(url=URL, token=TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-header",
   "metadata": {},
   "source": [
    "## 2. Test Cases\n",
    "\n",
    "We'll classify news summaries into topics: **Sci/Tech**, **Sports**, **Business**, or **World**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            'text': 'Google announces new AI chip designed to accelerate machine learning workloads.',\n",
    "            'ground_truth': 'Sci/Tech',\n",
    "        },\n",
    "        {\n",
    "            'text': 'The Lakers defeated the Celtics 112-108 in overtime, with LeBron James scoring 35 points.',\n",
    "            'ground_truth': 'Sports',\n",
    "        },\n",
    "        {\n",
    "            'text': 'Federal Reserve raises interest rates by 0.25% citing persistent inflation concerns.',\n",
    "            'ground_truth': 'Business',\n",
    "        },\n",
    "        {\n",
    "            'text': 'United Nations Security Council votes to impose new sanctions on North Korea.',\n",
    "            'ground_truth': 'World',\n",
    "        },\n",
    "        {\n",
    "            'text': 'Microsoft acquires gaming company Activision Blizzard for $69 billion.',\n",
    "            'ground_truth': 'Sci/Tech',\n",
    "        },\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judge-header",
   "metadata": {},
   "source": [
    "## 3. Create a CustomJudge\n",
    "\n",
    "Define a `prompt_template` with `{{ placeholder }}` markers and `output_fields` with expected types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judge",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_judge = CustomJudge(\n",
    "    model=LLM_MODEL_NAME,\n",
    "    credential=LLM_CREDENTIAL_NAME,\n",
    "    prompt_template=\"\"\"\n",
    "        Determine the topic of the given news summary. Pick one of: Sports, World, Sci/Tech, Business.\n",
    "\n",
    "        News Summary:\n",
    "        {{ news_summary }}\n",
    "    \"\"\",\n",
    "    output_fields={\n",
    "        'topic': {'type': 'string'},\n",
    "        'reasoning': {'type': 'string'},\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-header",
   "metadata": {},
   "source": [
    "## 4. Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for _, row in df.iterrows():\n",
    "    scores = simple_judge.score(inputs={'news_summary': row['text']})\n",
    "    scores_dict = {s.name: s for s in scores}\n",
    "    results.append(\n",
    "        {\n",
    "            'ground_truth': row['ground_truth'],\n",
    "            'predicted': scores_dict['topic'].label,\n",
    "            'reasoning': scores_dict['reasoning'].label,\n",
    "        }\n",
    "    )\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "accuracy = (results_df['ground_truth'] == results_df['predicted']).mean()\n",
    "print(f'Accuracy: {accuracy:.0%}')\n",
    "\n",
    "# Show misclassified\n",
    "misclassified = results_df[results_df['ground_truth'] != results_df['predicted']]\n",
    "if len(misclassified) > 0:\n",
    "    print(f'\\nMisclassified ({len(misclassified)}):')\n",
    "    for _, row in misclassified.iterrows():\n",
    "        print(f'  Expected: {row[\"ground_truth\"]}, Predicted: {row[\"predicted\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improve-header",
   "metadata": {},
   "source": [
    "## 5. Improve the Prompt\n",
    "\n",
    "The simple prompt may confuse tech company financial news with Business. Let's add clearer topic guidelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-judge",
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_judge = CustomJudge(\n",
    "    model=LLM_MODEL_NAME,\n",
    "    credential=LLM_CREDENTIAL_NAME,\n",
    "    prompt_template=\"\"\"\n",
    "        Determine the topic of the given news summary.\n",
    "\n",
    "        Use topic 'Sci/Tech' if the news summary is about a company or business in the tech industry, or if the news summary is about a scientific discovery or research, including health and medicine.\n",
    "        Use topic 'Sports' if the news summary is about a sports event or athlete.\n",
    "        Use topic 'Business' if the news summary is about a company or industry outside of science, technology, or sports.\n",
    "        Use topic 'World' if the news summary is about a global event or issue.\n",
    "\n",
    "        News Summary:\n",
    "        {{ news_summary }}\n",
    "    \"\"\",\n",
    "    output_fields={\n",
    "        'topic': {\n",
    "            'type': 'string',\n",
    "            'choices': [\n",
    "                'Sci/Tech',\n",
    "                'Sports',\n",
    "                'Business',\n",
    "                'World',\n",
    "            ],  # this restricts the LLM output\n",
    "        },\n",
    "        'reasoning': {'type': 'string'},\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run with improved judge\n",
    "improved_results = []\n",
    "for _, row in df.iterrows():\n",
    "    scores = improved_judge.score(inputs={'news_summary': row['text']})\n",
    "    scores_dict = {s.name: s for s in scores}\n",
    "    improved_results.append(\n",
    "        {\n",
    "            'ground_truth': row['ground_truth'],\n",
    "            'predicted': scores_dict['topic'].label,\n",
    "        }\n",
    "    )\n",
    "\n",
    "improved_df = pd.DataFrame(improved_results)\n",
    "original_accuracy = (results_df['ground_truth'] == results_df['predicted']).mean()\n",
    "improved_accuracy = (improved_df['ground_truth'] == improved_df['predicted']).mean()\n",
    "\n",
    "print(f'Simple prompt:   {original_accuracy:.0%}')\n",
    "print(f'Improved prompt: {improved_accuracy:.0%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- **Iterate on prompts** to improve accuracy\n",
    "- **Combine with built-in evaluators** like `Faithfulness`, `AnswerRelevance`\n",
    "- **Run experiments** with [Part 3: Datasets & Experiments](./Fiddler_Quickstart_Evals_Pt3_Datasets_Experiments.ipynb)\n",
    "\n",
    "**Resources:**\n",
    "- [Part 1: Fiddler Evaluators](./Fiddler_Quickstart_Evals_Pt1_Fiddler_Evaluators.ipynb)\n",
    "- [Fiddler Evals Documentation](https://docs.fiddler.ai/evaluations/overview)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
