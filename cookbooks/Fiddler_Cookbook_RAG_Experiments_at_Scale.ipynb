{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Fiddler Evals SDK - Datasets and Experiments\n",
    "\n",
    "This quickstart shows you how to create a Dataset, run structured RAG evaluation Experiments, and track results in Fiddler.\n",
    "\n",
    "**Prerequisites:**\n",
    "- A Fiddler account with API access\n",
    "- An LLM credential configured in **Settings > LLM Gateway**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Connect to Fiddler and create resources. Experiments are organized as: **Project > Application > Dataset > Experiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q fiddler-evals pandas\n",
    "\n",
    "import pandas as pd\n",
    "from fiddler_evals import Application, Dataset, Project, evaluate, init\n",
    "from fiddler_evals.evaluators import AnswerRelevance, ContextRelevance, RAGFaithfulness\n",
    "from fiddler_evals.pydantic_models.experiment import ExperimentItemResult\n",
    "from fiddler_evals.pydantic_models.score import Score\n",
    "\n",
    "URL = ''  # e.g., 'https://your-org.fiddler.ai'\n",
    "TOKEN = ''  # From Settings > Credentials\n",
    "LLM_CREDENTIAL_NAME = ''  # From Settings > LLM Gateway\n",
    "LLM_MODEL_NAME = ''\n",
    "\n",
    "init(url=URL, token=TOKEN)\n",
    "\n",
    "project = Project.get_or_create(name='quickstart')\n",
    "application = Application.get_or_create(name='rag-evaluation', project_id=project.id)\n",
    "dataset = Dataset.get_or_create(name='rag-test-cases', application_id=application.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-header",
   "metadata": {},
   "source": [
    "## 2. Test Cases\n",
    "\n",
    "Create 4 test cases with `expected_quality` labels (good/bad) to validate evaluators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_data = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            'scenario': '✅ Perfect Match',\n",
    "            'expected_quality': 'good',\n",
    "            'user_query': 'What is the capital of France?',\n",
    "            'retrieved_documents': [\n",
    "                'Paris is the capital and largest city of France.',\n",
    "                'France is located in Western Europe.',\n",
    "            ],\n",
    "            'rag_response': 'The capital of France is Paris.',\n",
    "        },\n",
    "        {\n",
    "            'scenario': '❌ Irrelevant Context',\n",
    "            'expected_quality': 'bad',\n",
    "            'user_query': 'How do I reset my password?',\n",
    "            'retrieved_documents': [\n",
    "                'To make pasta, boil water and add salt.',\n",
    "                'Italian cuisine features many pasta dishes.',\n",
    "            ],\n",
    "            'rag_response': 'To reset your password, go to the login page and click Forgot Password.',\n",
    "        },\n",
    "        {\n",
    "            'scenario': '❌ Hallucination',\n",
    "            'expected_quality': 'bad',\n",
    "            'user_query': 'What are the business hours?',\n",
    "            'retrieved_documents': [\n",
    "                'Our office is located at 123 Main Street.',\n",
    "                'We are closed on federal holidays.',\n",
    "            ],\n",
    "            'rag_response': 'Our business hours are Monday through Friday, 9 AM to 5 PM.',\n",
    "        },\n",
    "        {\n",
    "            'scenario': '❌ Irrelevant Answer',\n",
    "            'expected_quality': 'bad',\n",
    "            'user_query': 'What is your return policy?',\n",
    "            'retrieved_documents': [\n",
    "                'Returns are accepted within 30 days of purchase.',\n",
    "                'Items must be unused and in original packaging.',\n",
    "            ],\n",
    "            'rag_response': 'We offer free shipping on orders over $50. Delivery takes 3-5 business days.',\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "rag_data[['scenario', 'expected_quality', 'user_query', 'rag_response']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert data (skip if already exists)\n",
    "if not list(dataset.get_items()):\n",
    "    dataset.insert_from_pandas(\n",
    "        df=rag_data,\n",
    "        input_columns=['user_query', 'retrieved_documents', 'rag_response'],\n",
    "        expected_output_columns=['expected_quality'],\n",
    "        metadata_columns=['scenario'],\n",
    "    )\n",
    "    print(f'Inserted {len(rag_data)} test cases')\n",
    "else:\n",
    "    print('Dataset already has items, skipping insert')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experiment-header",
   "metadata": {},
   "source": [
    "## 3. Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_task(inputs: dict, extras: dict, metadata: dict) -> dict:\n",
    "    \"\"\"Return pre-recorded RAG response. Replace with your RAG pipeline in production.\"\"\"\n",
    "    return {'rag_response': inputs['rag_response']}\n",
    "\n",
    "\n",
    "evaluators = [\n",
    "    ContextRelevance(model=LLM_MODEL_NAME, credential=LLM_CREDENTIAL_NAME),\n",
    "    RAGFaithfulness(model=LLM_MODEL_NAME, credential=LLM_CREDENTIAL_NAME),\n",
    "    AnswerRelevance(model=LLM_MODEL_NAME, credential=LLM_CREDENTIAL_NAME),\n",
    "]\n",
    "\n",
    "result = evaluate(\n",
    "    dataset=dataset,\n",
    "    task=rag_task,\n",
    "    evaluators=evaluators,\n",
    "    score_fn_kwargs_mapping={\n",
    "        'user_query': lambda x: x['inputs']['user_query'],\n",
    "        'retrieved_documents': lambda x: x['inputs']['retrieved_documents'],\n",
    "        'rag_response': 'rag_response',\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f'Experiment: {result.experiment.name}')\n",
    "print(f'Evaluated {len(result.results)} test cases')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validate-header",
   "metadata": {},
   "source": [
    "## 4. Validate Against Golden Labels\n",
    "\n",
    "Check if the evaluators correctly identified quality issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validate",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_results = []\n",
    "correct = 0\n",
    "\n",
    "for r in result.results:\n",
    "    expected = r.dataset_item.expected_outputs.get('expected_quality')\n",
    "    has_problem = any(s.value < 0.5 for s in r.scores)\n",
    "    predicted = 'bad' if has_problem else 'good'\n",
    "\n",
    "    if expected == predicted:\n",
    "        correct += 1\n",
    "\n",
    "    validation_results.append(\n",
    "        ExperimentItemResult(\n",
    "            experiment_item=r.experiment_item,\n",
    "            dataset_item=r.dataset_item,\n",
    "            scores=[\n",
    "                Score(\n",
    "                    name='predicted_quality',\n",
    "                    evaluator_name='OverallQuality',\n",
    "                    value=1.0 if predicted == 'good' else 0.0,\n",
    "                    label=predicted,\n",
    "                    reasoning=f'Expected: {expected}',\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "    )\n",
    "\n",
    "result.experiment.add_results(validation_results)\n",
    "print(\n",
    "    f'Evaluator Accuracy: {correct}/{len(result.results)} ({100 * correct / len(result.results):.0f}%)'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-header",
   "metadata": {},
   "source": [
    "## View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'View in Fiddler: {URL}/evals/experiments/{result.experiment.id}')\n",
    "\n",
    "# Build results DataFrame\n",
    "rows = []\n",
    "for r, v in zip(result.results, validation_results):\n",
    "    row = {\n",
    "        'scenario': r.dataset_item.metadata.get('scenario'),\n",
    "        'expected': r.dataset_item.expected_outputs.get('expected_quality'),\n",
    "        'predicted': v.scores[0].label,\n",
    "    }\n",
    "    row.update({s.evaluator_name: s.value for s in r.scores})\n",
    "    rows.append(row)\n",
    "\n",
    "pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- Replace sample data with your golden test cases\n",
    "- Run multiple experiments testing different CustomJudge variations\n",
    "\n",
    "**Resources:**\n",
    "- [Part 1: Fiddler Evaluators](./Fiddler_Quickstart_Evals_Pt1_Fiddler_Evaluators.ipynb)\n",
    "- [Part 2: Custom Judge](./Fiddler_Quickstart_Evals_Pt2_CustomJudge.ipynb)\n",
    "- [Fiddler Evals Documentation](https://docs.fiddler.ai/evaluations/overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d87e75",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
