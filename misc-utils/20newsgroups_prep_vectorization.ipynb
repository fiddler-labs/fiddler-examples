{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f0c3549a",
      "metadata": {
        "id": "f0c3549a"
      },
      "source": [
        "# 20Newsgroups Pre-processing and Vectorization\n",
        "This notebook generates some useful assets that can be used by other examples and notebooks in Fiddler for demonstration and debugging of NLP use cases. In particular, we use the public 20Newsgroups dataset and group the original targets into more general news categories. We combine the raw text data and the original and the new targets in a pandas DataFrame and store it as a CSV file. Furthremore, we vectorize this dataset using two text embedding methods (TF-IDF and OpenAI embedding) and store the resulting embeddings vectors. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "F3ZScdVsgYzo",
      "metadata": {
        "id": "F3ZScdVsgYzo"
      },
      "source": [
        "## Fetch the 20 Newsgroup Dataset and Group the Labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yZh-dG6kdMoP",
      "metadata": {
        "id": "yZh-dG6kdMoP"
      },
      "source": [
        "First, we retrieve the 20Newsgroups dataset, which is available as part of the scikit-learn real-world dataset. This dataset contains around 18,000 newsgroup posts on 20 topics. The original dataset is available [here](http://qwone.com/~jason/20Newsgroups/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zCqhKtjrXFU1",
      "metadata": {
        "id": "zCqhKtjrXFU1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zEY0gOzyhrvY",
      "metadata": {
        "id": "zEY0gOzyhrvY"
      },
      "outputs": [],
      "source": [
        "data_bunch = fetch_20newsgroups(\n",
        "    subset = 'train',\n",
        "    shuffle=True,\n",
        "    random_state=1,\n",
        "    remove=('headers','footers','quotes')\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hnphKST_e617",
      "metadata": {
        "id": "hnphKST_e617"
      },
      "source": [
        "A target name from 20 topics is assigned to each data sample in the above dataset, and you can access all the target names by running the: \n",
        "```\n",
        "data_bunch.target_names\n",
        "```\n",
        "However, to make this example notebook simpler, we group similar topics and define more general targets as the following:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "M83tz1P5W8Fp",
      "metadata": {
        "id": "M83tz1P5W8Fp"
      },
      "outputs": [],
      "source": [
        "subcategories = {\n",
        "    \n",
        "    'computer': ['comp.graphics',\n",
        "                 'comp.os.ms-windows.misc',\n",
        "                 'comp.sys.ibm.pc.hardware',\n",
        "                 'comp.sys.mac.hardware',\n",
        "                 'comp.windows.x'],\n",
        "    \n",
        "    'politics': ['talk.politics.guns',\n",
        "                 'talk.politics.mideast',\n",
        "                 'talk.politics.misc'],\n",
        "    \n",
        "    'recreation':['rec.autos',\n",
        "                  'rec.motorcycles',\n",
        "                  'rec.sport.baseball',\n",
        "                  'rec.sport.hockey'],\n",
        "    \n",
        "    'science': ['sci.crypt',\n",
        "                'sci.electronics',\n",
        "                'sci.med',\n",
        "                'sci.space',],\n",
        "    \n",
        "    'religion': ['soc.religion.christian',\n",
        "                 'talk.religion.misc',\n",
        "                 'alt.atheism'],\n",
        "    \n",
        "    'forsale':['misc.forsale']\n",
        "}\n",
        "\n",
        "main_category = {}\n",
        "for key,l in subcategories.items():\n",
        "    for item in l:\n",
        "        main_category[item] = key"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TA9_uykyggmt",
      "metadata": {
        "id": "TA9_uykyggmt"
      },
      "source": [
        "Next we build a DataFrame in which both the original and the more general targets are stored toghether with the text docmunts and apply a few filters on the rows to make this dataset more usable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35f4d54b",
      "metadata": {
        "id": "35f4d54b"
      },
      "outputs": [],
      "source": [
        "MAX_TOKEN=4000\n",
        "MAX_LENGTH=8000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WcZh806fXKVK",
      "metadata": {
        "id": "WcZh806fXKVK",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "data_prep = [s.replace('\\n',' ').strip('\\n,=,|,-, ,\\,^') for s in data_bunch.data]\n",
        "data_series = pd.Series(data_prep)\n",
        "df = pd.DataFrame()\n",
        "df['original_text'] = data_series\n",
        "df['original_target'] = [data_bunch.target_names[t] for t in data_bunch.target]\n",
        "df['target'] = [main_category[data_bunch.target_names[t]] for t in data_bunch.target]\n",
        "df['original_text'].replace('', np.nan, inplace=True)\n",
        "df.dropna(axis=0, subset=['original_text'], inplace=True)\n",
        "df = df[df.target!='politics'] #delete political posts \n",
        "\n",
        "#more filters to pass OpenAI tokens limitation \n",
        "df['n_tokens'] = df['original_text'].apply(lambda s: len(s.split(' ')))\n",
        "df = df[df['n_tokens'] < MAX_TOKEN]\n",
        "df['string_size'] = df['original_text'].apply(lambda s: len(s))\n",
        "df = df[df['string_size'] < MAX_LENGTH]\n",
        "\n",
        "df.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4d5cd8d",
      "metadata": {
        "id": "e4d5cd8d"
      },
      "source": [
        "## OpenAI Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff0ef053",
      "metadata": {
        "id": "ff0ef053"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "MODEL = \"text-embedding-ada-002\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "557e3daf",
      "metadata": {
        "id": "557e3daf"
      },
      "source": [
        "### Batch query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d491d02",
      "metadata": {
        "id": "3d491d02"
      },
      "outputs": [],
      "source": [
        "def get_openai_embedding_batch(df, text_col_name, batch_size, model=MODEL):\n",
        "    if batch_size>2000:\n",
        "        raise ValueError('openai currently does not support chunks larger than 2000')\n",
        "    embeddings = []\n",
        "    for i in range(0, df.shape[0], batch_size):\n",
        "        batch_df = df.iloc[i:i+batch_size] if i+batch_size<df.shape[0] else df.iloc[i:]\n",
        "        response = openai.Embedding.create(\n",
        "            input=batch_df[text_col_name].tolist(),\n",
        "            model=model\n",
        "        )\n",
        "        response_embedding_list = [res['embedding'] for res in response['data']]\n",
        "        embeddings += response_embedding_list\n",
        "    embedding_col_names = ['openai_dim{}'.format(i+1) for i in range(len(embeddings[0]))]\n",
        "    return pd.DataFrame(embeddings, columns=embedding_col_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fba35a81",
      "metadata": {
        "id": "fba35a81"
      },
      "outputs": [],
      "source": [
        "batch_size = 2000\n",
        "text_col_name = 'original_text'\n",
        "openai_df = get_openai_embedding_batch(df, text_col_name, batch_size, model=\"text-embedding-ada-002\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e83a4d3",
      "metadata": {
        "id": "9e83a4d3"
      },
      "source": [
        "# TF-IDF Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4753a6d",
      "metadata": {
        "id": "a4753a6d"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "embedding_dimension = 300\n",
        "vectorizer = TfidfVectorizer(sublinear_tf=True,\n",
        "                             max_features=embedding_dimension,\n",
        "                             min_df=0.01,\n",
        "                             max_df=0.9,\n",
        "                             stop_words='english',\n",
        "                             token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b')\n",
        "\n",
        "tfidf_sparse = vectorizer.fit_transform(df['original_text'])\n",
        "embedding_cols = vectorizer.get_feature_names_out()\n",
        "embedding_col_names = ['tfidf_token_{}'.format(t) for t in embedding_cols]\n",
        "tfidf_df = pd.DataFrame.sparse.from_spmatrix(tfidf_sparse, columns=embedding_col_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef407c52",
      "metadata": {
        "id": "ef407c52"
      },
      "source": [
        "# Store Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efbfce5d",
      "metadata": {
        "id": "efbfce5d",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "df.to_csv('20newsgroups_preprocessed.csv',index=False)\n",
        "openai_df.to_csv('20newsgroups_openai_embeddings.csv',index=False)\n",
        "tfidf_df.to_csv('20newsgroups_tfidf_embeddings.csv',index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "dcc2ed42-2cd9-424c-bc04-f8fcd3c01a69",
        "F3ZScdVsgYzo",
        "JFzXvnYSYCcT",
        "RBVKIHG_YdrD",
        "sbUNWGfpZJrr",
        "roikR8aTmfNE",
        "30JODMh6mp1-"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "openai",
      "language": "python",
      "name": "openai"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}