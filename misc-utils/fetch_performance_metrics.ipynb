{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b26ba7f",
   "metadata": {},
   "source": [
    "# Fiddler Performance Metrics Fetcher\n",
    "\n",
    "This script fetches performance metrics for binary classification models from the Fiddler API.\n",
    "\n",
    "**Instructions:**\n",
    "1. Edit the configuration variables below with your API details\n",
    "2. Run all cells sequentially\n",
    "3. The script will generate `performance_metrics.csv` with your metrics\n",
    "\n",
    "**API Endpoint Used:** `/v3/analytics/metrics` with POST request\n",
    "**Required Parameters:** model_id, env_type, metrics (list of metric objects with 'id' field)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1668d8",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Edit these variables with your Fiddler API details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4a4a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - EDIT THESE VALUES\n",
    "API_URL = ''  # Your Fiddler base URL\n",
    "API_TOKEN = ''                   # Your API authentication token\n",
    "MODEL_ID = ''                     # Target model ID\n",
    "DATA_SOURCE = ''  # Data source: PRODUCTION, BASELINE, or VALIDATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d5ff20",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9ab8b7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from typing import Dict, Any, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd3157c",
   "metadata": {},
   "source": [
    "## Fetch Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c62628b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def fetch_performance_metrics(api_url: str, api_token: str, model_id: str, data_source: str = 'PRODUCTION') -> Optional[Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Fetch performance metrics from Fiddler API for binary classification model.\n",
    "    \n",
    "    Args:\n",
    "        api_url: Base URL for Fiddler API\n",
    "        api_token: API authentication token\n",
    "        model_id: Target model ID\n",
    "        data_source: Data source (PRODUCTION, BASELINE, or VALIDATION)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with performance metrics or None if failed\n",
    "    \"\"\"\n",
    "    \n",
    "    # Headers for authentication\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {api_token}',\n",
    "        'Content-Type': 'application/json',\n",
    "        'Accept': 'application/json'\n",
    "    }\n",
    "    \n",
    "    # API endpoint for analytics metrics\n",
    "    endpoint = f'{api_url.rstrip(\"/\")}/v3/analytics/metrics'\n",
    "    \n",
    "    # Payload for analytics metrics\n",
    "    payload = {\n",
    "        'model_id': model_id,\n",
    "        'env_type': data_source,  # Use env_type instead of data_source\n",
    "        'metrics': [\n",
    "            {'id': 'accuracy'},\n",
    "            {'id': 'precision'}, \n",
    "            {'id': 'recall'},\n",
    "            {'id': 'f1_score'},\n",
    "            {'id': 'auc'}\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(f\"Fetching metrics for model: {model_id}\")\n",
    "        print(f\"Data source: {data_source}\")\n",
    "        print(f\"API endpoint: {endpoint}\")\n",
    "        # Make API request - POST for analytics query\n",
    "        response = requests.post(endpoint, json=payload, headers=headers, timeout=30)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            print(\"‚úÖ Successfully fetched metrics from API\")\n",
    "            return data.get('data', data)  # Try 'data' key first, fallback to full response\n",
    "        else:\n",
    "            print(f\"‚ùå API request failed: {response.status_code}\")\n",
    "            print(f\"Response: {response.text}\")\n",
    "            return None\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚ùå Network error: {str(e)}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"‚ùå JSON decode error: {str(e)}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Unexpected error: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb27f5e9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def process_and_save_metrics(metrics_data: Any) -> bool:\n",
    "    \"\"\"\n",
    "    Process metrics data and save to CSV file.\n",
    "    \n",
    "    Args:\n",
    "        metrics_data: Raw metrics data from API (list of values)\n",
    "        \n",
    "    Returns:\n",
    "        True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    \n",
    "    # The API returns metrics in the order we requested them\n",
    "    metric_names = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC']\n",
    "    \n",
    "    # Handle both list format and dict format\n",
    "    if isinstance(metrics_data, list):\n",
    "        if len(metrics_data) != len(metric_names):\n",
    "            print(f\"‚ùå Expected {len(metric_names)} metrics, got {len(metrics_data)}\")\n",
    "            return False\n",
    "        required_metrics = dict(zip(metric_names, metrics_data))\n",
    "    else:\n",
    "        # Fallback for dict format\n",
    "        required_metrics = {\n",
    "            'Accuracy': metrics_data.get('accuracy'),\n",
    "            'Precision': metrics_data.get('precision'), \n",
    "            'Recall': metrics_data.get('recall'),\n",
    "            'F1 Score': metrics_data.get('f1_score'),\n",
    "            'AUC': metrics_data.get('auc')\n",
    "        }\n",
    "    \n",
    "    # Check which metrics are available\n",
    "    available_metrics = {}\n",
    "    missing_metrics = []\n",
    "    \n",
    "    for metric_name, value in required_metrics.items():\n",
    "        if value is not None:\n",
    "            available_metrics[metric_name] = value\n",
    "        else:\n",
    "            missing_metrics.append(metric_name)\n",
    "    \n",
    "    if missing_metrics:\n",
    "        print(f\"‚ö†Ô∏è  Warning: Missing metrics: {', '.join(missing_metrics)}\")\n",
    "    \n",
    "    if not available_metrics:\n",
    "        print(\"‚ùå No performance metrics found in API response\")\n",
    "        return False\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame([\n",
    "        {'Metric': metric, 'Value': value} \n",
    "        for metric, value in available_metrics.items()\n",
    "    ])\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_file = 'performance_metrics.csv'\n",
    "    try:\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"‚úÖ Performance metrics saved to {output_file}\")\n",
    "        print(f\"üìä Saved {len(available_metrics)} metrics:\")\n",
    "        for metric, value in available_metrics.items():\n",
    "            print(f\"   ‚Ä¢ {metric}: {value:.4f}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving CSV: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e65525",
   "metadata": {},
   "source": [
    "## Execute Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e58b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    \n",
    "    print(\"üöÄ Starting Fiddler Performance Metrics Fetcher\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Validate configuration\n",
    "    if (API_URL == 'https://your-fiddler-instance.com' or \n",
    "        API_TOKEN == 'your_api_token' or \n",
    "        MODEL_ID == 'your_model_id'):\n",
    "        print(\"‚ùå Please edit the configuration variables at the top of this script:\")\n",
    "        print(\"   ‚Ä¢ API_URL: Your Fiddler instance URL\")\n",
    "        print(\"   ‚Ä¢ API_TOKEN: Your API authentication token\") \n",
    "        print(\"   ‚Ä¢ MODEL_ID: Your target model ID\")\n",
    "        print(\"   ‚Ä¢ DATA_SOURCE: Environment type (PRODUCTION, BASELINE, or VALIDATION)\")\n",
    "        return\n",
    "    \n",
    "    # Fetch metrics from API\n",
    "    metrics_data = fetch_performance_metrics(API_URL, API_TOKEN, MODEL_ID, DATA_SOURCE)\n",
    "    \n",
    "    if metrics_data is None:\n",
    "        print(\"‚ùå Failed to fetch metrics from API\")\n",
    "        return\n",
    "    \n",
    "    # Process and save metrics\n",
    "    success = process_and_save_metrics(metrics_data)\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\nüéâ Script completed successfully!\")\n",
    "        print(\"üìÑ Check performance_metrics.csv for your results\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Script failed to complete\")\n",
    "\n",
    "# Execute main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
