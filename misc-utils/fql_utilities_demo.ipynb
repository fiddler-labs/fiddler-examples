{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FQL Utilities Demonstration\n",
    "\n",
    "This notebook demonstrates the FQL (Fiddler Query Language) utility functions available in the `fiddler_utils` package. These utilities help you parse, validate, transform, and analyze FQL expressions used in segments, custom metrics, and other Fiddler assets.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "* Access to a Fiddler environment with at least one model containing [segments](https://docs.fiddler.ai/product-guide/monitoring-platform/segments) or [custom metrics](https://docs.fiddler.ai/product-guide/monitoring-platform/custom-metrics)\n",
    "* [API toke](https://docs.fiddler.ai/configuration-guide/settings#credentials)n with [read access](https://docs.fiddler.ai/configuration-guide/access-control/role-based-access#understanding-permissions) (write access needed for Section 5 examples)\n",
    "* Python packages: [fiddler-client](https://docs.fiddler.ai/technical-reference/python-client-guides/installation-and-setup), `fiddler_utils`\n",
    "\n",
    "## What is FQL?\n",
    "\n",
    "[FQL is Fiddler's query language](https://docs.fiddler.ai/product-guide/monitoring-platform/fiddler-query-language) for defining:\n",
    "* **Segments** - Filter expressions to define data subsets\n",
    "* **Custom Metrics** - Calculated metrics using aggregation functions\n",
    "* **Alert Rules** - Conditions for triggering alerts\n",
    "\n",
    "### FQL Syntax Rules\n",
    "\n",
    "* **Column names:** Always in double quotes (e.g., `\"column_name\"`)\n",
    "* **String values:** Always in single quotes (e.g., `'value'`)\n",
    "* **Numeric values:** No quotes (e.g., `42`, `3.14`)\n",
    "* **Operators:** `==`, `!=`, `>`, `<`, `>=`, `<=`, `and`, `or`, `not`\n",
    "* **Functions:** `sum()`, `avg()`, `if()`, `fp()`, `fn()`, `tp()`, `tn()`, etc.\n",
    "\n",
    "### Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiddler_utils.connection import get_or_init\n",
    "import sys\n",
    "from typing import Set, Dict\n",
    "import fiddler as fdl\n",
    "from fiddler_utils import fql\n",
    "\n",
    "# Add parent directory to path to import fiddler_utils\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "print(\"âœ“ Imports successful\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fiddler environment configuration\n",
    "URL = \"\"  # Example: 'https://your_company_name.fiddler.ai'\n",
    "TOKEN = \"\"  # Your API token\n",
    "\n",
    "# Model to use for examples (we'll list available models if not specified)\n",
    "PROJECT_NAME = \"\"  # Example: 'my_project'\n",
    "MODEL_NAME = \"\"  # Example: 'my_model'\n",
    "MODEL_VERSION = \"\"  # Example: 'v1' (optional)\n",
    "\n",
    "# Set to False to actually execute modifications in Section 5\n",
    "DRY_RUN = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Standalone FQL Utilities\n",
    "\n",
    "These functions work with FQL expressions directly, without requiring a Fiddler connection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Extract Column References\n",
    "\n",
    "The `extract_columns()` function identifies all column names referenced in an FQL expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example FQL expressions\n",
    "examples = [\n",
    "    '\"age\" > 30 and \"geography\" == \\'California\\'',\n",
    "    'sum(if(fp(), 1, 0) * \"transaction_value\")',\n",
    "    '\"credit_score\" >= 700 and \"loan_amount\" < 50000 and \"region\" == \\'West\\'',\n",
    "    'avg(\"response_time\") > 100',\n",
    "    '(sum(if((\"probability_churn\">0.8 and \"gender\"== \\'Nonbinary\\'), 1, 0))/sum(if((\"gender\"== \\'Nonbinary\\'), 1, 0)))/(sum(if((\"probability_churn\">0.8 and \"gender\"== \\'Male\\'), 1, 0))/sum(if((\"gender\"== \\'Male\\'), 1, 0)))',\n",
    "]\n",
    "\n",
    "print(\"Column Extraction Examples:\\n\")\n",
    "for expr in examples:\n",
    "    columns = fql.extract_columns(expr)\n",
    "    print(f\"Expression: {expr}\")\n",
    "    print(f\"  Columns: {columns}\")\n",
    "    print(f\"  Count: {len(columns)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Validate FQL Syntax\n",
    "\n",
    "The `validate_fql_syntax()` function performs basic syntax validation to catch common errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valid and invalid FQL expressions\n",
    "test_expressions = [\n",
    "    ('\"age\" > 30', \"Valid simple expression\"),\n",
    "    ('\"unclosed > 30', \"Unbalanced double quotes\"),\n",
    "    ('\"age\" > \\'30', \"Unbalanced single quotes\"),\n",
    "    (\"sum(if(fp(), 1, 0)\", \"Unbalanced parentheses\"),\n",
    "    (\"\\\"\\\" == 'value'\", \"Empty column reference\"),\n",
    "    ('\"status\" == \\'active\\' and \"verified\" == true', \"Valid complex expression\"),\n",
    "]\n",
    "\n",
    "print(\"FQL Syntax Validation:\\n\")\n",
    "for expr, description in test_expressions:\n",
    "    is_valid, error_msg = fql.validate_fql_syntax(expr)\n",
    "    status = \"âœ“\" if is_valid else \"âœ—\"\n",
    "    print(f\"{status} {description}\")\n",
    "    print(f\"  Expression: {expr}\")\n",
    "    if not is_valid:\n",
    "        print(f\"  Error: {error_msg}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Normalize Expressions\n",
    "\n",
    "The `normalize_expression()` function standardizes whitespace and formatting for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expressions with inconsistent formatting\n",
    "messy_expressions = [\n",
    "    '\"age\"   >  30',\n",
    "    '\"status\"==\"active\"',\n",
    "    \"sum(  if(  fp(  ),1,0)  )\",\n",
    "    \"\\\"region\\\"   in  ['West','East','North']\",\n",
    "]\n",
    "\n",
    "print(\"Expression Normalization:\\n\")\n",
    "for expr in messy_expressions:\n",
    "    normalized = fql.normalize_expression(expr)\n",
    "    print(f\"Original:    {expr}\")\n",
    "    print(f\"Normalized:  {normalized}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Extract FQL Functions\n",
    "\n",
    "The `get_fql_functions()` function identifies all function calls in an expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expressions with various functions\n",
    "function_examples = [\n",
    "    (\"sum(if(fp(), 1, 0))\", \"Custom metric with false positives\"),\n",
    "    ('avg(\"response_time\")', \"Simple average\"),\n",
    "    (\"count(if(\\\"status\\\" == 'failed', 1, 0))\", \"Conditional count\"),\n",
    "    ('sum(if(tp(), \"revenue\", 0)) - sum(if(fp(), \"cost\", 0))', \"Net value calculation\"),\n",
    "    (\n",
    "        '(sum(if((\"probability_churn\">0.8 and \"gender\"== \\'Nonbinary\\'), 1, 0))/sum(if((\"gender\"== \\'Nonbinary\\'), 1, 0)))/(sum(if((\"probability_churn\">0.8 and \"gender\"== \\'Male\\'), 1, 0))/sum(if((\"gender\"== \\'Male\\'), 1, 0)))',\n",
    "        \"Disparate Impact Non Binary\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(\"FQL Function Extraction:\\n\")\n",
    "for expr, description in function_examples:\n",
    "    functions = fql.get_fql_functions(expr)\n",
    "    print(f\"Description: {description}\")\n",
    "    print(f\"  Expression: {expr}\")\n",
    "    print(f\"  Functions: {functions}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Distinguish Simple Filters from Aggregations\n",
    "\n",
    "The `is_simple_filter()` function helps determine if an expression is a simple filter (usable in segments) or contains aggregations (typically for custom metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mix of simple and complex expressions\n",
    "classification_examples = [\n",
    "    ('\"age\" > 30 and \"status\" == \\'active\\'', \"Segment filter\"),\n",
    "    (\"sum(if(fp(), 1, 0))\", \"Custom metric\"),\n",
    "    (\"\\\"region\\\" in ['West', 'East']\", \"Segment filter with list\"),\n",
    "    ('avg(\"transaction_value\")', \"Aggregation metric\"),\n",
    "    ('if(\"premium\" == true, \"discount\", 0)', \"Conditional (no aggregation)\"),\n",
    "]\n",
    "\n",
    "print(\"Expression Classification:\\n\")\n",
    "for expr, description in classification_examples:\n",
    "    is_simple = fql.is_simple_filter(expr)\n",
    "    expr_type = \"Simple Filter\" if is_simple else \"Aggregation/Complex\"\n",
    "    icon = \"ðŸ“Š\" if is_simple else \"ðŸ“ˆ\"\n",
    "    print(f\"{icon} {description}\")\n",
    "    print(f\"  Type: {expr_type}\")\n",
    "    print(f\"  Expression: {expr}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Split AND Conditions\n",
    "\n",
    "The `split_fql_and_condition()` function breaks down complex filter expressions into individual conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex expressions with AND conditions\n",
    "complex_expr = '\"age\" > 30 and \"geography\" == \\'California\\' and \"credit_score\" >= 700'\n",
    "\n",
    "print(\"Splitting AND Conditions:\\n\")\n",
    "print(\"Original expression:\")\n",
    "print(f\"  {complex_expr}\\n\")\n",
    "\n",
    "parts = fql.split_fql_and_condition(complex_expr)\n",
    "print(f\"Split into {len(parts)} conditions:\\n\")\n",
    "for i, part in enumerate(parts, 1):\n",
    "    print(f\"  {i}. {part}\")\n",
    "\n",
    "# Note: This is a simple split and may not handle all cases\n",
    "print(\n",
    "    \"\\nâš ï¸ Note: Simple implementation - may not handle 'and' inside function calls correctly\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Live Fiddler Integration\n",
    "\n",
    "Connect to your Fiddler environment and analyze real FQL expressions from existing assets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Connect to Fiddler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Fiddler client\n",
    "if URL and TOKEN:\n",
    "    get_or_init(url=URL, token=TOKEN, log_level=\"ERROR\")\n",
    "    print(\"âœ“ Connected to Fiddler\")\n",
    "else:\n",
    "    print(\"âš ï¸ Please set URL and TOKEN in the configuration section above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 List Available Projects and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all projects\n",
    "if URL and TOKEN:\n",
    "    projects = list(fdl.Project.list())\n",
    "    print(f\"Available Projects ({len(projects)}):\")\n",
    "    print()\n",
    "\n",
    "    for project in projects[:10]:  # Show first 10\n",
    "        try:\n",
    "            models = list(fdl.Model.list(project_id=project.id))\n",
    "            print(f\"ðŸ“ {project.name}\")\n",
    "            for model in models[:5]:  # Show first 5 models per project\n",
    "                print(f\"  â””â”€ {model.name} (ID: {model.id})\")\n",
    "            if len(models) > 5:\n",
    "                print(f\"  â””â”€ ... and {len(models) - 5} more models\")\n",
    "        except Exception as e:\n",
    "            print(f\"  â””â”€ Error listing models: {e}\")\n",
    "        print()\n",
    "\n",
    "    if len(projects) > 10:\n",
    "        print(f\"... and {len(projects) - 10} more projects\")\n",
    "\n",
    "    print(\"\\nâ„¹ï¸ Set PROJECT_NAME and MODEL_NAME above to focus on a specific model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Get Model and Analyze Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get specified model or use first available\n",
    "if URL and TOKEN:\n",
    "    if PROJECT_NAME and MODEL_NAME:\n",
    "        try:\n",
    "            # Use correct API: Project.get_or_create and Model.from_name\n",
    "            project = fdl.Project.get_or_create(name=PROJECT_NAME)\n",
    "            model = fdl.Model.from_name(\n",
    "                name=MODEL_NAME, project_id=project.id, version=MODEL_VERSION\n",
    "            )\n",
    "            print(f\"âœ“ Using model: {project.name}/{model.name}/{model.version}\")\n",
    "        except fdl.NotFound:\n",
    "            print(f\"âœ— Model not found: {PROJECT_NAME}/{MODEL_NAME}/{MODEL_VERSION}\")\n",
    "            print(\"  Using first available model instead...\")\n",
    "            model = None\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Error getting model: {e}\")\n",
    "            print(\"  Using first available model instead...\")\n",
    "            model = None\n",
    "    else:\n",
    "        model = None\n",
    "\n",
    "    # Fallback to first model with segments\n",
    "    if model is None:\n",
    "        for project in fdl.Project.list():\n",
    "            try:\n",
    "                models = list(fdl.Model.list(project_id=project.id))\n",
    "                for m in models:\n",
    "                    segments = list(fdl.Segment.list(model_id=m.id))\n",
    "                    if segments:\n",
    "                        model = m\n",
    "                        print(\n",
    "                            f\"âœ“ Using model: {project.name}/{model.name} (found {len(segments)} segments)\"\n",
    "                        )\n",
    "                        break\n",
    "                if model:\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"  An error occurred: {e}\")\n",
    "                continue\n",
    "\n",
    "    if model is None:\n",
    "        print(\"âš ï¸ No models with segments found. Skipping live examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze segments from the model\n",
    "if URL and TOKEN and model:\n",
    "    try:\n",
    "        segments = list(fdl.Segment.list(model_id=model.id))\n",
    "\n",
    "        if segments:\n",
    "            print(f\"Analyzing {len(segments)} Segments:\\n\")\n",
    "\n",
    "            for segment in segments:\n",
    "                print(f\"ðŸ“Š Segment: {segment.name}\")\n",
    "                print(f\"  Expression: {segment.definition}\")\n",
    "\n",
    "                # Extract columns\n",
    "                columns = fql.extract_columns(segment.definition)\n",
    "                print(f\"  Columns: {columns}\")\n",
    "\n",
    "                # Check if simple filter\n",
    "                is_simple = fql.is_simple_filter(segment.definition)\n",
    "                print(\n",
    "                    f\"  Type: {'Simple filter' if is_simple else 'Contains aggregations'}\"\n",
    "                )\n",
    "\n",
    "                # Validate syntax\n",
    "                is_valid, error = fql.validate_fql_syntax(segment.definition)\n",
    "                status = \"âœ“ Valid\" if is_valid else f\"âœ— Invalid: {error}\"\n",
    "                print(f\"  Syntax: {status}\")\n",
    "\n",
    "                # Get functions used\n",
    "                functions = fql.get_fql_functions(segment.definition)\n",
    "                if functions:\n",
    "                    print(f\"  Functions: {functions}\")\n",
    "\n",
    "                print()\n",
    "        else:\n",
    "            print(\"â„¹ï¸ No segments found for this model\")\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error analyzing segments: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Analyze Custom Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze custom metrics from the model\n",
    "if URL and TOKEN and model:\n",
    "    try:\n",
    "        custom_metrics = list(fdl.CustomMetric.list(model_id=model.id))\n",
    "\n",
    "        if custom_metrics:\n",
    "            print(f\"Analyzing {len(custom_metrics)} Custom Metrics:\\n\")\n",
    "\n",
    "            for metric in custom_metrics:\n",
    "                print(f\"ðŸ“ˆ Custom Metric: {metric.name}\")\n",
    "                print(f\"  Definition: {metric.definition}\")\n",
    "\n",
    "                # Extract columns\n",
    "                columns = fql.extract_columns(metric.definition)\n",
    "                print(f\"  Columns: {columns}\")\n",
    "\n",
    "                # Get functions used\n",
    "                functions = fql.get_fql_functions(metric.definition)\n",
    "                print(f\"  Functions: {functions}\")\n",
    "\n",
    "                # Check if it's actually a simple filter (unusual for custom metrics)\n",
    "                is_simple = fql.is_simple_filter(metric.definition)\n",
    "                if is_simple:\n",
    "                    print(\"  âš ï¸ No aggregations detected (unusual for custom metrics)\")\n",
    "\n",
    "                # Validate syntax\n",
    "                is_valid, error = fql.validate_fql_syntax(metric.definition)\n",
    "                status = \"âœ“ Valid\" if is_valid else f\"âœ— Invalid: {error}\"\n",
    "                print(f\"  Syntax: {status}\")\n",
    "\n",
    "                print()\n",
    "        else:\n",
    "            print(\"â„¹ï¸ No custom metrics found for this model\")\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error analyzing custom metrics: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Schema Compatibility Validation\n",
    "\n",
    "Validate that FQL expressions only reference columns that exist in a model's schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Get Model Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch model schema\n",
    "if URL and TOKEN and model:\n",
    "    try:\n",
    "        # Get model spec to extract column names\n",
    "        model_info = fdl.Model.get(id_=model.id)\n",
    "\n",
    "        # Extract all column names from the spec\n",
    "        valid_columns = set()\n",
    "\n",
    "        # Helper function to extract column names from spec lists\n",
    "        def extract_col_names(col_list):\n",
    "            \"\"\"Extract column names from a list that may contain strings or objects.\"\"\"\n",
    "            if not col_list:\n",
    "                return []\n",
    "            names = []\n",
    "            for col in col_list:\n",
    "                if isinstance(col, str):\n",
    "                    names.append(col)\n",
    "                elif hasattr(col, \"column_name\"):\n",
    "                    names.append(col.column_name)\n",
    "                elif hasattr(col, \"name\"):\n",
    "                    names.append(col.name)\n",
    "            return names\n",
    "\n",
    "        # Columns from spec\n",
    "        if hasattr(model_info, \"spec\") and model_info.spec:\n",
    "            spec = model_info.spec\n",
    "\n",
    "            # Input columns\n",
    "            if hasattr(spec, \"inputs\") and spec.inputs:\n",
    "                valid_columns.update(extract_col_names(spec.inputs))\n",
    "\n",
    "            # Output columns\n",
    "            if hasattr(spec, \"outputs\") and spec.outputs:\n",
    "                valid_columns.update(extract_col_names(spec.outputs))\n",
    "\n",
    "            # Target columns\n",
    "            if hasattr(spec, \"targets\") and spec.targets:\n",
    "                valid_columns.update(extract_col_names(spec.targets))\n",
    "\n",
    "            # Metadata columns\n",
    "            if hasattr(spec, \"metadata\") and spec.metadata:\n",
    "                valid_columns.update(extract_col_names(spec.metadata))\n",
    "\n",
    "            # Custom features\n",
    "            if hasattr(spec, \"custom_features\") and spec.custom_features:\n",
    "                valid_columns.update(extract_col_names(spec.custom_features))\n",
    "\n",
    "        print(f\"âœ“ Model Schema Loaded: {len(valid_columns)} columns\\n\")\n",
    "        print(f\"Available columns: {sorted(valid_columns)[:20]}\")\n",
    "        if len(valid_columns) > 20:\n",
    "            print(f\"... and {len(valid_columns) - 20} more columns\")\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error fetching model schema: {e}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "        valid_columns = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Validate Segment Expressions Against Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate all segment expressions\n",
    "if URL and TOKEN and model and valid_columns:\n",
    "    try:\n",
    "        segments = list(fdl.Segment.list(model_id=model.id))\n",
    "\n",
    "        if segments:\n",
    "            print(f\"Validating {len(segments)} Segment Expressions:\\n\")\n",
    "\n",
    "            all_valid = True\n",
    "            validation_results = []\n",
    "\n",
    "            for segment in segments:\n",
    "                is_valid, missing_cols = fql.validate_column_references(\n",
    "                    segment.definition, valid_columns\n",
    "                )\n",
    "\n",
    "                validation_results.append(\n",
    "                    {\"name\": segment.name, \"valid\": is_valid, \"missing\": missing_cols}\n",
    "                )\n",
    "\n",
    "                if not is_valid:\n",
    "                    all_valid = False\n",
    "\n",
    "            # Show results\n",
    "            for result in validation_results:\n",
    "                status = \"âœ“\" if result[\"valid\"] else \"âœ—\"\n",
    "                print(f\"{status} {result['name']}\")\n",
    "                if not result[\"valid\"]:\n",
    "                    print(f\"  Missing columns: {result['missing']}\")\n",
    "\n",
    "            print()\n",
    "            if all_valid:\n",
    "                print(\"âœ… All segment expressions are valid!\")\n",
    "            else:\n",
    "                invalid_count = sum(1 for r in validation_results if not r[\"valid\"])\n",
    "                print(f\"âš ï¸ {invalid_count} segment(s) reference missing columns\")\n",
    "        else:\n",
    "            print(\"â„¹ï¸ No segments to validate\")\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error validating segments: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Validate Custom Metric Expressions Against Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate all custom metric expressions\n",
    "if URL and TOKEN and model and valid_columns:\n",
    "    try:\n",
    "        custom_metrics = list(fdl.CustomMetric.list(model_id=model.id))\n",
    "\n",
    "        if custom_metrics:\n",
    "            print(f\"Validating {len(custom_metrics)} Custom Metric Expressions:\\n\")\n",
    "\n",
    "            all_valid = True\n",
    "            validation_results = []\n",
    "\n",
    "            for metric in custom_metrics:\n",
    "                is_valid, missing_cols = fql.validate_column_references(\n",
    "                    metric.definition, valid_columns\n",
    "                )\n",
    "\n",
    "                validation_results.append(\n",
    "                    {\"name\": metric.name, \"valid\": is_valid, \"missing\": missing_cols}\n",
    "                )\n",
    "\n",
    "                if not is_valid:\n",
    "                    all_valid = False\n",
    "\n",
    "            # Show results\n",
    "            for result in validation_results:\n",
    "                status = \"âœ“\" if result[\"valid\"] else \"âœ—\"\n",
    "                print(f\"{status} {result['name']}\")\n",
    "                if not result[\"valid\"]:\n",
    "                    print(f\"  Missing columns: {result['missing']}\")\n",
    "\n",
    "            print()\n",
    "            if all_valid:\n",
    "                print(\"âœ… All custom metric expressions are valid!\")\n",
    "            else:\n",
    "                invalid_count = sum(1 for r in validation_results if not r[\"valid\"])\n",
    "                print(f\"âš ï¸ {invalid_count} custom metric(s) reference missing columns\")\n",
    "        else:\n",
    "            print(\"â„¹ï¸ No custom metrics to validate\")\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error validating custom metrics: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Test Validation with Synthetic Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic test cases\n",
    "if valid_columns and len(valid_columns) > 0:\n",
    "    # Get some real column names for testing\n",
    "    sample_cols = list(valid_columns)[:3]\n",
    "\n",
    "    test_cases = [\n",
    "        (f'\"{sample_cols[0]}\" > 30', \"Valid expression with real column\"),\n",
    "        (\n",
    "            f'\"{sample_cols[0]}\" > 30 and \"nonexistent_column\" == 1',\n",
    "            \"Invalid - references missing column\",\n",
    "        ),\n",
    "        (\n",
    "            '\"fake_col_1\" == \\'test\\' or \"fake_col_2\" > 100',\n",
    "            \"Invalid - multiple missing columns\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    if len(sample_cols) >= 2:\n",
    "        test_cases.append(\n",
    "            (\n",
    "                f'\"{sample_cols[0]}\" > 30 and \"{sample_cols[1]}\" == \\'active\\'',\n",
    "                \"Valid expression with multiple real columns\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    print(\"Testing Validation with Synthetic Examples:\\n\")\n",
    "\n",
    "    for expr, description in test_cases:\n",
    "        is_valid, missing = fql.validate_column_references(expr, valid_columns)\n",
    "        status = \"âœ“\" if is_valid else \"âœ—\"\n",
    "\n",
    "        print(f\"{status} {description}\")\n",
    "        print(f\"  Expression: {expr}\")\n",
    "        if not is_valid:\n",
    "            print(f\"  Missing: {missing}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"â„¹ï¸ No valid columns available for synthetic testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Column Mapping and Asset Migration\n",
    "\n",
    "Transform FQL expressions when migrating assets between models with different schemas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Simple Column Name Replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Renaming columns in an expression\n",
    "original_expr = '\"age\" > 30 and \"geography\" == \\'California\\' and \"credit_score\" >= 700'\n",
    "\n",
    "# Define column mapping (old_name -> new_name)\n",
    "column_mapping = {\n",
    "    \"age\": \"customer_age\",\n",
    "    \"geography\": \"location\",\n",
    "    \"credit_score\": \"fico_score\",\n",
    "}\n",
    "\n",
    "print(\"Simple Column Name Replacement:\\n\")\n",
    "print(\"Original expression:\")\n",
    "print(f\"  {original_expr}\\n\")\n",
    "\n",
    "print(\"Column mapping:\")\n",
    "for old, new in column_mapping.items():\n",
    "    print(f\"  {old} â†’ {new}\")\n",
    "print()\n",
    "\n",
    "transformed_expr = fql.replace_column_names(original_expr, column_mapping)\n",
    "print(\"Transformed expression:\")\n",
    "print(f\"  {transformed_expr}\\n\")\n",
    "\n",
    "# Verify the transformation\n",
    "original_cols = fql.extract_columns(original_expr)\n",
    "transformed_cols = fql.extract_columns(transformed_expr)\n",
    "\n",
    "print(\"Verification:\")\n",
    "print(f\"  Original columns: {original_cols}\")\n",
    "print(f\"  Transformed columns: {transformed_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Complex Expression Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform custom metric with aggregations\n",
    "custom_metric_expr = 'sum(if(fp(), \"transaction_value\", 0)) / count(if(fp(), 1, 0))'\n",
    "\n",
    "metric_mapping = {\"transaction_value\": \"txn_amount\"}\n",
    "\n",
    "print(\"Complex Expression Transformation:\\n\")\n",
    "print(\"Original custom metric:\")\n",
    "print(f\"  {custom_metric_expr}\\n\")\n",
    "\n",
    "transformed_metric = fql.replace_column_names(custom_metric_expr, metric_mapping)\n",
    "print(\"Transformed custom metric:\")\n",
    "print(f\"  {transformed_metric}\\n\")\n",
    "\n",
    "# Verify functions are preserved\n",
    "original_functions = fql.get_fql_functions(custom_metric_expr)\n",
    "transformed_functions = fql.get_fql_functions(transformed_metric)\n",
    "\n",
    "print(\"Verification:\")\n",
    "print(f\"  Functions preserved: {original_functions == transformed_functions}\")\n",
    "print(f\"  Functions: {transformed_functions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Interactive Column Mapping Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to build column mapping between two models\n",
    "def build_column_mapping_interactive(\n",
    "    source_columns: Set[str], target_columns: Set[str]\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"Interactively build a column mapping between source and target schemas.\n",
    "\n",
    "    This is a simplified version for demonstration. In practice, you might:\n",
    "    - Use fuzzy matching to suggest mappings\n",
    "    - Allow user input for manual mapping\n",
    "    - Handle partial mappings\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "\n",
    "    # Exact matches (case-insensitive)\n",
    "    source_lower = {col: col for col in source_columns}\n",
    "    target_lower = {col.lower(): col for col in target_columns}\n",
    "\n",
    "    for source_col in source_columns:\n",
    "        if source_col.lower() in target_lower:\n",
    "            target_col = target_lower[source_col.lower()]\n",
    "            if source_col != target_col:\n",
    "                mapping[source_col] = target_col\n",
    "\n",
    "    return mapping\n",
    "\n",
    "\n",
    "# Example: Two models with similar but different schemas\n",
    "source_schema = {\"age\", \"geography\", \"credit_score\", \"income\", \"employment_status\"}\n",
    "target_schema = {\n",
    "    \"customer_age\",\n",
    "    \"location\",\n",
    "    \"fico_score\",\n",
    "    \"annual_income\",\n",
    "    \"employment_status\",\n",
    "}\n",
    "\n",
    "print(\"Interactive Column Mapping Builder:\\n\")\n",
    "print(f\"Source schema: {sorted(source_schema)}\")\n",
    "print(f\"Target schema: {sorted(target_schema)}\\n\")\n",
    "\n",
    "# Auto-detect exact matches\n",
    "auto_mapping = build_column_mapping_interactive(source_schema, target_schema)\n",
    "print(f\"Auto-detected mappings: {auto_mapping}\\n\")\n",
    "\n",
    "# Identify unmapped columns\n",
    "unmapped_source = source_schema - set(auto_mapping.keys()) - target_schema\n",
    "unmapped_target = target_schema - set(auto_mapping.values()) - source_schema\n",
    "\n",
    "print(f\"Unmapped source columns: {unmapped_source}\")\n",
    "print(f\"Unmapped target columns: {unmapped_target}\\n\")\n",
    "\n",
    "# Manual mapping for demonstration\n",
    "print(\"Suggested manual mappings:\")\n",
    "manual_mapping = {\n",
    "    \"age\": \"customer_age\",\n",
    "    \"geography\": \"location\",\n",
    "    \"credit_score\": \"fico_score\",\n",
    "    \"income\": \"annual_income\",\n",
    "}\n",
    "\n",
    "for source, target in manual_mapping.items():\n",
    "    print(f\"  {source} â†’ {target}\")\n",
    "\n",
    "# Combined mapping\n",
    "full_mapping = {**auto_mapping, **manual_mapping}\n",
    "print(f\"\\nFinal mapping: {full_mapping}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 End-to-End Migration Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete workflow: Migrate a segment from one model to another\n",
    "print(\"End-to-End Segment Migration Workflow:\\n\")\n",
    "\n",
    "# Source segment\n",
    "source_segment_name = \"High Risk Customers\"\n",
    "source_segment_expr = (\n",
    "    '\"age\" < 25 and \"credit_score\" < 650 and \"geography\" == \\'California\\''\n",
    ")\n",
    "\n",
    "print(\"Step 1: Source Segment\")\n",
    "print(f\"  Name: {source_segment_name}\")\n",
    "print(f\"  Expression: {source_segment_expr}\\n\")\n",
    "\n",
    "# Extract columns from source expression\n",
    "print(\"Step 2: Extract Column References\")\n",
    "source_cols = fql.extract_columns(source_segment_expr)\n",
    "print(f\"  Columns: {source_cols}\\n\")\n",
    "\n",
    "# Define target schema and mapping\n",
    "target_schema_cols = {\"customer_age\", \"fico_score\", \"location\", \"income_bracket\"}\n",
    "migration_mapping = {\n",
    "    \"age\": \"customer_age\",\n",
    "    \"credit_score\": \"fico_score\",\n",
    "    \"geography\": \"location\",\n",
    "}\n",
    "\n",
    "print(\"Step 3: Apply Column Mapping\")\n",
    "print(f\"  Target schema: {target_schema_cols}\")\n",
    "print(f\"  Mapping: {migration_mapping}\\n\")\n",
    "\n",
    "# Transform expression\n",
    "target_segment_expr = fql.replace_column_names(source_segment_expr, migration_mapping)\n",
    "print(\"Step 4: Transform Expression\")\n",
    "print(f\"  Transformed: {target_segment_expr}\\n\")\n",
    "\n",
    "# Validate against target schema\n",
    "print(\"Step 5: Validate Against Target Schema\")\n",
    "is_valid, missing_cols = fql.validate_column_references(\n",
    "    target_segment_expr, target_schema_cols\n",
    ")\n",
    "\n",
    "if is_valid:\n",
    "    print(\"  âœ… Expression is valid for target model\")\n",
    "    print(\"\\nStep 6: Ready to Create Segment\")\n",
    "    if DRY_RUN:\n",
    "        print(\"  [DRY RUN] Would create segment:\")\n",
    "        print(f\"    Name: {source_segment_name}\")\n",
    "        print(f\"    Expression: {target_segment_expr}\")\n",
    "    else:\n",
    "        print(\"  Set DRY_RUN=False to create the segment\")\n",
    "else:\n",
    "    print(f\"  âœ— Validation failed - missing columns: {missing_cols}\")\n",
    "    print(\"  Cannot proceed with migration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Batch Migration with Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Migrate multiple segments at once\n",
    "source_segments = [\n",
    "    {\"name\": \"High Risk\", \"expr\": '\"age\" < 25 and \"credit_score\" < 650'},\n",
    "    {\"name\": \"Premium Customers\", \"expr\": '\"income\" > 100000 and \"credit_score\" > 750'},\n",
    "    {\"name\": \"California Only\", \"expr\": \"\\\"geography\\\" == 'California'\"},\n",
    "]\n",
    "\n",
    "batch_mapping = {\n",
    "    \"age\": \"customer_age\",\n",
    "    \"credit_score\": \"fico_score\",\n",
    "    \"income\": \"annual_income\",\n",
    "    \"geography\": \"location\",\n",
    "}\n",
    "\n",
    "target_cols = {\"customer_age\", \"fico_score\", \"annual_income\", \"location\"}\n",
    "\n",
    "print(\"Batch Segment Migration:\\n\")\n",
    "\n",
    "results = []\n",
    "for segment in source_segments:\n",
    "    print(f\"Processing: {segment['name']}\")\n",
    "\n",
    "    # Transform\n",
    "    transformed = fql.replace_column_names(segment[\"expr\"], batch_mapping)\n",
    "\n",
    "    # Validate\n",
    "    is_valid, missing = fql.validate_column_references(transformed, target_cols)\n",
    "\n",
    "    result = {\n",
    "        \"name\": segment[\"name\"],\n",
    "        \"original\": segment[\"expr\"],\n",
    "        \"transformed\": transformed,\n",
    "        \"valid\": is_valid,\n",
    "        \"missing\": missing,\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "    status = \"âœ“\" if is_valid else \"âœ—\"\n",
    "    print(f\"  {status} Transformed: {transformed}\")\n",
    "    if not is_valid:\n",
    "        print(f\"    Missing: {missing}\")\n",
    "    print()\n",
    "\n",
    "# Summary\n",
    "valid_count = sum(1 for r in results if r[\"valid\"])\n",
    "print(\"Summary:\")\n",
    "print(f\"  Total segments: {len(results)}\")\n",
    "print(f\"  Valid after transformation: {valid_count}\")\n",
    "print(f\"  Failed validation: {len(results) - valid_count}\")\n",
    "\n",
    "if valid_count == len(results):\n",
    "    print(\"\\nâœ… All segments ready for migration!\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ {len(results) - valid_count} segment(s) need manual review\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Advanced Patterns and Best Practices\n",
    "\n",
    "Combining multiple utilities for robust FQL workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Comprehensive Expression Validation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_fql_comprehensive(expression: str, valid_columns: Set[str]) -> Dict:\n",
    "    \"\"\"Run comprehensive validation on an FQL expression.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with validation results and metadata\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        \"expression\": expression,\n",
    "        \"checks\": {},\n",
    "        \"all_valid\": True,\n",
    "        \"warnings\": [],\n",
    "        \"metadata\": {},\n",
    "    }\n",
    "\n",
    "    # Check 1: Syntax validation\n",
    "    is_valid, error = fql.validate_fql_syntax(expression)\n",
    "    results[\"checks\"][\"syntax\"] = {\"valid\": is_valid, \"error\": error}\n",
    "    if not is_valid:\n",
    "        results[\"all_valid\"] = False\n",
    "\n",
    "    # Check 2: Column references\n",
    "    is_valid, missing = fql.validate_column_references(expression, valid_columns)\n",
    "    results[\"checks\"][\"columns\"] = {\"valid\": is_valid, \"missing\": missing}\n",
    "    if not is_valid:\n",
    "        results[\"all_valid\"] = False\n",
    "\n",
    "    # Metadata: Extract columns\n",
    "    columns = fql.extract_columns(expression)\n",
    "    results[\"metadata\"][\"columns\"] = list(columns)\n",
    "    results[\"metadata\"][\"column_count\"] = len(columns)\n",
    "\n",
    "    # Metadata: Extract functions\n",
    "    functions = fql.get_fql_functions(expression)\n",
    "    results[\"metadata\"][\"functions\"] = list(functions)\n",
    "    results[\"metadata\"][\"has_aggregations\"] = not fql.is_simple_filter(expression)\n",
    "\n",
    "    # Warning: Check for complexity\n",
    "    if len(columns) > 5:\n",
    "        results[\"warnings\"].append(f\"Complex expression with {len(columns)} columns\")\n",
    "\n",
    "    if len(functions) > 3:\n",
    "        results[\"warnings\"].append(f\"Multiple nested functions ({len(functions)})\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Test the pipeline\n",
    "test_schema = {\"age\", \"income\", \"credit_score\", \"status\", \"region\"}\n",
    "\n",
    "test_expressions = [\n",
    "    '\"age\" > 30 and \"status\" == \\'active\\'',\n",
    "    'sum(if(fp(), \"transaction_value\", 0))',  # Missing column\n",
    "    '\"age\" > 30 and \"income\" > 50000 and \"credit_score\" >= 700',\n",
    "]\n",
    "\n",
    "print(\"Comprehensive Validation Pipeline:\\n\")\n",
    "\n",
    "for expr in test_expressions:\n",
    "    print(f\"Expression: {expr}\")\n",
    "    results = validate_fql_comprehensive(expr, test_schema)\n",
    "\n",
    "    # Show results\n",
    "    overall = \"âœ… PASS\" if results[\"all_valid\"] else \"âœ— FAIL\"\n",
    "    print(f\"  Overall: {overall}\")\n",
    "\n",
    "    # Checks\n",
    "    for check_name, check_result in results[\"checks\"].items():\n",
    "        status = \"âœ“\" if check_result[\"valid\"] else \"âœ—\"\n",
    "        print(\n",
    "            f\"  {status} {check_name.title()}: {'Valid' if check_result['valid'] else check_result.get('error') or check_result.get('missing')}\"\n",
    "        )\n",
    "\n",
    "    # Metadata\n",
    "    print(\"  Metadata:\")\n",
    "    print(f\"    Columns: {results['metadata']['columns']}\")\n",
    "    print(f\"    Functions: {results['metadata']['functions']}\")\n",
    "    print(f\"    Has aggregations: {results['metadata']['has_aggregations']}\")\n",
    "\n",
    "    # Warnings\n",
    "    if results[\"warnings\"]:\n",
    "        print(\"  Warnings:\")\n",
    "        for warning in results[\"warnings\"]:\n",
    "            print(f\"    âš ï¸ {warning}\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Expression Comparison and Deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use normalization to find duplicate expressions\n",
    "segment_expressions = [\n",
    "    '\"age\" > 30 and \"status\" == \\'active\\'',\n",
    "    '\"age\"   >   30   and   \"status\"   ==   \\'active\\'',  # Same, different whitespace\n",
    "    '\"status\" == \\'active\\' and \"age\" > 30',  # Different order, semantically same\n",
    "    '\"age\" > 25 and \"status\" == \\'active\\'',  # Actually different\n",
    "]\n",
    "\n",
    "print(\"Expression Comparison and Deduplication:\\n\")\n",
    "\n",
    "normalized_map = {}\n",
    "for i, expr in enumerate(segment_expressions, 1):\n",
    "    normalized = fql.normalize_expression(expr)\n",
    "\n",
    "    if normalized in normalized_map:\n",
    "        print(f\"Expression {i}: DUPLICATE of Expression {normalized_map[normalized]}\")\n",
    "    else:\n",
    "        print(f\"Expression {i}: UNIQUE\")\n",
    "        normalized_map[normalized] = i\n",
    "\n",
    "    print(f\"  Original:    {expr}\")\n",
    "    print(f\"  Normalized:  {normalized}\")\n",
    "    print()\n",
    "\n",
    "print(\n",
    "    f\"Summary: {len(normalized_map)} unique expressions out of {len(segment_expressions)} total\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Safe Expression Modification Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_column_replacement(\n",
    "    expression: str,\n",
    "    mapping: Dict[str, str],\n",
    "    target_schema: Set[str],\n",
    "    dry_run: bool = True,\n",
    ") -> Dict:\n",
    "    \"\"\"Safely replace column names with validation.\n",
    "\n",
    "    Args:\n",
    "        expression: Original FQL expression\n",
    "        mapping: Column name mapping (old -> new)\n",
    "        target_schema: Valid columns in target schema\n",
    "        dry_run: If True, only simulate the change\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with transformation results\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        \"original\": expression,\n",
    "        \"transformed\": None,\n",
    "        \"success\": False,\n",
    "        \"errors\": [],\n",
    "        \"warnings\": [],\n",
    "        \"dry_run\": dry_run,\n",
    "    }\n",
    "\n",
    "    # Step 1: Validate original syntax\n",
    "    is_valid, error = fql.validate_fql_syntax(expression)\n",
    "    if not is_valid:\n",
    "        result[\"errors\"].append(f\"Original expression has syntax error: {error}\")\n",
    "        return result\n",
    "\n",
    "    # Step 2: Extract columns from original\n",
    "    original_cols = fql.extract_columns(expression)\n",
    "\n",
    "    # Step 3: Check if all columns to be replaced exist\n",
    "    cols_to_replace = set(mapping.keys())\n",
    "    missing_in_expr = cols_to_replace - original_cols\n",
    "    if missing_in_expr:\n",
    "        result[\"warnings\"].append(\n",
    "            f\"Columns in mapping not found in expression: {missing_in_expr}\"\n",
    "        )\n",
    "\n",
    "    # Step 4: Apply transformation\n",
    "    transformed = fql.replace_column_names(expression, mapping)\n",
    "    result[\"transformed\"] = transformed\n",
    "\n",
    "    # Step 5: Validate transformed syntax\n",
    "    is_valid, error = fql.validate_fql_syntax(transformed)\n",
    "    if not is_valid:\n",
    "        result[\"errors\"].append(f\"Transformed expression has syntax error: {error}\")\n",
    "        return result\n",
    "\n",
    "    # Step 6: Validate against target schema\n",
    "    is_valid, missing = fql.validate_column_references(transformed, target_schema)\n",
    "    if not is_valid:\n",
    "        result[\"errors\"].append(\n",
    "            f\"Transformed expression references missing columns: {missing}\"\n",
    "        )\n",
    "        return result\n",
    "\n",
    "    # Step 7: Success!\n",
    "    result[\"success\"] = True\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Test safe replacement\n",
    "print(\"Safe Expression Modification Workflow:\\n\")\n",
    "\n",
    "test_expr = '\"age\" > 30 and \"credit_score\" >= 700'\n",
    "test_mapping = {\"age\": \"customer_age\", \"credit_score\": \"fico_score\"}\n",
    "test_target_schema = {\"customer_age\", \"fico_score\", \"location\", \"income\"}\n",
    "\n",
    "result = safe_column_replacement(\n",
    "    test_expr, test_mapping, test_target_schema, dry_run=DRY_RUN\n",
    ")\n",
    "\n",
    "print(f\"Original: {result['original']}\")\n",
    "print(f\"Transformed: {result['transformed']}\")\n",
    "print(f\"Status: {'âœ… SUCCESS' if result['success'] else 'âœ— FAILED'}\")\n",
    "\n",
    "if result[\"errors\"]:\n",
    "    print(\"\\nErrors:\")\n",
    "    for error in result[\"errors\"]:\n",
    "        print(f\"  âœ— {error}\")\n",
    "\n",
    "if result[\"warnings\"]:\n",
    "    print(\"\\nWarnings:\")\n",
    "    for warning in result[\"warnings\"]:\n",
    "        print(f\"  âš ï¸ {warning}\")\n",
    "\n",
    "if result[\"dry_run\"] and result[\"success\"]:\n",
    "    print(\"\\nâ„¹ï¸ DRY RUN MODE - No changes made\")\n",
    "    print(\"   Set DRY_RUN=False to apply transformation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Expression Analysis Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_expression_complexity(expression: str) -> Dict:\n",
    "    \"\"\"Analyze FQL expression complexity and characteristics.\"\"\"\n",
    "    return {\n",
    "        \"length\": len(expression),\n",
    "        \"columns\": fql.extract_columns(expression),\n",
    "        \"column_count\": len(fql.extract_columns(expression)),\n",
    "        \"functions\": fql.get_fql_functions(expression),\n",
    "        \"function_count\": len(fql.get_fql_functions(expression)),\n",
    "        \"is_simple_filter\": fql.is_simple_filter(expression),\n",
    "        \"has_aggregations\": not fql.is_simple_filter(expression),\n",
    "        \"and_conditions\": len(fql.split_fql_and_condition(expression)),\n",
    "        \"complexity_score\": (\n",
    "            len(fql.extract_columns(expression)) * 1.0\n",
    "            + len(fql.get_fql_functions(expression)) * 2.0\n",
    "            + len(fql.split_fql_and_condition(expression)) * 0.5\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "# Analyze various expressions\n",
    "expressions_to_analyze = [\n",
    "    '\"age\" > 30',\n",
    "    '\"age\" > 30 and \"status\" == \\'active\\'',\n",
    "    \"\\\"age\\\" > 30 and \\\"status\\\" == 'active' and \\\"region\\\" in ['West', 'East']\",\n",
    "    \"sum(if(fp(), 1, 0))\",\n",
    "    'sum(if(fp(), \"value\", 0)) / count(if(tp(), 1, 0))',\n",
    "]\n",
    "\n",
    "print(\"Expression Complexity Analysis:\\n\")\n",
    "\n",
    "for expr in expressions_to_analyze:\n",
    "    analysis = analyze_expression_complexity(expr)\n",
    "\n",
    "    print(f\"Expression: {expr}\")\n",
    "    print(\n",
    "        f\"  Type: {'Simple Filter' if analysis['is_simple_filter'] else 'Aggregation/Metric'}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Columns: {analysis['column_count']} ({', '.join(analysis['columns']) if analysis['columns'] else 'none'})\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Functions: {analysis['function_count']} ({', '.join(analysis['functions']) if analysis['functions'] else 'none'})\"\n",
    "    )\n",
    "    print(f\"  AND conditions: {analysis['and_conditions']}\")\n",
    "    print(f\"  Complexity score: {analysis['complexity_score']:.1f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **FQL Syntax Fundamentals**\n",
    "   * Column names in double quotes: `\"column_name\"`\n",
    "   * String values in single quotes: `'value'`\n",
    "   * Numeric values unquoted: `42`\n",
    "\n",
    "2. **Core Utility Functions**\n",
    "   * `extract_columns()` - Find all column references\n",
    "   * `validate_fql_syntax()` - Catch syntax errors\n",
    "   * `validate_column_references()` - Check schema compatibility\n",
    "   * `replace_column_names()` - Transform expressions for migration\n",
    "   * `normalize_expression()` - Standardize formatting\n",
    "   * `get_fql_functions()` - Identify functions used\n",
    "   * `is_simple_filter()` - Distinguish filters from aggregations\n",
    "   * `split_fql_and_condition()` - Break down complex conditions\n",
    "\n",
    "3. **Common Use Cases**\n",
    "   * **Asset migration:** Copy segments/metrics between models with different schemas\n",
    "   * **Validation:** Verify expressions before deployment\n",
    "   * **Analysis:** Understand expression complexity and dependencies\n",
    "   * **Deduplication:** Find semantically identical expressions\n",
    "\n",
    "4. **Best Practices**\n",
    "   * Always validate syntax before applying transformations\n",
    "   * Check schema compatibility after column name replacements\n",
    "   * Use dry-run mode for testing transformations\n",
    "   * Normalize expressions when comparing for equality\n",
    "   * Build comprehensive validation pipelines for production workflows\n",
    "\n",
    "### Common Gotchas\n",
    "\n",
    "* **Quote consistency:** Mixing single/double quotes will cause syntax errors\n",
    "* **Partial column name matches:** Use word boundaries in replacements to avoid partial matches\n",
    "* **Expression order:** `\"a\" and \"b\"` vs `\"b\" and \"a\"` are semantically same but string-different\n",
    "* **Function detection:** `split_fql_and_condition()` uses simple pattern matching and may not handle complex nested cases\n",
    "* **Schema validation:** Only checks if columns exist, not data types or value compatibility\n",
    "\n",
    "### When to Use Each Function\n",
    "\n",
    "| Function | Use When |\n",
    "|----------|----------|\n",
    "| `extract_columns()` | You need to know what data columns an expression depends on |\n",
    "| `validate_fql_syntax()` | Before saving expressions to catch obvious syntax errors |\n",
    "| `validate_column_references()` | Migrating assets or checking if expression will work on a model |\n",
    "| `replace_column_names()` | Copying assets between models with different column names |\n",
    "| `normalize_expression()` | Comparing expressions or finding duplicates |\n",
    "| `get_fql_functions()` | Analyzing expression complexity or checking for specific functions |\n",
    "| `is_simple_filter()` | Determining if an expression can be used as a segment filter |\n",
    "| `split_fql_and_condition()` | Breaking down complex filters into individual conditions |\n",
    "\n",
    "### Resources\n",
    "\n",
    "* **Fiddler FQL Documentation:** https://docs.fiddler.ai\n",
    "* **fiddler_utils package:** See `fiddler_utils/fql.py` for source code\n",
    "* **Additional utilities:** See `/misc-utils/README.md` for other helpful tools"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
