{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FQL Utilities Demonstration\n",
    "\n",
    "This notebook demonstrates the FQL (Fiddler Query Language) utility functions available in the `fiddler_utils` package. These utilities help you parse, validate, transform, and analyze FQL expressions used in segments, custom metrics, and other Fiddler assets.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "* Access to a Fiddler environment with at least one model containing [segments](https://docs.fiddler.ai/product-guide/monitoring-platform/segments) or [custom metrics](https://docs.fiddler.ai/product-guide/monitoring-platform/custom-metrics)\n",
    "* [API toke](https://docs.fiddler.ai/configuration-guide/settings#credentials)n with [read access](https://docs.fiddler.ai/configuration-guide/access-control/role-based-access#understanding-permissions) (write access needed for Section 5 examples)\n",
    "* Python packages: [fiddler-client](https://docs.fiddler.ai/technical-reference/python-client-guides/installation-and-setup), `fiddler_utils`\n",
    "\n",
    "## What is FQL?\n",
    "\n",
    "[FQL is Fiddler's query language](https://docs.fiddler.ai/product-guide/monitoring-platform/fiddler-query-language) for defining:\n",
    "* **Segments** - Filter expressions to define data subsets\n",
    "* **Custom Metrics** - Calculated metrics using aggregation functions\n",
    "* **Alert Rules** - Conditions for triggering alerts\n",
    "\n",
    "### FQL Syntax Rules\n",
    "\n",
    "* **Column names:** Always in double quotes (e.g., `\"column_name\"`)\n",
    "* **String values:** Always in single quotes (e.g., `'value'`)\n",
    "* **Numeric values:** No quotes (e.g., `42`, `3.14`)\n",
    "* **Operators:** `==`, `!=`, `>`, `<`, `>=`, `<=`, `and`, `or`, `not`\n",
    "* **Functions:** `sum()`, `avg()`, `if()`, `fp()`, `fn()`, `tp()`, `tn()`, etc.\n",
    "\n",
    "### Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "**Quick Navigation:**\n",
    "\n",
    "1. [**Section 1: Standalone FQL Utilities**](#Section-1:-Standalone-FQL-Utilities)\n",
    "   * [1.1 Extract Column References](#1.1-Extract-Column-References)\n",
    "   * [1.2 Validate FQL Syntax](#1.2-Validate-FQL-Syntax)\n",
    "   * [1.3 Normalize Expressions](#1.3-Normalize-Expressions)\n",
    "   * [1.4 Extract FQL Functions](#1.4-Extract-FQL-Functions)\n",
    "   * [1.5 Distinguish Simple Filters from Aggregations](#1.5-Distinguish-Simple-Filters-from-Aggregations)\n",
    "   * [1.6 Split AND Conditions](#1.6-Split-AND-Conditions)\n",
    "\n",
    "2. [**Section 2: Live Fiddler Integration**](#Section-2:-Live-Fiddler-Integration)\n",
    "   * [2.1 Connect to Fiddler](#2.1-Connect-to-Fiddler)\n",
    "   * [2.2 List Available Projects and Models](#2.2-List-Available-Projects-and-Models)\n",
    "   * [2.3 Get Model and Analyze Segments](#2.3-Get-Model-and-Analyze-Segments)\n",
    "   * [2.4 Analyze Custom Metrics](#2.4-Analyze-Custom-Metrics)\n",
    "\n",
    "3. [**Section 3: Column Mapping and Asset Migration**](#Section-3:-Column-Mapping-and-Asset-Migration)\n",
    "   * [3.1 Simple Column Name Replacement](#3.1-Simple-Column-Name-Replacement)\n",
    "   * [3.2 Complex Expression Transformation](#3.2-Complex-Expression-Transformation)\n",
    "   * [3.3 Interactive Column Mapping Builder](#3.3-Interactive-Column-Mapping-Builder)\n",
    "   * [3.4 End-to-End Migration Example](#3.4-End-to-End-Migration-Example)\n",
    "   * [3.5 Batch Migration with Validation](#3.5-Batch-Migration-with-Validation)\n",
    "\n",
    "4. [**Section 4: Advanced Patterns and Best Practices**](#Section-4:-Advanced-Patterns-and-Best-Practices)\n",
    "   * [4.1 Comprehensive Expression Validation Pipeline](#4.1-Comprehensive-Expression-Validation-Pipeline)\n",
    "   * [4.2 Expression Comparison and Deduplication](#4.2-Expression-Comparison-and-Deduplication)\n",
    "   * [4.3 Safe Expression Modification Workflow](#4.3-Safe-Expression-Modification-Workflow)\n",
    "   * [4.4 Expression Analysis Report](#4.4-Expression-Analysis-Report)\n",
    "\n",
    "5. [**Section 5: UUID Reference Management & Safe Metric Updates**](#Section-5:-UUID-Reference-Management-&-Safe-Metric-Updates) üÜï\n",
    "   * [5.1 Import Reference Management Utilities](#5.1-Import-Reference-Management-Utilities)\n",
    "   * [5.2 Demonstrating the UUID Problem](#5.2-Demonstrating-the-UUID-Problem)\n",
    "   * [5.3 Finding All References Before Updating](#5.3-Finding-All-References-Before-Updating)\n",
    "   * [5.4 Safe Metric Update with Automatic Reference Migration](#5.4-Safe-Metric-Update-with-Automatic-Reference-Migration)\n",
    "\n",
    "6. [**Section 6: Testing FQL Before Creating Metrics**](#Section-6:-Testing-FQL-Before-Creating-Metrics) üÜï\n",
    "   * [6.1 Import Testing Utilities](#6.1-Import-Testing-Utilities)\n",
    "   * [6.2 Local Pre-Validation (Fast)](#6.2-Local-Pre-Validation-(Fast))\n",
    "   * [6.3 Real Testing with Temporary Metrics](#6.3-Real-Testing-with-Temporary-Metrics)\n",
    "   * [6.4 Complete Validation Workflow](#6.4-Complete-Validation-Workflow)\n",
    "   * [6.5 Batch Testing Multiple Metrics](#6.5-Batch-Testing-Multiple-Metrics)\n",
    "   * [6.6 Cleanup Orphaned Test Metrics](#6.6-Cleanup-Orphaned-Test-Metrics)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from typing import Dict, Set\n",
    "\n",
    "import fiddler as fdl\n",
    "\n",
    "from fiddler_utils import fql\n",
    "from fiddler_utils.connection import get_or_init\n",
    "\n",
    "# Add parent directory to path to import fiddler_utils\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "print(\"‚úì Imports successful\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fiddler environment configuration\n",
    "URL = \"\"  # Example: 'https://your_company_name.fiddler.ai'\n",
    "TOKEN = \"\"  # Your API token\n",
    "\n",
    "# Model to use for examples (we'll list available models if not specified)\n",
    "PROJECT_NAME = \"\"  # Example: 'my_project'\n",
    "MODEL_NAME = \"\"  # Example: 'my_model'\n",
    "MODEL_VERSION = \"\"  # Example: 'v1' (optional)\n",
    "\n",
    "# Set to False to actually execute modifications in Section 5\n",
    "DRY_RUN = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Standalone FQL Utilities\n",
    "\n",
    "These functions work with FQL expressions directly, without requiring a Fiddler connection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Extract Column References\n",
    "\n",
    "The `extract_columns()` function identifies all column names referenced in an FQL expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example FQL expressions\n",
    "examples = [\n",
    "    '\"age\" > 30 and \"geography\" == \\'California\\'',\n",
    "    'sum(if(fp(), 1, 0) * \"transaction_value\")',\n",
    "    '\"credit_score\" >= 700 and \"loan_amount\" < 50000 and \"region\" == \\'West\\'',\n",
    "    'avg(\"response_time\") > 100',\n",
    "    '(sum(if((\"probability_churn\">0.8 and \"gender\"== \\'Nonbinary\\'), 1, 0))/sum(if((\"gender\"== \\'Nonbinary\\'), 1, 0)))/(sum(if((\"probability_churn\">0.8 and \"gender\"== \\'Male\\'), 1, 0))/sum(if((\"gender\"== \\'Male\\'), 1, 0)))',\n",
    "]\n",
    "\n",
    "print(\"Column Extraction Examples:\\n\")\n",
    "for expr in examples:\n",
    "    columns = fql.extract_columns(expr)\n",
    "    print(f\"Expression: {expr}\")\n",
    "    print(f\"  Columns: {columns}\")\n",
    "    print(f\"  Count: {len(columns)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Validate FQL Syntax\n",
    "\n",
    "The `validate_fql_syntax()` function performs basic syntax validation to catch common errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valid and invalid FQL expressions\n",
    "test_expressions = [\n",
    "    ('\"age\" > 30', \"Valid simple expression\"),\n",
    "    ('\"unclosed > 30', \"Unbalanced double quotes\"),\n",
    "    ('\"age\" > \\'30', \"Unbalanced single quotes\"),\n",
    "    (\"sum(if(fp(), 1, 0)\", \"Unbalanced parentheses\"),\n",
    "    (\"\\\"\\\" == 'value'\", \"Empty column reference\"),\n",
    "    ('\"status\" == \\'active\\' and \"verified\" == true', \"Valid complex expression\"),\n",
    "]\n",
    "\n",
    "print(\"FQL Syntax Validation:\\n\")\n",
    "for expr, description in test_expressions:\n",
    "    is_valid, error_msg = fql.validate_fql_syntax(expr)\n",
    "    status = \"‚úì\" if is_valid else \"‚úó\"\n",
    "    print(f\"{status} {description}\")\n",
    "    print(f\"  Expression: {expr}\")\n",
    "    if not is_valid:\n",
    "        print(f\"  Error: {error_msg}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Normalize Expressions\n",
    "\n",
    "The `normalize_expression()` function standardizes whitespace and formatting for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expressions with inconsistent formatting\n",
    "messy_expressions = [\n",
    "    '\"age\"   >  30',\n",
    "    '\"status\"==\"active\"',\n",
    "    \"sum(  if(  fp(  ),1,0)  )\",\n",
    "    \"\\\"region\\\"   in  ['West','East','North']\",\n",
    "]\n",
    "\n",
    "print(\"Expression Normalization:\\n\")\n",
    "for expr in messy_expressions:\n",
    "    normalized = fql.normalize_expression(expr)\n",
    "    print(f\"Original:    {expr}\")\n",
    "    print(f\"Normalized:  {normalized}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Extract FQL Functions\n",
    "\n",
    "The `get_fql_functions()` function identifies all function calls in an expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expressions with various functions\n",
    "function_examples = [\n",
    "    (\"sum(if(fp(), 1, 0))\", \"Custom metric with false positives\"),\n",
    "    ('avg(\"response_time\")', \"Simple average\"),\n",
    "    (\"count(if(\\\"status\\\" == 'failed', 1, 0))\", \"Conditional count\"),\n",
    "    ('sum(if(tp(), \"revenue\", 0)) - sum(if(fp(), \"cost\", 0))', \"Net value calculation\"),\n",
    "    (\n",
    "        '(sum(if((\"probability_churn\">0.8 and \"gender\"== \\'Nonbinary\\'), 1, 0))/sum(if((\"gender\"== \\'Nonbinary\\'), 1, 0)))/(sum(if((\"probability_churn\">0.8 and \"gender\"== \\'Male\\'), 1, 0))/sum(if((\"gender\"== \\'Male\\'), 1, 0)))',\n",
    "        \"Disparate Impact Non Binary\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(\"FQL Function Extraction:\\n\")\n",
    "for expr, description in function_examples:\n",
    "    functions = fql.get_fql_functions(expr)\n",
    "    print(f\"Description: {description}\")\n",
    "    print(f\"  Expression: {expr}\")\n",
    "    print(f\"  Functions: {functions}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Distinguish Simple Filters from Aggregations\n",
    "\n",
    "The `is_simple_filter()` function helps determine if an expression is a simple filter (usable in segments) or contains aggregations (typically for custom metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mix of simple and complex expressions\n",
    "classification_examples = [\n",
    "    ('\"age\" > 30 and \"status\" == \\'active\\'', \"Segment filter\"),\n",
    "    (\"sum(if(fp(), 1, 0))\", \"Custom metric\"),\n",
    "    (\"\\\"region\\\" in ['West', 'East']\", \"Segment filter with list\"),\n",
    "    ('avg(\"transaction_value\")', \"Aggregation metric\"),\n",
    "    ('if(\"premium\" == true, \"discount\", 0)', \"Conditional (no aggregation)\"),\n",
    "]\n",
    "\n",
    "print(\"Expression Classification:\\n\")\n",
    "for expr, description in classification_examples:\n",
    "    is_simple = fql.is_simple_filter(expr)\n",
    "    expr_type = \"Simple Filter\" if is_simple else \"Aggregation/Complex\"\n",
    "    icon = \"üìä\" if is_simple else \"üìà\"\n",
    "    print(f\"{icon} {description}\")\n",
    "    print(f\"  Type: {expr_type}\")\n",
    "    print(f\"  Expression: {expr}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Split AND Conditions\n",
    "\n",
    "The `split_fql_and_condition()` function breaks down complex filter expressions into individual conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex expressions with AND conditions\n",
    "complex_expr = '\"age\" > 30 and \"geography\" == \\'California\\' and \"credit_score\" >= 700'\n",
    "\n",
    "print(\"Splitting AND Conditions:\\n\")\n",
    "print(\"Original expression:\")\n",
    "print(f\"  {complex_expr}\\n\")\n",
    "\n",
    "parts = fql.split_fql_and_condition(complex_expr)\n",
    "print(f\"Split into {len(parts)} conditions:\\n\")\n",
    "for i, part in enumerate(parts, 1):\n",
    "    print(f\"  {i}. {part}\")\n",
    "\n",
    "# Note: This is a simple split and may not handle all cases\n",
    "print(\n",
    "    \"\\n‚ö†Ô∏è Note: Simple implementation - may not handle 'and' inside function calls correctly\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Live Fiddler Integration\n",
    "\n",
    "Connect to your Fiddler environment and analyze real FQL expressions from existing assets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Connect to Fiddler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Fiddler client\n",
    "if URL and TOKEN:\n",
    "    get_or_init(url=URL, token=TOKEN, log_level=\"ERROR\")\n",
    "    print(\"‚úì Connected to Fiddler\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Please set URL and TOKEN in the configuration section above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 List Available Projects and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all projects\n",
    "if URL and TOKEN:\n",
    "    projects = list(fdl.Project.list())\n",
    "    print(f\"Available Projects ({len(projects)}):\\n\")\n",
    "\n",
    "    for project in projects[:10]:  # Show first 10\n",
    "        try:\n",
    "            models = list(fdl.Model.list(project_id=project.id))\n",
    "            print(f\"üìÅ {project.name}\")\n",
    "            for model in models[:5]:  # Show first 5 models per project\n",
    "                print(f\"  ‚îî‚îÄ {model.name} (ID: {model.id})\")\n",
    "            if len(models) > 5:\n",
    "                print(f\"  ‚îî‚îÄ ... and {len(models) - 5} more models\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚îî‚îÄ Error listing models: {e}\")\n",
    "        print()\n",
    "\n",
    "    if len(projects) > 10:\n",
    "        print(f\"... and {len(projects) - 10} more projects\")\n",
    "\n",
    "    print(\"\\n‚ÑπÔ∏è Set PROJECT_NAME and MODEL_NAME above to focus on a specific model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Get Model and Analyze Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get specified model or use first available\n",
    "if URL and TOKEN:\n",
    "    if PROJECT_NAME and MODEL_NAME:\n",
    "        try:\n",
    "            project = fdl.Project.get_or_create(name=PROJECT_NAME)\n",
    "            model = fdl.Model.from_name(\n",
    "                name=MODEL_NAME, project_id=project.id, version=MODEL_VERSION\n",
    "            )\n",
    "            print(f\"‚úì Using model: {project.name}/{model.name}/{model.version}\")\n",
    "        except fdl.NotFound:\n",
    "            print(f\"‚úó Model not found: {PROJECT_NAME}/{MODEL_NAME}/{MODEL_VERSION}\")\n",
    "            print(\"  Using first available model instead...\")\n",
    "            model = None\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Error getting model: {e}\")\n",
    "            print(\"  Using first available model instead...\")\n",
    "            model = None\n",
    "    else:\n",
    "        model = None\n",
    "\n",
    "    # Fallback to first model with segments\n",
    "    if model is None:\n",
    "        for project in fdl.Project.list():\n",
    "            try:\n",
    "                models = list(fdl.Model.list(project_id=project.id))\n",
    "                for m in models:\n",
    "                    segments = list(fdl.Segment.list(model_id=m.id))\n",
    "                    if segments:\n",
    "                        model = m\n",
    "                        print(\n",
    "                            f\"‚úì Using model: {project.name}/{model.name} (found {len(segments)} segments)\"\n",
    "                        )\n",
    "                        break\n",
    "                if model:\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"  An error occurred: {e}\")\n",
    "                continue\n",
    "\n",
    "    if model is None:\n",
    "        print(\"‚ö†Ô∏è No models with segments found. Skipping live examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze segments from the model\n",
    "if URL and TOKEN and model:\n",
    "    try:\n",
    "        segments = list(fdl.Segment.list(model_id=model.id))\n",
    "\n",
    "        if segments:\n",
    "            print(f\"Analyzing {len(segments)} Segments:\\n\")\n",
    "\n",
    "            for segment in segments:\n",
    "                print(f\"üìä Segment: {segment.name}\")\n",
    "                print(f\"  Expression: {segment.definition}\")\n",
    "\n",
    "                # Extract columns\n",
    "                columns = fql.extract_columns(segment.definition)\n",
    "                print(f\"  Columns: {columns}\")\n",
    "\n",
    "                # Check if simple filter\n",
    "                is_simple = fql.is_simple_filter(segment.definition)\n",
    "                print(\n",
    "                    f\"  Type: {'Simple filter' if is_simple else 'Contains aggregations'}\"\n",
    "                )\n",
    "\n",
    "                # Validate syntax\n",
    "                is_valid, error = fql.validate_fql_syntax(segment.definition)\n",
    "                status = \"‚úì Valid\" if is_valid else f\"‚úó Invalid: {error}\"\n",
    "                print(f\"  Syntax: {status}\")\n",
    "\n",
    "                # Get functions used\n",
    "                functions = fql.get_fql_functions(segment.definition)\n",
    "                if functions:\n",
    "                    print(f\"  Functions: {functions}\")\n",
    "\n",
    "                print()\n",
    "        else:\n",
    "            print(\"‚ÑπÔ∏è No segments found for this model\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error analyzing segments: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Analyze Custom Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze custom metrics from the model\n",
    "if URL and TOKEN and model:\n",
    "    try:\n",
    "        custom_metrics = list(fdl.CustomMetric.list(model_id=model.id))\n",
    "\n",
    "        if custom_metrics:\n",
    "            print(f\"Analyzing {len(custom_metrics)} Custom Metrics:\\n\")\n",
    "\n",
    "            for metric in custom_metrics:\n",
    "                print(f\"üìà Custom Metric: {metric.name}\")\n",
    "                print(f\"  Definition: {metric.definition}\")\n",
    "\n",
    "                # Extract columns\n",
    "                columns = fql.extract_columns(metric.definition)\n",
    "                print(f\"  Columns: {columns}\")\n",
    "\n",
    "                # Get functions used\n",
    "                functions = fql.get_fql_functions(metric.definition)\n",
    "                print(f\"  Functions: {functions}\")\n",
    "\n",
    "                # Check if it's actually a simple filter (unusual for custom metrics)\n",
    "                is_simple = fql.is_simple_filter(metric.definition)\n",
    "                if is_simple:\n",
    "                    print(\"  ‚ö†Ô∏è No aggregations detected (unusual for custom metrics)\")\n",
    "\n",
    "                # Validate syntax\n",
    "                is_valid, error = fql.validate_fql_syntax(metric.definition)\n",
    "                status = \"‚úì Valid\" if is_valid else f\"‚úó Invalid: {error}\"\n",
    "                print(f\"  Syntax: {status}\")\n",
    "\n",
    "                print()\n",
    "        else:\n",
    "            print(\"‚ÑπÔ∏è No custom metrics found for this model\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error analyzing custom metrics: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Column Mapping and Asset Migration\n",
    "\n",
    "Transform FQL expressions when migrating assets between models with different schemas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Simple Column Name Replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Renaming columns in an expression\n",
    "original_expr = '\"age\" > 30 and \"geography\" == \\'California\\' and \"credit_score\" >= 700'\n",
    "\n",
    "# Define column mapping (old_name -> new_name)\n",
    "column_mapping = {\n",
    "    \"age\": \"customer_age\",\n",
    "    \"geography\": \"location\",\n",
    "    \"credit_score\": \"fico_score\",\n",
    "}\n",
    "\n",
    "print(\"Simple Column Name Replacement:\\n\")\n",
    "print(\"Original expression:\")\n",
    "print(f\"  {original_expr}\\n\")\n",
    "\n",
    "print(\"Column mapping:\")\n",
    "for old, new in column_mapping.items():\n",
    "    print(f\"  {old} ‚Üí {new}\")\n",
    "print()\n",
    "\n",
    "transformed_expr = fql.replace_column_names(original_expr, column_mapping)\n",
    "print(\"Transformed expression:\")\n",
    "print(f\"  {transformed_expr}\\n\")\n",
    "\n",
    "# Verify the transformation\n",
    "original_cols = fql.extract_columns(original_expr)\n",
    "transformed_cols = fql.extract_columns(transformed_expr)\n",
    "\n",
    "print(\"Verification:\")\n",
    "print(f\"  Original columns: {original_cols}\")\n",
    "print(f\"  Transformed columns: {transformed_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Complex Expression Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform custom metric with aggregations\n",
    "custom_metric_expr = 'sum(if(fp(), \"transaction_value\", 0)) / count(if(fp(), 1, 0))'\n",
    "\n",
    "metric_mapping = {\"transaction_value\": \"txn_amount\"}\n",
    "\n",
    "print(\"Complex Expression Transformation:\\n\")\n",
    "print(\"Original custom metric:\")\n",
    "print(f\"  {custom_metric_expr}\\n\")\n",
    "\n",
    "transformed_metric = fql.replace_column_names(custom_metric_expr, metric_mapping)\n",
    "print(\"Transformed custom metric:\")\n",
    "print(f\"  {transformed_metric}\\n\")\n",
    "\n",
    "# Verify functions are preserved\n",
    "original_functions = fql.get_fql_functions(custom_metric_expr)\n",
    "transformed_functions = fql.get_fql_functions(transformed_metric)\n",
    "\n",
    "print(\"Verification:\")\n",
    "print(f\"  Functions preserved: {original_functions == transformed_functions}\")\n",
    "print(f\"  Functions: {transformed_functions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Interactive Column Mapping Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to build column mapping between two models\n",
    "def build_column_mapping_interactive(\n",
    "    source_columns: Set[str], target_columns: Set[str]\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"Interactively build a column mapping between source and target schemas.\n",
    "\n",
    "    This is a simplified version for demonstration. In practice, you might:\n",
    "    - Use fuzzy matching to suggest mappings\n",
    "    - Allow user input for manual mapping\n",
    "    - Handle partial mappings\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "\n",
    "    # Exact matches (case-insensitive)\n",
    "    target_lower = {col.lower(): col for col in target_columns}\n",
    "\n",
    "    for source_col in source_columns:\n",
    "        if source_col.lower() in target_lower:\n",
    "            target_col = target_lower[source_col.lower()]\n",
    "            if source_col != target_col:\n",
    "                mapping[source_col] = target_col\n",
    "\n",
    "    return mapping\n",
    "\n",
    "\n",
    "# Example: Two models with similar but different schemas\n",
    "source_schema = {\"age\", \"geography\", \"credit_score\", \"income\", \"employment_status\"}\n",
    "target_schema = {\n",
    "    \"customer_age\",\n",
    "    \"location\",\n",
    "    \"fico_score\",\n",
    "    \"annual_income\",\n",
    "    \"employment_status\",\n",
    "}\n",
    "\n",
    "print(\"Interactive Column Mapping Builder:\\n\")\n",
    "print(f\"Source schema: {sorted(source_schema)}\")\n",
    "print(f\"Target schema: {sorted(target_schema)}\\n\")\n",
    "\n",
    "# Auto-detect exact matches\n",
    "auto_mapping = build_column_mapping_interactive(source_schema, target_schema)\n",
    "print(f\"Auto-detected mappings: {auto_mapping}\\n\")\n",
    "\n",
    "# Identify unmapped columns\n",
    "unmapped_source = source_schema - set(auto_mapping.keys()) - target_schema\n",
    "unmapped_target = target_schema - set(auto_mapping.values()) - source_schema\n",
    "\n",
    "print(f\"Unmapped source columns: {unmapped_source}\")\n",
    "print(f\"Unmapped target columns: {unmapped_target}\\n\")\n",
    "\n",
    "# Manual mapping for demonstration\n",
    "print(\"Suggested manual mappings:\")\n",
    "manual_mapping = {\n",
    "    \"age\": \"customer_age\",\n",
    "    \"geography\": \"location\",\n",
    "    \"credit_score\": \"fico_score\",\n",
    "    \"income\": \"annual_income\",\n",
    "}\n",
    "\n",
    "for source, target in manual_mapping.items():\n",
    "    print(f\"  {source} ‚Üí {target}\")\n",
    "\n",
    "# Combined mapping\n",
    "full_mapping = {**auto_mapping, **manual_mapping}\n",
    "print(f\"\\nFinal mapping: {full_mapping}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 End-to-End Migration Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete workflow: Migrate a segment from one model to another\n",
    "print(\"End-to-End Segment Migration Workflow:\\n\")\n",
    "\n",
    "# Source segment\n",
    "source_segment_name = \"High Risk Customers\"\n",
    "source_segment_expr = (\n",
    "    '\"age\" < 25 and \"credit_score\" < 650 and \"geography\" == \\'California\\''\n",
    ")\n",
    "\n",
    "print(\"Step 1: Source Segment\")\n",
    "print(f\"  Name: {source_segment_name}\")\n",
    "print(f\"  Expression: {source_segment_expr}\\n\")\n",
    "\n",
    "# Extract columns from source expression\n",
    "print(\"Step 2: Extract Column References\")\n",
    "source_cols = fql.extract_columns(source_segment_expr)\n",
    "print(f\"  Columns: {source_cols}\\n\")\n",
    "\n",
    "# Define target schema and mapping\n",
    "target_schema_cols = {\"customer_age\", \"fico_score\", \"location\", \"income_bracket\"}\n",
    "migration_mapping = {\n",
    "    \"age\": \"customer_age\",\n",
    "    \"credit_score\": \"fico_score\",\n",
    "    \"geography\": \"location\",\n",
    "}\n",
    "\n",
    "print(\"Step 3: Apply Column Mapping\")\n",
    "print(f\"  Target schema: {target_schema_cols}\")\n",
    "print(f\"  Mapping: {migration_mapping}\\n\")\n",
    "\n",
    "# Transform expression\n",
    "target_segment_expr = fql.replace_column_names(source_segment_expr, migration_mapping)\n",
    "print(\"Step 4: Transform Expression\")\n",
    "print(f\"  Transformed: {target_segment_expr}\\n\")\n",
    "\n",
    "# Validate against target schema\n",
    "print(\"Step 5: Validate Against Target Schema\")\n",
    "is_valid, missing_cols = fql.validate_column_references(\n",
    "    target_segment_expr, target_schema_cols\n",
    ")\n",
    "\n",
    "if is_valid:\n",
    "    print(\"  ‚úÖ Expression is valid for target model\")\n",
    "    print(\"\\nStep 6: Ready to Create Segment\")\n",
    "    if DRY_RUN:\n",
    "        print(\"  [DRY RUN] Would create segment:\")\n",
    "        print(f\"    Name: {source_segment_name}\")\n",
    "        print(f\"    Expression: {target_segment_expr}\")\n",
    "    else:\n",
    "        print(\"  Set DRY_RUN=False to create the segment\")\n",
    "else:\n",
    "    print(f\"  ‚úó Validation failed - missing columns: {missing_cols}\")\n",
    "    print(\"  Cannot proceed with migration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Batch Migration with Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Migrate multiple segments at once\n",
    "source_segments = [\n",
    "    {\"name\": \"High Risk\", \"expr\": '\"age\" < 25 and \"credit_score\" < 650'},\n",
    "    {\"name\": \"Premium Customers\", \"expr\": '\"income\" > 100000 and \"credit_score\" > 750'},\n",
    "    {\"name\": \"California Only\", \"expr\": \"\\\"geography\\\" == 'California'\"},\n",
    "]\n",
    "\n",
    "batch_mapping = {\n",
    "    \"age\": \"customer_age\",\n",
    "    \"credit_score\": \"fico_score\",\n",
    "    \"income\": \"annual_income\",\n",
    "    \"geography\": \"location\",\n",
    "}\n",
    "\n",
    "target_cols = {\"customer_age\", \"fico_score\", \"annual_income\", \"location\"}\n",
    "\n",
    "print(\"Batch Segment Migration:\\n\")\n",
    "\n",
    "results = []\n",
    "for segment in source_segments:\n",
    "    print(f\"Processing: {segment['name']}\")\n",
    "\n",
    "    # Transform\n",
    "    transformed = fql.replace_column_names(segment[\"expr\"], batch_mapping)\n",
    "\n",
    "    # Validate\n",
    "    is_valid, missing = fql.validate_column_references(transformed, target_cols)\n",
    "\n",
    "    result = {\n",
    "        \"name\": segment[\"name\"],\n",
    "        \"original\": segment[\"expr\"],\n",
    "        \"transformed\": transformed,\n",
    "        \"valid\": is_valid,\n",
    "        \"missing\": missing,\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "    status = \"‚úì\" if is_valid else \"‚úó\"\n",
    "    print(f\"  {status} Transformed: {transformed}\")\n",
    "    if not is_valid:\n",
    "        print(f\"    Missing: {missing}\")\n",
    "    print()\n",
    "\n",
    "# Summary\n",
    "valid_count = sum(1 for r in results if r[\"valid\"])\n",
    "print(\"Summary:\")\n",
    "print(f\"  Total segments: {len(results)}\")\n",
    "print(f\"  Valid after transformation: {valid_count}\")\n",
    "print(f\"  Failed validation: {len(results) - valid_count}\")\n",
    "\n",
    "if valid_count == len(results):\n",
    "    print(\"\\n‚úÖ All segments ready for migration!\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è {len(results) - valid_count} segment(s) need manual review\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Advanced Patterns and Best Practices\n",
    "\n",
    "Combining multiple utilities for robust FQL workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Advanced Patterns and Best Practices\n",
    "\n",
    "Combining multiple utilities for robust FQL workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Comprehensive Expression Validation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_fql_comprehensive(expression: str, valid_columns: Set[str]) -> Dict:\n",
    "    \"\"\"Run comprehensive validation on an FQL expression.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with validation results and metadata\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        \"expression\": expression,\n",
    "        \"checks\": {},\n",
    "        \"all_valid\": True,\n",
    "        \"warnings\": [],\n",
    "        \"metadata\": {},\n",
    "    }\n",
    "\n",
    "    # Check 1: Syntax validation\n",
    "    is_valid, error = fql.validate_fql_syntax(expression)\n",
    "    results[\"checks\"][\"syntax\"] = {\"valid\": is_valid, \"error\": error}\n",
    "    if not is_valid:\n",
    "        results[\"all_valid\"] = False\n",
    "\n",
    "    # Check 2: Column references\n",
    "    is_valid, missing = fql.validate_column_references(expression, valid_columns)\n",
    "    results[\"checks\"][\"columns\"] = {\"valid\": is_valid, \"missing\": missing}\n",
    "    if not is_valid:\n",
    "        results[\"all_valid\"] = False\n",
    "\n",
    "    # Metadata: Extract columns\n",
    "    columns = fql.extract_columns(expression)\n",
    "    results[\"metadata\"][\"columns\"] = list(columns)\n",
    "    results[\"metadata\"][\"column_count\"] = len(columns)\n",
    "\n",
    "    # Metadata: Extract functions\n",
    "    functions = fql.get_fql_functions(expression)\n",
    "    results[\"metadata\"][\"functions\"] = list(functions)\n",
    "    results[\"metadata\"][\"has_aggregations\"] = not fql.is_simple_filter(expression)\n",
    "\n",
    "    # Warning: Check for complexity\n",
    "    if len(columns) > 5:\n",
    "        results[\"warnings\"].append(f\"Complex expression with {len(columns)} columns\")\n",
    "\n",
    "    if len(functions) > 3:\n",
    "        results[\"warnings\"].append(f\"Multiple nested functions ({len(functions)})\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Test the pipeline\n",
    "test_schema = {\"age\", \"income\", \"credit_score\", \"status\", \"region\"}\n",
    "\n",
    "test_expressions = [\n",
    "    '\"age\" > 30 and \"status\" == \\'active\\'',\n",
    "    'sum(if(fp(), \"transaction_value\", 0))',  # Missing column\n",
    "    '\"age\" > 30 and \"income\" > 50000 and \"credit_score\" >= 700',\n",
    "]\n",
    "\n",
    "print(\"Comprehensive Validation Pipeline:\\n\")\n",
    "\n",
    "for expr in test_expressions:\n",
    "    print(f\"Expression: {expr}\")\n",
    "    results = validate_fql_comprehensive(expr, test_schema)\n",
    "\n",
    "    # Show results\n",
    "    overall = \"‚úÖ PASS\" if results[\"all_valid\"] else \"‚úó FAIL\"\n",
    "    print(f\"  Overall: {overall}\")\n",
    "\n",
    "    # Checks\n",
    "    for check_name, check_result in results[\"checks\"].items():\n",
    "        status = \"‚úì\" if check_result[\"valid\"] else \"‚úó\"\n",
    "        print(\n",
    "            f\"  {status} {check_name.title()}: {'Valid' if check_result['valid'] else check_result.get('error') or check_result.get('missing')}\"\n",
    "        )\n",
    "\n",
    "    # Metadata\n",
    "    print(\"  Metadata:\")\n",
    "    print(f\"    Columns: {results['metadata']['columns']}\")\n",
    "    print(f\"    Functions: {results['metadata']['functions']}\")\n",
    "    print(f\"    Has aggregations: {results['metadata']['has_aggregations']}\")\n",
    "\n",
    "    # Warnings\n",
    "    if results[\"warnings\"]:\n",
    "        print(\"  Warnings:\")\n",
    "        for warning in results[\"warnings\"]:\n",
    "            print(f\"    ‚ö†Ô∏è {warning}\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Expression Comparison and Deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use normalization to find duplicate expressions\n",
    "segment_expressions = [\n",
    "    '\"age\" > 30 and \"status\" == \\'active\\'',\n",
    "    '\"age\"   >   30   and   \"status\"   ==   \\'active\\'',  # Same, different whitespace\n",
    "    '\"status\" == \\'active\\' and \"age\" > 30',  # Different order, semantically same\n",
    "    '\"age\" > 25 and \"status\" == \\'active\\'',  # Actually different\n",
    "]\n",
    "\n",
    "print(\"Expression Comparison and Deduplication:\\n\")\n",
    "\n",
    "normalized_map = {}\n",
    "for i, expr in enumerate(segment_expressions, 1):\n",
    "    normalized = fql.normalize_expression(expr)\n",
    "\n",
    "    if normalized in normalized_map:\n",
    "        print(f\"Expression {i}: DUPLICATE of Expression {normalized_map[normalized]}\")\n",
    "    else:\n",
    "        print(f\"Expression {i}: UNIQUE\")\n",
    "        normalized_map[normalized] = i\n",
    "\n",
    "    print(f\"  Original:    {expr}\")\n",
    "    print(f\"  Normalized:  {normalized}\")\n",
    "    print()\n",
    "\n",
    "print(\n",
    "    f\"Summary: {len(normalized_map)} unique expressions out of {len(segment_expressions)} total\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Safe Expression Modification Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_column_replacement(\n",
    "    expression: str,\n",
    "    mapping: Dict[str, str],\n",
    "    target_schema: Set[str],\n",
    "    dry_run: bool = True,\n",
    ") -> Dict:\n",
    "    \"\"\"Safely replace column names with validation.\n",
    "\n",
    "    Args:\n",
    "        expression: Original FQL expression\n",
    "        mapping: Column name mapping (old -> new)\n",
    "        target_schema: Valid columns in target schema\n",
    "        dry_run: If True, only simulate the change\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with transformation results\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        \"original\": expression,\n",
    "        \"transformed\": None,\n",
    "        \"success\": False,\n",
    "        \"errors\": [],\n",
    "        \"warnings\": [],\n",
    "        \"dry_run\": dry_run,\n",
    "    }\n",
    "\n",
    "    # Step 1: Validate original syntax\n",
    "    is_valid, error = fql.validate_fql_syntax(expression)\n",
    "    if not is_valid:\n",
    "        result[\"errors\"].append(f\"Original expression has syntax error: {error}\")\n",
    "        return result\n",
    "\n",
    "    # Step 2: Extract columns from original\n",
    "    original_cols = fql.extract_columns(expression)\n",
    "\n",
    "    # Step 3: Check if all columns to be replaced exist\n",
    "    cols_to_replace = set(mapping.keys())\n",
    "    missing_in_expr = cols_to_replace - original_cols\n",
    "    if missing_in_expr:\n",
    "        result[\"warnings\"].append(\n",
    "            f\"Columns in mapping not found in expression: {missing_in_expr}\"\n",
    "        )\n",
    "\n",
    "    # Step 4: Apply transformation\n",
    "    transformed = fql.replace_column_names(expression, mapping)\n",
    "    result[\"transformed\"] = transformed\n",
    "\n",
    "    # Step 5: Validate transformed syntax\n",
    "    is_valid, error = fql.validate_fql_syntax(transformed)\n",
    "    if not is_valid:\n",
    "        result[\"errors\"].append(f\"Transformed expression has syntax error: {error}\")\n",
    "        return result\n",
    "\n",
    "    # Step 6: Validate against target schema\n",
    "    is_valid, missing = fql.validate_column_references(transformed, target_schema)\n",
    "    if not is_valid:\n",
    "        result[\"errors\"].append(\n",
    "            f\"Transformed expression references missing columns: {missing}\"\n",
    "        )\n",
    "        return result\n",
    "\n",
    "    # Step 7: Success!\n",
    "    result[\"success\"] = True\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Test safe replacement\n",
    "print(\"Safe Expression Modification Workflow:\\n\")\n",
    "\n",
    "test_expr = '\"age\" > 30 and \"credit_score\" >= 700'\n",
    "test_mapping = {\"age\": \"customer_age\", \"credit_score\": \"fico_score\"}\n",
    "test_target_schema = {\"customer_age\", \"fico_score\", \"location\", \"income\"}\n",
    "\n",
    "result = safe_column_replacement(\n",
    "    test_expr, test_mapping, test_target_schema, dry_run=DRY_RUN\n",
    ")\n",
    "\n",
    "print(f\"Original: {result['original']}\")\n",
    "print(f\"Transformed: {result['transformed']}\")\n",
    "print(f\"Status: {'‚úÖ SUCCESS' if result['success'] else '‚úó FAILED'}\")\n",
    "\n",
    "if result[\"errors\"]:\n",
    "    print(\"\\nErrors:\")\n",
    "    for error in result[\"errors\"]:\n",
    "        print(f\"  ‚úó {error}\")\n",
    "\n",
    "if result[\"warnings\"]:\n",
    "    print(\"\\nWarnings:\")\n",
    "    for warning in result[\"warnings\"]:\n",
    "        print(f\"  ‚ö†Ô∏è {warning}\")\n",
    "\n",
    "if result[\"dry_run\"] and result[\"success\"]:\n",
    "    print(\"\\n‚ÑπÔ∏è DRY RUN MODE - No changes made\")\n",
    "    print(\"   Set DRY_RUN=False to apply transformation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Expression Analysis Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_expression_complexity(expression: str) -> Dict:\n",
    "    \"\"\"Analyze FQL expression complexity and characteristics.\"\"\"\n",
    "    return {\n",
    "        \"length\": len(expression),\n",
    "        \"columns\": fql.extract_columns(expression),\n",
    "        \"column_count\": len(fql.extract_columns(expression)),\n",
    "        \"functions\": fql.get_fql_functions(expression),\n",
    "        \"function_count\": len(fql.get_fql_functions(expression)),\n",
    "        \"is_simple_filter\": fql.is_simple_filter(expression),\n",
    "        \"has_aggregations\": not fql.is_simple_filter(expression),\n",
    "        \"and_conditions\": len(fql.split_fql_and_condition(expression)),\n",
    "        \"complexity_score\": (\n",
    "            len(fql.extract_columns(expression)) * 1.0\n",
    "            + len(fql.get_fql_functions(expression)) * 2.0\n",
    "            + len(fql.split_fql_and_condition(expression)) * 0.5\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "# Analyze various expressions\n",
    "expressions_to_analyze = [\n",
    "    '\"age\" > 30',\n",
    "    '\"age\" > 30 and \"status\" == \\'active\\'',\n",
    "    \"\\\"age\\\" > 30 and \\\"status\\\" == 'active' and \\\"region\\\" in ['West', 'East']\",\n",
    "    \"sum(if(fp(), 1, 0))\",\n",
    "    'sum(if(fp(), \"value\", 0)) / count(if(tp(), 1, 0))',\n",
    "]\n",
    "\n",
    "print(\"Expression Complexity Analysis:\\n\")\n",
    "\n",
    "for expr in expressions_to_analyze:\n",
    "    analysis = analyze_expression_complexity(expr)\n",
    "\n",
    "    print(f\"Expression: {expr}\")\n",
    "    print(\n",
    "        f\"  Type: {'Simple Filter' if analysis['is_simple_filter'] else 'Aggregation/Metric'}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Columns: {analysis['column_count']} ({', '.join(analysis['columns']) if analysis['columns'] else 'none'})\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Functions: {analysis['function_count']} ({', '.join(analysis['functions']) if analysis['functions'] else 'none'})\"\n",
    "    )\n",
    "    print(f\"  AND conditions: {analysis['and_conditions']}\")\n",
    "    print(f\"  Complexity score: {analysis['complexity_score']:.1f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Critical Limitations & What These Utilities Don't Solve\n",
    "\n",
    "**While these utilities are powerful, it's important to understand their limitations:**\n",
    "\n",
    "### What is Solved ‚úÖ\n",
    "\n",
    "1. **Cross-Model Migration**\n",
    "   - Column name mapping and transformation\n",
    "   - Schema validation before migration\n",
    "   - Batch migration with error handling\n",
    "\n",
    "2. **Reference Management**\n",
    "   - Find all Charts/Alerts using a metric\n",
    "   - Safe metric updates with automatic reference migration\n",
    "   - Prevents broken dashboards\n",
    "\n",
    "3. **FQL Testing**\n",
    "   - Local syntax validation (fast pre-check)\n",
    "   - Real testing via temporary metrics\n",
    "   - Automatic cleanup\n",
    "\n",
    "### What is Not Solved ‚ùå\n",
    "\n",
    "1. **Core API Limitation: Metric Immutability**\n",
    "   - Custom Metrics still cannot be truly edited\n",
    "   - `safe_update_metric()` is a delete+recreate workaround\n",
    "   - The Fiddler API itself does not support metric modification\n",
    "\n",
    "2. **Dry-Run Limitations**\n",
    "   - Local validation **cannot** test Fiddler-specific functions:\n",
    "     - `tp()`, `fp()`, `tn()`, `fn()` - require prediction data\n",
    "     - `jsd()`, `psi()` - require baseline data in Fiddler\n",
    "     - Data integrity functions - require model spec\n",
    "   - Testing **must** create temporary metrics in Fiddler\n",
    "   - No true \"preview\" without API call\n",
    "\n",
    "3. **Reference Migration Caveats**\n",
    "   - Chart updates use unofficial API (may change)\n",
    "   - Alert recreation may reset notification configs\n",
    "   - Cannot rollback UUID changes after commit\n",
    "   - Some alert properties may not be preserved\n",
    "\n",
    "4. **Semantic Validation Gaps**\n",
    "   - Cannot detect division by zero\n",
    "   - Cannot validate data type compatibility\n",
    "   - Cannot check runtime performance\n",
    "   - Cannot estimate calculation cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **FQL Syntax Fundamentals**\n",
    "   * Column names in double quotes: `\"column_name\"`\n",
    "   * String values in single quotes: `'value'`\n",
    "   * Numeric values unquoted: `42`\n",
    "\n",
    "2. **Core Utility Functions**\n",
    "   * `extract_columns()` - Find all column references\n",
    "   * `validate_fql_syntax()` - Catch syntax errors\n",
    "   * `validate_column_references()` - Check schema compatibility\n",
    "   * `replace_column_names()` - Transform expressions for migration\n",
    "   * `normalize_expression()` - Standardize formatting\n",
    "   * `get_fql_functions()` - Identify functions used\n",
    "   * `is_simple_filter()` - Distinguish filters from aggregations\n",
    "   * `split_fql_and_condition()` - Break down complex conditions\n",
    "\n",
    "3. **Common Use Cases**\n",
    "   * **Asset migration:** Copy segments/metrics between models with different schemas\n",
    "   * **Validation:** Verify expressions before deployment\n",
    "   * **Analysis:** Understand expression complexity and dependencies\n",
    "   * **Deduplication:** Find semantically identical expressions\n",
    "\n",
    "4. **Best Practices**\n",
    "   * Always validate syntax before applying transformations\n",
    "   * Check schema compatibility after column name replacements\n",
    "   * Use dry-run mode for testing transformations\n",
    "   * Normalize expressions when comparing for equality\n",
    "   * Build comprehensive validation pipelines for production workflows\n",
    "\n",
    "### Common Gotchas\n",
    "\n",
    "* **Quote consistency:** Mixing single/double quotes will cause syntax errors\n",
    "* **Partial column name matches:** Use word boundaries in replacements to avoid partial matches\n",
    "* **Expression order:** `\"a\" and \"b\"` vs `\"b\" and \"a\"` are semantically same but string-different\n",
    "* **Function detection:** `split_fql_and_condition()` uses simple pattern matching and may not handle complex nested cases\n",
    "* **Schema validation:** Only checks if columns exist, not data types or value compatibility\n",
    "\n",
    "### When to Use Each Function\n",
    "\n",
    "| Function | Use When |\n",
    "|----------|----------|\n",
    "| `extract_columns()` | You need to know what data columns an expression depends on |\n",
    "| `validate_fql_syntax()` | Before saving expressions to catch obvious syntax errors |\n",
    "| `validate_column_references()` | Migrating assets or checking if expression will work on a model |\n",
    "| `replace_column_names()` | Copying assets between models with different column names |\n",
    "| `normalize_expression()` | Comparing expressions or finding duplicates |\n",
    "| `get_fql_functions()` | Analyzing expression complexity or checking for specific functions |\n",
    "| `is_simple_filter()` | Determining if an expression can be used as a segment filter |\n",
    "| `split_fql_and_condition()` | Breaking down complex filters into individual conditions |\n",
    "\n",
    "### Resources\n",
    "\n",
    "* **Fiddler FQL Documentation:** https://docs.fiddler.ai\n",
    "* **fiddler_utils package:** See `fiddler_utils/fql.py` for source code\n",
    "* **Additional utilities:** See `/misc-utils/README.md` for other helpful tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: UUID Reference Management & Safe Metric Updates\n",
    "\n",
    "‚ö†Ô∏è Custom Metrics cannot be modified once created. When you delete and recreate a metric (to \"update\" it), it gets a **new UUID**, which breaks all Charts and Alerts that reference the old UUID.\n",
    "\n",
    "This section demonstrates:\n",
    "1. The UUID breakage problem\n",
    "2. How to find all references before updating\n",
    "3. Safe update workflow with automatic reference migration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Import Reference Management Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import reference management utilities\n",
    "from fiddler_utils.assets.references import (\n",
    "    find_all_metric_references,\n",
    "    safe_update_metric,\n",
    ")\n",
    "\n",
    "print(\"‚úì Reference management utilities imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-60",
   "metadata": {},
   "source": [
    "### 5.2 Demonstrating the UUID Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the UUID breakage problem\n",
    "# NOTE: This is a demonstration - run only if you understand the impact!\n",
    "\n",
    "if URL and TOKEN and model and not DRY_RUN:\n",
    "    print(\"‚ö†Ô∏è WARNING: This demonstration will:\")\n",
    "    print(\"  1. Create a test custom metric\")\n",
    "    print(\"  2. Delete and recreate it (simulating an update)\")\n",
    "    print(\"  3. Show that the UUID changes\")\n",
    "    print()\n",
    "    print(\"Set DRY_RUN=True to skip this demonstration\")\n",
    "    print()\n",
    "    \n",
    "    # Create a test metric\n",
    "    test_metric = fdl.CustomMetric(\n",
    "        model_id=model.id,\n",
    "        name='__demo_uuid_problem',\n",
    "        description='TEST METRIC - Demonstrating UUID change on recreation',\n",
    "        definition='sum(if(fp(), 1, 0))'\n",
    "    )\n",
    "    test_metric.create()\n",
    "    \n",
    "    original_id = test_metric.id\n",
    "    print(\"‚úì Created metric '__demo_uuid_problem'\")\n",
    "    print(f\"  Original UUID: {original_id}\")\n",
    "    print()\n",
    "    \n",
    "    # Delete and recreate (simulating an update)\n",
    "    test_metric.delete()\n",
    "    print(\"Deleted metric...\")\n",
    "    print()\n",
    "    \n",
    "    # Recreate with same name\n",
    "    test_metric_v2 = fdl.CustomMetric(\n",
    "        model_id=model.id,\n",
    "        name='__demo_uuid_problem',\n",
    "        description='TEST METRIC - Recreated version',\n",
    "        definition='sum(if(fn(), 1, 0))'  # Changed definition\n",
    "    )\n",
    "    test_metric_v2.create()\n",
    "    \n",
    "    new_id = test_metric_v2.id\n",
    "    print(f\"‚úì Recreated metric with same name\")\n",
    "    print(f\"  New UUID: {new_id}\")\n",
    "    print()\n",
    "    \n",
    "    # Compare UUIDs\n",
    "    print(\"üî¥ PROBLEM DEMONSTRATED:\")\n",
    "    print(f\"  UUIDs are different: {original_id != new_id}\")\n",
    "    print(f\"  Any Charts/Alerts referencing {original_id} are now BROKEN\")\n",
    "    print()\n",
    "    \n",
    "    # Cleanup\n",
    "    test_metric_v2.delete()\n",
    "    print(\"‚úì Cleaned up test metric\")\n",
    "else:\n",
    "    print(\"DRY_RUN=True - Skipping UUID problem demonstration\")\n",
    "    print()\n",
    "    print(\"The Problem:\")\n",
    "    print(\"  1. You create a metric ‚Üí Gets UUID abc-123\")\n",
    "    print(\"  2. Charts and Alerts reference UUID abc-123\")\n",
    "    print(\"  3. You delete and recreate metric ‚Üí Gets NEW UUID xyz-789\")\n",
    "    print(\"  4. Charts and Alerts still reference abc-123 (BROKEN!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-64",
   "metadata": {},
   "source": [
    "### 5.3 Finding All References Before Updating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all references to a custom metric before updating it\n",
    "if URL and TOKEN and model:\n",
    "    # Get a custom metric (use first available)\n",
    "    custom_metrics = list(fdl.CustomMetric.list(model_id=model.id))\n",
    "    \n",
    "    if custom_metrics:\n",
    "        # Use first metric for demonstration\n",
    "        metric = custom_metrics[0]\n",
    "        \n",
    "        print(f\"Analyzing references to metric: {metric.name}\")\n",
    "        print(f\"  Metric ID: {metric.id}\")\n",
    "        print(f\"  Definition: {metric.definition}\")\n",
    "        print()\n",
    "        \n",
    "        # Find all references\n",
    "        try:\n",
    "            refs = find_all_metric_references(\n",
    "                metric_id=metric.id,\n",
    "                project_id=project.id,\n",
    "                url=URL,\n",
    "                token=TOKEN\n",
    "            )\n",
    "            \n",
    "            print(\"Reference Discovery Results:\")\n",
    "            print(f\"  Charts: {refs['chart_count']}\")\n",
    "            print(f\"  Alerts: {refs['alert_count']}\")\n",
    "            print(f\"  Total References: {refs['total_count']}\")\n",
    "            print()\n",
    "            \n",
    "            if refs['has_references']:\n",
    "                print(\"‚ö†Ô∏è WARNING: Deleting this metric will break:\")\n",
    "                \n",
    "                if refs['charts']:\n",
    "                    print(f\"\\n  Charts ({len(refs['charts'])}):\")\n",
    "                    for chart in refs['charts'][:5]:  # Show first 5\n",
    "                        print(f\"    - {chart.get('title', 'Untitled')}\")\n",
    "                    if len(refs['charts']) > 5:\n",
    "                        print(f\"    ... and {len(refs['charts']) - 5} more\")\n",
    "                \n",
    "                if refs['alerts']:\n",
    "                    print(f\"\\n  Alerts ({len(refs['alerts'])}):\")\n",
    "                    for alert in refs['alerts'][:5]:  # Show first 5\n",
    "                        print(f\"    - {alert.name}\")\n",
    "                    if len(refs['alerts']) > 5:\n",
    "                        print(f\"    ... and {len(refs['alerts']) - 5} more\")\n",
    "            else:\n",
    "                print(\"‚úì No references found - safe to delete/update\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error finding references: {e}\")\n",
    "            print(\"Note: Chart API may require url/token parameters\")\n",
    "    else:\n",
    "        print(\"No custom metrics found in this model\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Please set URL, TOKEN, and ensure model is loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-68",
   "metadata": {},
   "source": [
    "### 5.4 Safe Metric Update with Automatic Reference Migration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safely update a metric with automatic reference migration\n",
    "if URL and TOKEN and model and not DRY_RUN:\n",
    "    print(\"‚ö†Ô∏è This will update a real metric and migrate references\")\n",
    "    print(\"Set DRY_RUN=True to see the workflow without making changes\")\n",
    "    print()\n",
    "    \n",
    "    # For demonstration, create a test metric\n",
    "    print(\"Step 1: Creating test metric...\")\n",
    "    test_metric = fdl.CustomMetric(\n",
    "        model_id=model.id,\n",
    "        name='__demo_safe_update',\n",
    "        description='Test metric for safe update demonstration',\n",
    "        definition='sum(if(fp(), 1, 0))'\n",
    "    )\n",
    "    test_metric.create()\n",
    "    print(f\"  ‚úì Created metric (ID: {test_metric.id})\")\n",
    "    print()\n",
    "    \n",
    "    # Use safe update\n",
    "    print(\"Step 2: Updating metric with safe_update_metric()...\")\n",
    "    new_metric, report = safe_update_metric(\n",
    "        metric=test_metric,\n",
    "        new_definition='sum(if(fn(), 1, 0))',  # Changed to false negatives\n",
    "        auto_migrate=True,\n",
    "        project_id=project.id,\n",
    "        url=URL,\n",
    "        token=TOKEN\n",
    "    )\n",
    "    \n",
    "    print()\n",
    "    print(\"Update Complete!\")\n",
    "    print(f\"  Old ID: {report['old_metric_id']}\")\n",
    "    print(f\"  New ID: {report['new_metric_id']}\")\n",
    "    print(f\"  Metric Name: {report['metric_name']}\")\n",
    "    print()\n",
    "    print(\"Migration Results:\")\n",
    "    print(f\"  Migrated: {report['migrated_count']} references\")\n",
    "    print(f\"  Failed: {report['failed_count']} references\")\n",
    "    print()\n",
    "    \n",
    "    if report['chart_migrations']:\n",
    "        print(\"Chart Migrations:\")\n",
    "        for migration in report['chart_migrations']:\n",
    "            status = \"‚úì\" if migration['success'] else \"‚úó\"\n",
    "            print(f\"  {status} {migration['title']}\")\n",
    "    \n",
    "    if report['alert_migrations']:\n",
    "        print(\"Alert Migrations:\")\n",
    "        for migration in report['alert_migrations']:\n",
    "            status = \"‚úì\" if migration['success'] else \"‚úó\"\n",
    "            print(f\"  {status} {migration['name']}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    print()\n",
    "    print(\"Cleaning up test metric...\")\n",
    "    new_metric.delete()\n",
    "    print(\"‚úì Done\")\n",
    "    \n",
    "else:\n",
    "    print(\"DRY_RUN Mode - Safe Update Workflow:\")\n",
    "    print()\n",
    "    print(\"The safe_update_metric() function:\")\n",
    "    print(\"  1. Finds all Charts and Alerts referencing the metric\")\n",
    "    print(\"  2. Validates the new FQL definition\")\n",
    "    print(\"  3. Deletes the old metric\")\n",
    "    print(\"  4. Creates new metric with same name (gets new UUID)\")\n",
    "    print(\"  5. Updates ALL Charts to reference the new UUID\")\n",
    "    print(\"  6. Updates ALL Alerts to reference the new UUID\")\n",
    "    print(\"  7. Returns migration report\")\n",
    "    print()\n",
    "    print(\"Result: Metric is updated WITHOUT breaking dashboards!\")\n",
    "    print()\n",
    "    print(\"Example usage:\")\n",
    "    print(\"\"\"\n",
    "    new_metric, report = safe_update_metric(\n",
    "        metric=my_metric,\n",
    "        new_definition='sum(if(tp(), 1, 0))',\n",
    "        auto_migrate=True,\n",
    "        project_id=project.id,\n",
    "        url=URL,\n",
    "        token=TOKEN\n",
    "    )\n",
    "    \n",
    "    print(f\"Migrated {report['migrated_count']} references\")\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Testing FQL Before Creating Metrics\n",
    "\n",
    "**The Challenge:** Fiddler does not provide a \"validate\" or \"dry-run\" API for FQL. The ONLY way to know if your metric definition will work is to create it.\n",
    "\n",
    "**The Solution:** Use temporary metrics for testing:\n",
    "1. Local pre-validation (fast, catches syntax errors)\n",
    "2. Temporary metric testing in Fiddler (real validation)\n",
    "3. Automatic cleanup\n",
    "\n",
    "This approach lets you iterate on FQL definitions without polluting your metrics list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-69",
   "metadata": {},
   "source": [
    "### 6.1 Import Testing Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import FQL testing utilities\n",
    "from fiddler_utils.testing import (\n",
    "    batch_test_metrics,\n",
    "    cleanup_orphaned_test_metrics,\n",
    "    test_metric_definition,\n",
    "    validate_and_preview_metric,\n",
    "    validate_metric_syntax_local,\n",
    ")\n",
    "\n",
    "print(\"‚úì FQL testing utilities imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-73",
   "metadata": {},
   "source": [
    "### 6.2 Local Pre-Validation (Fast)\n",
    "\n",
    "Local validation catches obvious errors without making API calls. It's fast but limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast local validation\n",
    "if model:\n",
    "    # Test various FQL expressions locally\n",
    "    test_expressions = [\n",
    "        ('sum(if(fp(), 1, 0))', \"Valid aggregation\"),\n",
    "        ('sum(if(fp(), 1, 0)', \"Missing closing paren\"),\n",
    "        ('\"age\" > 30', \"Simple filter - not valid for custom metrics\"),\n",
    "        ('sum(if(fp(), \"nonexistent_column\", 0))', \"References missing column\"),\n",
    "    ]\n",
    "    \n",
    "    print(\"Local Pre-Validation Results:\\n\")\n",
    "    \n",
    "    for expr, description in test_expressions:\n",
    "        print(f\"Testing: {description}\")\n",
    "        print(f\"  Expression: {expr}\")\n",
    "        \n",
    "        result = validate_metric_syntax_local(expr, model)\n",
    "        \n",
    "        if result['valid']:\n",
    "            print(\"  ‚úì Passed local validation\")\n",
    "        else:\n",
    "            print(f\"  ‚úó Failed: {result['errors']}\")\n",
    "        \n",
    "        if result['has_warnings']:\n",
    "            print(f\"  ‚ö†Ô∏è  Warnings: {result['warnings']}\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    print(\"Note: Local validation only catches syntax and schema errors.\")\n",
    "    print(\"It cannot validate Fiddler-specific functions (tp(), fp(), jsd(), etc.)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Model not loaded - skipping local validation demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-77",
   "metadata": {},
   "source": [
    "### 6.3 Real Testing with Temporary Metrics\n",
    "\n",
    "The only way to truly validate FQL is to create a metric in Fiddler. We use temporary metrics with auto-cleanup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test FQL by creating temporary metric in Fiddler\n",
    "if URL and TOKEN and model and not DRY_RUN:\n",
    "    print(\"Testing FQL with Temporary Metrics\\n\")\n",
    "    \n",
    "    # Test 1: Valid metric\n",
    "    print(\"Test 1: Valid FQL expression\")\n",
    "    result = test_metric_definition(\n",
    "        model_id=model.id,\n",
    "        definition='sum(if(fp(), 1, 0))',\n",
    "        cleanup=True\n",
    "    )\n",
    "    \n",
    "    if result['valid']:\n",
    "        print(\"  ‚úì Expression is valid!\")\n",
    "        print(f\"    Temp metric created: {result['temp_metric_name']}\")\n",
    "        print(f\"    Cleaned up: {result['cleaned_up']}\")\n",
    "    else:\n",
    "        print(f\"  ‚úó Expression failed: {result['error']}\")\n",
    "    print()\n",
    "    \n",
    "    # Test 2: Invalid metric (syntax error)\n",
    "    print(\"Test 2: Invalid FQL (syntax error)\")\n",
    "    result = test_metric_definition(\n",
    "        model_id=model.id,\n",
    "        definition='sum(if(fp(), 1, 0',  # Missing closing paren\n",
    "        cleanup=True\n",
    "    )\n",
    "    \n",
    "    if result['valid']:\n",
    "        print(\"  ‚úì Expression is valid!\")\n",
    "    else:\n",
    "        print(f\"  ‚úó Expression failed (expected): {result['error']}\")\n",
    "    print()\n",
    "    \n",
    "    # Test 3: Invalid metric (Fiddler-specific error)\n",
    "    print(\"Test 3: FQL that might fail Fiddler validation\")\n",
    "    result = test_metric_definition(\n",
    "        model_id=model.id,\n",
    "        definition='sum(fp()) / 0',  # Division by zero\n",
    "        cleanup=True\n",
    "    )\n",
    "    \n",
    "    if result['valid']:\n",
    "        print(\"  ‚úì Expression passed Fiddler validation\")\n",
    "        print(\"    (Note: Fiddler may still accept this)\")\n",
    "    else:\n",
    "        print(f\"  ‚úó Expression failed: {result['error']}\")\n",
    "    \n",
    "else:\n",
    "    print(\"DRY_RUN Mode - Testing Workflow Explanation:\\n\")\n",
    "    print(\"test_metric_definition() workflow:\")\n",
    "    print(\"  1. Generate unique temp metric name (__test_<timestamp>_<random>)\")\n",
    "    print(\"  2. Create metric in Fiddler\")\n",
    "    print(\"  3. Fiddler validates the FQL definition\")\n",
    "    print(\"  4. If successful, return success\")\n",
    "    print(\"  5. If failed, capture error message\")\n",
    "    print(\"  6. Delete temp metric (cleanup)\")\n",
    "    print()\n",
    "    print(\"This is the ONLY way to truly validate FQL!\")\n",
    "    print(\"Local validation cannot test tp(), fp(), jsd(), etc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-81",
   "metadata": {},
   "source": [
    "### 6.4 Complete Validation Workflow\n",
    "\n",
    "Combine local and Fiddler testing for comprehensive validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete validation workflow\n",
    "if URL and TOKEN and model and not DRY_RUN:\n",
    "    print(\"Complete Validation Workflow\\n\")\n",
    "    \n",
    "    # Test a metric definition with both local and Fiddler validation\n",
    "    definition = 'sum(if(fp(), 1, 0)) / sum(1)'\n",
    "    \n",
    "    print(f\"Testing definition: {definition}\")\n",
    "    print()\n",
    "    \n",
    "    result = validate_and_preview_metric(\n",
    "        model_id=model.id,\n",
    "        definition=definition\n",
    "    )\n",
    "    \n",
    "    # Show local validation results\n",
    "    if result['local_validation']:\n",
    "        print(\"Step 1: Local Validation\")\n",
    "        local = result['local_validation']\n",
    "        if local['valid']:\n",
    "            print(\"  ‚úì Passed\")\n",
    "        else:\n",
    "            print(f\"  ‚úó Failed: {local['errors']}\")\n",
    "        \n",
    "        if local['has_warnings']:\n",
    "            print(f\"  ‚ö†Ô∏è  Warnings:\")\n",
    "            for warning in local['warnings']:\n",
    "                print(f\"    - {warning}\")\n",
    "        print()\n",
    "    \n",
    "    # Show Fiddler test results\n",
    "    if result['fiddler_test']:\n",
    "        print(\"Step 2: Fiddler Validation\")\n",
    "        fiddler = result['fiddler_test']\n",
    "        if fiddler['valid']:\n",
    "            print(\"  ‚úì Passed\")\n",
    "        else:\n",
    "            print(f\"  ‚úó Failed: {fiddler['error']}\")\n",
    "        print()\n",
    "    \n",
    "    # Show recommendation\n",
    "    print(\"Recommendation:\")\n",
    "    print(f\"  {result['recommendation']}\")\n",
    "    print()\n",
    "    \n",
    "    if result['valid']:\n",
    "        print(\"You can now safely create the real metric!\")\n",
    "        print(\"\"\"\n",
    "        metric = fdl.CustomMetric(\n",
    "            model_id=model.id,\n",
    "            name='fp_rate',\n",
    "            definition='{}',\n",
    "        )\n",
    "        metric.create()\n",
    "        \"\"\".format(definition))\n",
    "    \n",
    "else:\n",
    "    print(\"DRY_RUN Mode - Complete Workflow:\\n\")\n",
    "    print(\"validate_and_preview_metric() combines:\")\n",
    "    print(\"  1. Fast local validation (syntax, schema)\")\n",
    "    print(\"  2. Real Fiddler testing (creates temp metric)\")\n",
    "    print(\"  3. Automatic cleanup\")\n",
    "    print(\"  4. Clear recommendation\")\n",
    "    print()\n",
    "    print(\"Use this before creating any custom metric!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-85",
   "metadata": {},
   "source": [
    "### 6.5 Batch Testing Multiple Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multiple metric definitions at once\n",
    "if URL and TOKEN and model and not DRY_RUN:\n",
    "    print(\"Batch Testing Multiple Metrics\\n\")\n",
    "    \n",
    "    # Define multiple metrics to test\n",
    "    metric_definitions = [\n",
    "        {'name': 'FP Count', 'definition': 'sum(if(fp(), 1, 0))'},\n",
    "        {'name': 'FN Count', 'definition': 'sum(if(fn(), 1, 0))'},\n",
    "        {'name': 'TP Count', 'definition': 'sum(if(tp(), 1, 0))'},\n",
    "        {'name': 'Accuracy', 'definition': 'sum(if(tp() or tn(), 1, 0)) / sum(1)'},\n",
    "    ]\n",
    "    \n",
    "    # Batch test\n",
    "    results = batch_test_metrics(\n",
    "        model_id=model.id,\n",
    "        definitions=metric_definitions,\n",
    "        delay_between_tests=0.5\n",
    "    )\n",
    "    \n",
    "    # Show results\n",
    "    print(\"Batch Test Results:\\n\")\n",
    "    valid_count = sum(1 for r in results if r['valid'])\n",
    "    \n",
    "    for result in results:\n",
    "        status = \"‚úì\" if result['valid'] else \"‚úó\"\n",
    "        error_msg = f\" - {result['error']}\" if not result['valid'] else \"\"\n",
    "        print(f\"{status} {result['name']}{error_msg}\")\n",
    "    \n",
    "    print()\n",
    "    print(f\"Summary: {valid_count}/{len(results)} definitions are valid\")\n",
    "    print()\n",
    "    \n",
    "    if valid_count == len(results):\n",
    "        print(\"‚úÖ All definitions validated! Ready to create metrics.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Some definitions failed. Review errors above.\")\n",
    "    \n",
    "else:\n",
    "    print(\"DRY_RUN Mode - Batch Testing:\\n\")\n",
    "    print(\"batch_test_metrics() efficiently tests multiple definitions:\")\n",
    "    print(\"  - Creates/deletes temp metric for each\")\n",
    "    print(\"  - Adds delay between tests (avoid rate limiting)\")\n",
    "    print(\"  - Returns results for all definitions\")\n",
    "    print()\n",
    "    print(\"Use this when developing a suite of custom metrics!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-89",
   "metadata": {},
   "source": [
    "### 6.6 Cleanup Orphaned Test Metrics\n",
    "\n",
    "If testing was interrupted, temporary metrics may remain. Clean them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup any orphaned test metrics\n",
    "if URL and TOKEN and model and not DRY_RUN:\n",
    "    print(\"Cleaning Up Orphaned Test Metrics\\n\")\n",
    "    \n",
    "    deleted = cleanup_orphaned_test_metrics(model.id)\n",
    "    \n",
    "    if deleted > 0:\n",
    "        print(f\"‚úì Cleaned up {deleted} orphaned test metrics\")\n",
    "    else:\n",
    "        print(\"‚úì No orphaned test metrics found\")\n",
    "    \n",
    "else:\n",
    "    print(\"DRY_RUN Mode - Cleanup:\\n\")\n",
    "    print(\"cleanup_orphaned_test_metrics() finds and deletes:\")\n",
    "    print(\"  - All metrics starting with '__test_'\")\n",
    "    print(\"  - Leftover from interrupted testing\")\n",
    "    print()\n",
    "    print(\"Run this periodically to keep your metrics clean!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
