{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0c3549a",
   "metadata": {
    "id": "f0c3549a"
   },
   "source": [
    "# Monitoring a Multiclass Classifier Model for NLP Data with Fiddler\n",
    "Unstructured data such as text are usually represented as high-dimensional vectors when processed by ML models. In this example notebook we present how [Fiddler Vector Monitoring](https://www.fiddler.ai/blog/monitoring-natural-language-processing-and-computer-vision-models-part-1) can be used to monitor NLP models using a text classification use case.\n",
    "\n",
    "We use the 20Newsgroups dataset and train a multi-class classifier applied to TF-IDF embedding vectors of text data. Data preparation and model training is done using [this notebook](https://colab.research.google.com/drive/12RuC0LOhafF4KTkBvo3sCfV-IOvS6rp-?usp=sharing), while labeled data is imported. We monitor this model at production time and assess the performance of Fiddler's vector monitoring by manufacturing synthetc drift via sampling from specific text categories at different deployment time intervals.\n",
    "\n",
    "---\n",
    "\n",
    "Now we perform the following steps to demonstrate how Fiddler NLP monitoring works: \n",
    "\n",
    "1. Connect to Fiddler and Create a Project\n",
    "2. Upload the Assets\n",
    "3. Upload Baseline Data to Fiddler\n",
    "4. Add Metadata About the Model \n",
    "5. Manufacture Synthetic Data Drift and Publish Production Events\n",
    "6. Get insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6IvnT5Rpwpcy",
   "metadata": {
    "id": "6IvnT5Rpwpcy"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380bfb82",
   "metadata": {
    "id": "380bfb82"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sbUNWGfpZJrr",
   "metadata": {
    "id": "sbUNWGfpZJrr"
   },
   "source": [
    "# 1. Connect to Fiddler and Create a Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z1LoKQAouoDk",
   "metadata": {
    "id": "z1LoKQAouoDk"
   },
   "source": [
    "First we install and import the Fiddler Python client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TJKuEt6oZkUy",
   "metadata": {
    "id": "TJKuEt6oZkUy"
   },
   "outputs": [],
   "source": [
    "!pip install -q fiddler-client\n",
    "import fiddler as fdl\n",
    "print(f\"Running client version {fdl.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buFkZ_xkZgyN",
   "metadata": {
    "id": "buFkZ_xkZgyN"
   },
   "source": [
    "Before you can add information about your model with Fiddler, you'll need to connect using our API client.\n",
    "\n",
    "---\n",
    "\n",
    "**We need a few pieces of information to get started.**\n",
    "1. The URL you're using to connect to Fiddler\n",
    "2. Your organization ID\n",
    "3. Your authorization token\n",
    "\n",
    "The latter two of these can be found by pointing your browser to your Fiddler URL and navigating to the **Settings** page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41e2e8d",
   "metadata": {
    "id": "a41e2e8d"
   },
   "outputs": [],
   "source": [
    "URL = ''  # Make sure to include the full URL (including https://).\n",
    "ORG_ID = ''\n",
    "AUTH_TOKEN = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024b0fda",
   "metadata": {
    "id": "024b0fda"
   },
   "source": [
    "Next we run the following code block to connect to the Fiddler API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627e583c",
   "metadata": {
    "id": "627e583c"
   },
   "outputs": [],
   "source": [
    "client = fdl.FiddlerApi(\n",
    "    url=URL,\n",
    "    org_id=ORG_ID,\n",
    "    auth_token=AUTH_TOKEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c513f82",
   "metadata": {
    "id": "3c513f82"
   },
   "source": [
    "Once you connect, you can create a new project by specifying a unique project ID in the client's `create_project` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CWukS_D9aEU4",
   "metadata": {
    "id": "CWukS_D9aEU4"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = 'nlp_multiclass'\n",
    "\n",
    "if not PROJECT_ID in client.list_projects():\n",
    "    print(f'Creating project: {PROJECT_ID}')\n",
    "    client.create_project(PROJECT_ID)\n",
    "else:\n",
    "    print(f'Project: {PROJECT_ID} already exists')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1884a477",
   "metadata": {
    "id": "1884a477"
   },
   "source": [
    "# 2. Upload the Assets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kLTJ_yQ6z7Kk",
   "metadata": {
    "id": "kLTJ_yQ6z7Kk"
   },
   "source": [
    "Now we retrieve the 20Newsgroup dataset together with the TF-IDF embedding of each data point and the predicted class probabilities computed by a logistic regression classifier for five different class labels: 'computer', 'forsale' 'recreation', 'religion', and 'science'. The data is generated by [this notebook](https://colab.research.google.com/drive/12RuC0LOhafF4KTkBvo3sCfV-IOvS6rp-?usp=sharing) which stores two data frames; a baseline data frame and a production data frame. The production events in specific time intervals will be sampled from the production data frame where additional filters are applied to mimic the real-world scenarios (e.g., sampling from a particular news topic in an interval). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c45cddf",
   "metadata": {
    "id": "4c45cddf"
   },
   "outputs": [],
   "source": [
    "DATA_PATH = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/'\n",
    "\n",
    "baseline_df = pd.read_csv(DATA_PATH + '20newsgroups_baseline')\n",
    "production_df = pd.read_csv(DATA_PATH + '20newsgroups_production')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20028e2",
   "metadata": {
    "id": "c20028e2"
   },
   "outputs": [],
   "source": [
    "embedding_col_names = [col for col in baseline_df.columns if col.startswith('tfidf_token')]\n",
    "prediction_col_names= [col for col in baseline_df.columns if col.startswith('prob_')]\n",
    "target_classes      = [col[5:] for col in prediction_col_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zsQyrCi2lCw7",
   "metadata": {
    "id": "zsQyrCi2lCw7"
   },
   "source": [
    "# 3. Upload Baseline Data to Fiddler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Tdixub9oG3S9",
   "metadata": {
    "id": "Tdixub9oG3S9"
   },
   "source": [
    "Now we create a [DatasetInfo](https://docs.fiddler.ai/reference/fdldatasetinfo) object to describe our baseline dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84084cb6",
   "metadata": {
    "id": "84084cb6"
   },
   "outputs": [],
   "source": [
    "baseline_samples = baseline_df.copy()\n",
    "dataset_info = fdl.DatasetInfo.from_dataframe(baseline_samples, max_inferred_cardinality=20)\n",
    "dataset_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dagjbUlUKcXW",
   "metadata": {
    "id": "dagjbUlUKcXW"
   },
   "source": [
    "Next we call the [upload_dataset()](https://docs.fiddler.ai/reference/clientupload_dataset) API to upload a baseline  to Fiddler. In addition to the baseline data, we also uploaded the whole production data framework as the 'test_data' dataset which allows us to look at the model performance metrics for unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eS3ofgYrapRm",
   "metadata": {
    "id": "eS3ofgYrapRm"
   },
   "outputs": [],
   "source": [
    "DATASET_ID = 'newsgroups_baseline'\n",
    "\n",
    "if not DATASET_ID in client.list_datasets(project_id=PROJECT_ID):\n",
    "    print(f'Upload dataset {DATASET_ID}')\n",
    "    client.upload_dataset(\n",
    "    project_id=PROJECT_ID,\n",
    "    dataset_id=DATASET_ID,\n",
    "    dataset={'baseline':baseline_samples,\n",
    "             'test_data':production_df\n",
    "            },\n",
    "    info=dataset_info\n",
    ")\n",
    "else:\n",
    "    print(f'Dataset: {DATASET_ID} already exists in Project: {PROJECT_ID}.\\n'\n",
    "               'The new dataset is not uploaded. (please use a different name.)') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roikR8aTmfNE",
   "metadata": {
    "id": "roikR8aTmfNE"
   },
   "source": [
    "# 4. Add Metadata About the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1Ex_5KxtiHs5",
   "metadata": {
    "id": "1Ex_5KxtiHs5"
   },
   "source": [
    "Next we should tell Fiddler a bit more about our model by creating a [model_info](https://docs.fiddler.ai/reference/fdlmodelinfo) object that specifies the model's task, inputs, outputs, and other information such as the custom features and the targets for a multi-class classification model.\n",
    "\n",
    "While Fiddler monitores all the \n",
    "\n",
    "Let's first show how custom features can be used to monitor vectors such as the NLP embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bIWeThOD0XVh",
   "metadata": {
    "id": "bIWeThOD0XVh"
   },
   "source": [
    "### Define Custom Features for Vector Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QJv_IGSb0jCS",
   "metadata": {
    "id": "QJv_IGSb0jCS"
   },
   "source": [
    "In addition to univariate numerical features which Fiddler monitored by default, users can define custom features and ask Fiddler to monitor them. A custom feature is specified by a group of dataset columns that need to be monitored together as a vector.\n",
    "\n",
    "Before creating a model info object, we define a custom feature using the [CustomFeature.from_columns()](https://docs.fiddler.ai/reference/fdlcustomfeaturefrom_columns) API. When creating a custom feature, a name must be assigned to the custom feature using the `custom_name` argument. Each custom feature appears in the monitoring tab in Fiddler UI with this assigned name. Finally, the default clustering setup can be modified by passing the number of cluster centroids to the `n_clusters` argument.\n",
    "\n",
    "Here we define a custom feature that is a vector whose elements are the tf-idf embedding columns in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dlUZbFLya6rv",
   "metadata": {
    "id": "dlUZbFLya6rv"
   },
   "outputs": [],
   "source": [
    "CF1 = fdl.CustomFeature.from_columns(cols=embedding_col_names, n_clusters=6, custom_name='tfidf_vector')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30JODMh6mp1-",
   "metadata": {
    "id": "30JODMh6mp1-"
   },
   "source": [
    "### Generate ModelInfo Object and Add Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hRZNx26pwTye",
   "metadata": {
    "id": "hRZNx26pwTye"
   },
   "source": [
    "Since this notebook demonstrates a monitoring-only use case and model predictions are already added to both baseline and production data, there is no need to access the model directly or to build a surrogate model and we use the [add_model()](https://docs.fiddler.ai/reference/clientadd_model) API. This requires passing a [model_info](https://docs.fiddler.ai/reference/fdlmodelinfo) object which conitains information about our model's task, inputs, outputs, targets and custom features that we would like to be monitored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p8D9FUQObBnA",
   "metadata": {
    "id": "p8D9FUQObBnA"
   },
   "outputs": [],
   "source": [
    "model_task = fdl.ModelTask.MULTICLASS_CLASSIFICATION\n",
    "model_target = 'target'\n",
    "model_outputs = prediction_col_names\n",
    "model_features = embedding_col_names\n",
    "\n",
    "model_info = fdl.ModelInfo.from_dataset_info(\n",
    "    dataset_info=dataset_info,\n",
    "    dataset_id=DATASET_ID,\n",
    "    features=model_features,\n",
    "    target=model_target,\n",
    "    outputs=model_outputs,\n",
    "    custom_features = [CF1],\n",
    "    model_task=model_task,\n",
    "    categorical_target_class_details=target_classes,\n",
    "    metadata_cols=['original_text'],\n",
    "    description='A multi-class calssifier for NLP data that uses text embeddings'\n",
    ")\n",
    "model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "poOvXsXzb1rz",
   "metadata": {
    "id": "poOvXsXzb1rz"
   },
   "outputs": [],
   "source": [
    "MODEL_ID = 'multiclass_logistic_reg'\n",
    "\n",
    "if not MODEL_ID in client.list_models(project_id=PROJECT_ID):\n",
    "    client.add_model(\n",
    "        project_id=PROJECT_ID,\n",
    "        dataset_id=DATASET_ID,\n",
    "        model_id=MODEL_ID,\n",
    "        model_info=model_info\n",
    "    )\n",
    "else:\n",
    "    print(f'Model: {MODEL_ID} already exists in Project: {PROJECT_ID}. Please use a different name.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "L6L9yGBdcVFU",
   "metadata": {
    "id": "L6L9yGBdcVFU"
   },
   "source": [
    "# 5. Manufacture Synthetic Data Drift and Publish Production Events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uWB1dDYPnRSf",
   "metadata": {
    "id": "uWB1dDYPnRSf"
   },
   "source": [
    "Now we publish some production events into Fiddler. We publish events in data batches and manually create data drift by sampling from particular newsgroups. This allows us to test the performance of Fiddler vector monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8XjoPjatcfQq",
   "metadata": {
    "id": "8XjoPjatcfQq"
   },
   "outputs": [],
   "source": [
    "batch_size = 400 #number of events per time bin\n",
    "event_batches_df=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZS59y4c6ck8v",
   "metadata": {
    "id": "ZS59y4c6ck8v"
   },
   "source": [
    "For sanity check, we use the baseline data as the first event batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dXt8J0xciwG",
   "metadata": {
    "id": "8dXt8J0xciwG"
   },
   "outputs": [],
   "source": [
    "event_batches_df.append(baseline_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TKE-B2WMcoos",
   "metadata": {
    "id": "TKE-B2WMcoos"
   },
   "source": [
    "Next sample from all categories (same as baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddfKZrScsmo",
   "metadata": {
    "id": "2ddfKZrScsmo"
   },
   "outputs": [],
   "source": [
    "n_intervals = 3\n",
    "for i in range(n_intervals):\n",
    "    event_batches_df.append(production_df.sample(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VjO7AtctcxB-",
   "metadata": {
    "id": "VjO7AtctcxB-"
   },
   "source": [
    "Now we generate synthetic data drift by adding event batches that are sampled from specific newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yHDujd4Hc2E7",
   "metadata": {
    "id": "yHDujd4Hc2E7"
   },
   "outputs": [],
   "source": [
    "T1 = ['computer','science','recreation']\n",
    "T2 = ['science','recreation']\n",
    "T3 = ['recreation']\n",
    "T4 = ['computer','science','religion','forsale']\n",
    "T5 = ['forsale']\n",
    "synthetic_intervals = [T1,T2,T3,T4,T5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4m92NT0lc4BJ",
   "metadata": {
    "id": "4m92NT0lc4BJ"
   },
   "outputs": [],
   "source": [
    "for categories in synthetic_intervals:\n",
    "    production_df_subset = production_df[production_df['target'].isin(categories)]\n",
    "    event_batches_df.append(production_df_subset.sample(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XFFVfF8Fc6Nj",
   "metadata": {
    "id": "XFFVfF8Fc6Nj"
   },
   "source": [
    "Add more batches sampled from all categories (no data drift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QuLXDzpqc9Pz",
   "metadata": {
    "id": "QuLXDzpqc9Pz"
   },
   "outputs": [],
   "source": [
    "n_intervals = 3\n",
    "for i in range(n_intervals):\n",
    "    event_batches_df.append(production_df.sample(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3Mj18tq6dBwG",
   "metadata": {
    "id": "3Mj18tq6dBwG"
   },
   "source": [
    "### Add Timestamp to Batches and Publish Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eXQb_I0fdDmF",
   "metadata": {
    "id": "eXQb_I0fdDmF"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "daily_time_gap = 24*3600*1000 #daily time gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "K47yq6zBdIlp",
   "metadata": {
    "id": "K47yq6zBdIlp"
   },
   "outputs": [],
   "source": [
    "#start from 20 days back\n",
    "timestamp=time.time()*1000 - 20*daily_time_gap\n",
    "for event_df in event_batches_df:\n",
    "    event_df['timestamp'] = timestamp\n",
    "    timestamp += daily_time_gap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7QrQ_2KMdKZR",
   "metadata": {
    "id": "7QrQ_2KMdKZR"
   },
   "source": [
    "Publish events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197GcBxRdKGx",
   "metadata": {
    "id": "197GcBxRdKGx"
   },
   "outputs": [],
   "source": [
    "for event_df in event_batches_df:\n",
    "    client.publish_events_batch(\n",
    "        project_id=PROJECT_ID,\n",
    "        model_id=MODEL_ID,\n",
    "        batch_source=event_df,\n",
    "        timestamp_field= 'timestamp'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf936e3-407b-4f1e-9374-3072930c4dc4",
   "metadata": {
    "id": "fbf936e3-407b-4f1e-9374-3072930c4dc4"
   },
   "source": [
    "# 6. Get insights\n",
    "\n",
    "\n",
    "**You're all done!**\n",
    "  \n",
    "You can now head to your Fiddler environment and start getting enhanced observability into your model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qeV8BFLe9Pby",
   "metadata": {
    "id": "qeV8BFLe9Pby"
   },
   "source": [
    "In particular, you can go to charts and check the resulting drift chart for the TF-IDF embedding vectors. Below is a screenshot of the a data drift chart after running this notebook. The areas of higher drift correspond to the oversampling done above from certain newsgroup topics.  The areas of low drift correspond to the sampling done just from the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1941924f-25db-4a56-8fc8-ad7e972ea2d7",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/nlp_multiclass_1.png\" />\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
