{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0c3549a",
   "metadata": {
    "id": "f0c3549a"
   },
   "source": [
    "# A Notebook for Generating Labeled Data for a Multi-class NLP Classification\n",
    "This notebook generates some useful assets (data/model) that can be used by other examples and notebooks for demonstration and debugging of NLP use cases in Fiddler. In particular, we use the public 20Newsgroups dataset and apply a TF-IDF vectorization to find embedding vectors of text data. Then we split the data into training and test samples and apply a logistic regression model to predict the probability of each the target for each data point. To make the classification task simpler, We group the original targets into more general news categories. In the end, we concatenate all the results in a pandas DataFrame and store both the labeled training and labeled test data as CSV files. This data can be used as baseline and production data in Fiddler when model artifacts and surrogate models are not required. We also store the trained model as a pickle file, for scenarios where access to the model is also required.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "F3ZScdVsgYzo",
   "metadata": {
    "id": "F3ZScdVsgYzo"
   },
   "source": [
    "# Fetch the 20 Newsgroup Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yZh-dG6kdMoP",
   "metadata": {
    "id": "yZh-dG6kdMoP"
   },
   "source": [
    "First, we retrieve the 20Newsgroups dataset, which is available as part of the scikit-learn real-world dataset. This dataset contains around 18,000 newsgroup posts on 20 topics. The original dataset is available [here](http://qwone.com/~jason/20Newsgroups/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zCqhKtjrXFU1",
   "metadata": {
    "id": "zCqhKtjrXFU1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zEY0gOzyhrvY",
   "metadata": {
    "id": "zEY0gOzyhrvY"
   },
   "outputs": [],
   "source": [
    "data_bunch = fetch_20newsgroups(\n",
    "    subset = 'train',\n",
    "    shuffle=True,\n",
    "    random_state=1,\n",
    "    remove=('headers','footers','quotes')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hnphKST_e617",
   "metadata": {
    "id": "hnphKST_e617"
   },
   "source": [
    "A target name from 20 topics is assigned to each data sample in the above dataset, and you can access all the target names by running the: \n",
    "```\n",
    "data_bunch.target_names\n",
    "```\n",
    "However, to make this example notebook simpler, we group similar topics and define more general targets as the following:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "M83tz1P5W8Fp",
   "metadata": {
    "id": "M83tz1P5W8Fp"
   },
   "outputs": [],
   "source": [
    "subcategories = {\n",
    "    \n",
    "    'computer': ['comp.graphics',\n",
    "                 'comp.os.ms-windows.misc',\n",
    "                 'comp.sys.ibm.pc.hardware',\n",
    "                 'comp.sys.mac.hardware',\n",
    "                 'comp.windows.x'],\n",
    "    \n",
    "    'politics': ['talk.politics.guns',\n",
    "                 'talk.politics.mideast',\n",
    "                 'talk.politics.misc'],\n",
    "    \n",
    "    'recreation':['rec.autos',\n",
    "                  'rec.motorcycles',\n",
    "                  'rec.sport.baseball',\n",
    "                  'rec.sport.hockey'],\n",
    "    \n",
    "    'science': ['sci.crypt',\n",
    "                'sci.electronics',\n",
    "                'sci.med',\n",
    "                'sci.space',],\n",
    "    \n",
    "    'religion': ['soc.religion.christian',\n",
    "                 'talk.religion.misc',\n",
    "                 'alt.atheism'],\n",
    "    \n",
    "    'forsale':['misc.forsale']\n",
    "}\n",
    "\n",
    "main_category = {}\n",
    "for key,l in subcategories.items():\n",
    "    for item in l:\n",
    "        main_category[item] = key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TA9_uykyggmt",
   "metadata": {
    "id": "TA9_uykyggmt"
   },
   "source": [
    "Finally, we run some preprocessing and store the data in a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WcZh806fXKVK",
   "metadata": {
    "id": "WcZh806fXKVK"
   },
   "outputs": [],
   "source": [
    "data_prep = [s.replace('\\n',' ').strip('\\n,=,|,-, ,\\,^') for s in data_bunch.data]\n",
    "data_series = pd.Series(data_prep)\n",
    "df = pd.DataFrame()\n",
    "df['original_text'] = data_series\n",
    "df['original_target'] = [data_bunch.target_names[t] for t in data_bunch.target]\n",
    "df['target'] = [main_category[data_bunch.target_names[t]] for t in data_bunch.target]\n",
    "df['original_text'].replace('', np.nan, inplace=True)\n",
    "df.dropna(axis=0, subset=['original_text'], inplace=True)\n",
    "df = df[df.target!='politics'] #delete political posts \n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JFzXvnYSYCcT",
   "metadata": {
    "id": "JFzXvnYSYCcT"
   },
   "source": [
    "# TF-IDF *Vectorization*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vg781hoag6af",
   "metadata": {
    "id": "vg781hoag6af"
   },
   "source": [
    "Before training a model for predicting the targets, we transform the text data into a format that can be processed by standard ML models. This transformation step is often called \"vectorization\" and it is performed by embedding text data into high-dimensional vector space.  In this notebook, we use a simple TF-IDF vectorization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0FOIbZmeYP8a",
   "metadata": {
    "id": "0FOIbZmeYP8a"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f016bc6",
   "metadata": {
    "id": "2f016bc6"
   },
   "outputs": [],
   "source": [
    "embedding_dimension = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PNYxc5zrYSu1",
   "metadata": {
    "id": "PNYxc5zrYSu1"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(sublinear_tf=True,\n",
    "                             max_features=embedding_dimension,\n",
    "                             min_df=0.01,\n",
    "                             max_df=0.9,\n",
    "                             stop_words='english',\n",
    "                             token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b')\n",
    "\n",
    "tfidf_sparse = vectorizer.fit_transform(df['original_text'])\n",
    "embedding_cols = vectorizer.get_feature_names_out()\n",
    "embedding_col_names = ['tfidf_token_{}'.format(t) for t in embedding_cols]\n",
    "tfidf_df = pd.DataFrame.sparse.from_spmatrix(tfidf_sparse, columns=embedding_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed17dabb",
   "metadata": {
    "id": "ed17dabb",
    "outputId": "1cf73490-ee37-454d-88dc-9c07e501afdc"
   },
   "outputs": [],
   "source": [
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4UA5gdltnSb3",
   "metadata": {
    "id": "4UA5gdltnSb3"
   },
   "source": [
    "Now we concatenate the embedding representations and the DataFrame that we generated previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ulhYGfJqYXo6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 922
    },
    "id": "ulhYGfJqYXo6",
    "outputId": "a6e947c1-8965-44de-f4b0-df69f791561a"
   },
   "outputs": [],
   "source": [
    "df_all = pd.concat([df,tfidf_df], axis=1)\n",
    "df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RBVKIHG_YdrD",
   "metadata": {
    "id": "RBVKIHG_YdrD"
   },
   "source": [
    "# Train a Multiclass Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "einKU3xGoHVF",
   "metadata": {
    "id": "einKU3xGoHVF"
   },
   "source": [
    "We are now ready to train a classifier to predict the labels assigned to each data sample. We use the logistic regression classifier from scikit-learn for this task. We split the data into train and test subsets and we use 25% of data points to train a logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ss9nv7SEYnEO",
   "metadata": {
    "id": "ss9nv7SEYnEO"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JhFqymKoYrDM",
   "metadata": {
    "id": "JhFqymKoYrDM"
   },
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df_all, test_size=0.75, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5151ab9",
   "metadata": {
    "id": "c5151ab9"
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=1).fit(df_train[embedding_col_names], df_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Rog_WBnmYt7k",
   "metadata": {
    "id": "Rog_WBnmYt7k"
   },
   "outputs": [],
   "source": [
    "clf_classes = clf.classes_\n",
    "prob_col_names = ['prob_%s'%c for c in clf_classes]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Y2UbdtqRpXlU",
   "metadata": {
    "id": "Y2UbdtqRpXlU"
   },
   "source": [
    "Using the logistic regression classifier for a multi-class classification problem, we get a probability for each target label. We store all the predicted class probabilities as well as the predicted target for each data point in the training and test sets and we compute the prediction accuracy in each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4Dl64dFFYwQz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Dl64dFFYwQz",
    "outputId": "9a885fa7-03ce-48fc-d9ef-dcd32e10d663"
   },
   "outputs": [],
   "source": [
    "predictions_df_train = pd.DataFrame(index=df_train.index)\n",
    "predictions_df_train['predicted_target'] = clf.predict(df_train[embedding_col_names])\n",
    "predicted_probs = clf.predict_proba(df_train[embedding_col_names])\n",
    "for idx,col in enumerate(predicted_probs.T):\n",
    "    predictions_df_train[prob_col_names[idx]] = col\n",
    "baseline_df = pd.concat([predictions_df_train, df_train], axis=1)\n",
    "acc_baseline = sum(baseline_df['predicted_target'] == baseline_df['target'])/baseline_df.shape[0]\n",
    "print('accuracy on baseline:{:.2f}'.format(acc_baseline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plwP-HS4YyJ5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "plwP-HS4YyJ5",
    "outputId": "c74ac33f-5aba-4979-e53f-32a0cc25f21d"
   },
   "outputs": [],
   "source": [
    "predictions_df_test = pd.DataFrame(index=df_test.index)\n",
    "predictions_df_test['predicted_target'] = clf.predict(df_test[embedding_col_names])\n",
    "predicted_probs = clf.predict_proba(df_test[embedding_col_names])\n",
    "for idx,col in enumerate(predicted_probs.T):\n",
    "    predictions_df_test[prob_col_names[idx]] = col\n",
    "production_df = pd.concat([predictions_df_test, df_test], axis=1)\n",
    "acc_production = sum(production_df['predicted_target'] == production_df['target'])/production_df.shape[0]\n",
    "print('accuracy on test data:{:.2f}'.format(acc_production))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef407c52",
   "metadata": {
    "id": "ef407c52"
   },
   "source": [
    "# Store Data and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbfce5d",
   "metadata": {
    "id": "efbfce5d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "baseline_df.to_csv('20newsgroups_baseline.csv',index=False)\n",
    "production_df.to_csv('20newsgroups_production.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e148ae-7ddb-4466-aabd-5fb81059864d",
   "metadata": {},
   "outputs": [],
   "source": [
    "production_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7c3cb5",
   "metadata": {
    "id": "7d7c3cb5"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'LogisticRegression_clf'\n",
    "pickle.dump(clf, open(filename, 'wb')) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "dcc2ed42-2cd9-424c-bc04-f8fcd3c01a69",
    "F3ZScdVsgYzo",
    "JFzXvnYSYCcT",
    "RBVKIHG_YdrD",
    "sbUNWGfpZJrr",
    "roikR8aTmfNE",
    "30JODMh6mp1-"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
