{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4HCMkMtvGu9"
   },
   "source": [
    "# Fiddler Ranking Model Quick Start Guide\n",
    "\n",
    "Fiddler offer the ability for your teams to observe you ranking models to understand thier performance and catch issues like data drift before they affect your applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1yVGAH2vGvA"
   },
   "source": [
    "# Quickstart: Expedia Search Ranking\n",
    "The following dataset is coming from Expedia. It includes shopping and purchase data as well as information on price competitiveness. The data are organized around a set of “search result impressions”, or the ordered list of hotels that the user sees after they search for a hotel on the Expedia website. In addition to impressions from the existing algorithm, the data contain impressions where the hotels were randomly sorted, to avoid the position bias of the existing algorithm. The user response is provided as a click on a hotel. From: https://www.kaggle.com/c/expedia-personalized-sort/overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "583iLl9lvGvA"
   },
   "source": [
    "# 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OoDdHCPMvGvC"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import time as time\n",
    "import datetime\n",
    "import fiddler as fdl\n",
    "print(f\"Running Fiddler client version {fdl.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6f-rK__zvGvC"
   },
   "source": [
    "# 1. Connect to Fiddler and Create a Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "trtTWu6dvGvD"
   },
   "source": [
    "Before you can add information about your model with Fiddler, you'll need to connect using our API client.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**We need a few pieces of information to get started.**\n",
    "1. The URL you're using to connect to Fiddler\n",
    "2. Your authorization token\n",
    "\n",
    "These can be found by navigating to the **Settings** page of your Fiddler environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cUBetAWYvGvD"
   },
   "outputs": [],
   "source": [
    "URL = 'https://mainbuild.dev.fiddler.ai'  # Make sure to include the full URL (including https://).\n",
    "TOKEN = 'IkX6tatV_HrrxqoLO4q3rcaKUAPIK2jGk-oI4X4LAQ8'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Q6RX3UwvGvE"
   },
   "source": [
    "Next we use these credentials to connect to the Fiddler API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rho39PQcvGvE"
   },
   "outputs": [],
   "source": [
    "fdl.init(\n",
    "    url=URL,\n",
    "    token=TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CtJV4iIJvGvE"
   },
   "source": [
    "Once you connect, you can create a new project by specifying a unique project ID in the client's `Project.create` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m7V2wpm9vGvE"
   },
   "outputs": [],
   "source": [
    "PROJECT_NAME = 'search_ranking_example_004'\n",
    "\n",
    "project = fdl.Project(\n",
    "    name=PROJECT_NAME\n",
    ")\n",
    "\n",
    "project.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OiXy5tCwvGvF"
   },
   "source": [
    "# 2. Load a Data Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zb2QlLEevGvF"
   },
   "source": [
    "Now we retrieve the Expedia Dataset as a data sample for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vsjVExV9vGvF"
   },
   "outputs": [],
   "source": [
    "PATH_TO_SAMPLE_CSV = 'https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/v3/expedia_data_sample.csv'\n",
    "\n",
    "sample_df = pd.read_csv(PATH_TO_SAMPLE_CSV)\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UiMAH9QUvGvF"
   },
   "source": [
    "Fiddler uses this data sample to keep track of important information about your data.\n",
    "  \n",
    "This includes **data types**, **data ranges**, and **unique values** for categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I46fM8ybvGvG"
   },
   "source": [
    "# 3. Onboard Model Info and Upload the Model Artifact to Fiddler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "491n8K8ovGvG"
   },
   "outputs": [],
   "source": [
    "#create model directory to store your model files\n",
    "import os\n",
    "model_dir = \"model\"\n",
    "os.makedirs(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5h4cDewzvGvG"
   },
   "source": [
    "### 3.a Adding model info to Fiddler\n",
    "To add a Ranking model you must specify the ModelTask as `RANKING` in the model info object.  \n",
    "\n",
    "Additionally, you must provide the `group_by` argument that corresponds to the query search id. This `group_by` column should be present either in:\n",
    "- `features` : if it is used to build and run the model\n",
    "- `metadata_cols` : if not used by the model\n",
    "\n",
    "Optionally, you can give a `ranking_top_k` number (default is 50). This will be the number of results within each query to take into account while computing the performance metrics in monitoring.  \n",
    "\n",
    "Unless the prediction column was part of your baseline dataset, you must provide the minimum and maximum values predictions can take in a dictionary format (see below).  \n",
    "\n",
    "If your target is categorical (string), you need to provide the `target_class_order` argument. If your target is numerical and you don't specify this argument, Fiddler will infer it.   \n",
    "\n",
    "This will be the list of possible values for the target **ordered**. The first element should be the least relevant target level, the last element should be the most relevant target level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spec = fdl.ModelSpec(\n",
    "    inputs=list(sample_df.drop(columns=['binary_relevance', 'score', 'graded_relevance', 'position', 'timestamp']).columns),\n",
    "    outputs=['score'],\n",
    "    targets=['binary_relevance'],\n",
    "    metadata=['timestamp', 'graded_relevance', 'position']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_task = fdl.ModelTask.RANKING\n",
    "\n",
    "task_params = fdl.ModelTaskParams(\n",
    "    group_by='srch_id',\n",
    "    top_k=20,\n",
    "    target_class_order=[0, 1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_column = 'timestamp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'expedia_model'\n",
    "\n",
    "model = fdl.Model.from_data(\n",
    "    name=MODEL_NAME,\n",
    "    project_id=project.id,\n",
    "    source=sample_df,\n",
    "    spec=model_spec,\n",
    "    task=model_task,\n",
    "    task_params=task_params,\n",
    "    event_ts_col=timestamp_column\n",
    ")\n",
    "\n",
    "model.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Upload our data sample as a baseline dataset\n",
    "\n",
    "In order to add a model artifact, we need a baseline first.\n",
    "A baseline is a dataset which can be used to represent \"golden data,\" or data which our model expects to receive in production.\n",
    "\n",
    "We can publush the data sample from earlier to add it as a baseline.\n",
    "\n",
    "For ranking, we need to ingest all events from a given query or search ID together. To do that, we need to transform the data to a grouped format.  \n",
    "You can use the `group_by` utility function to do the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = 'expedia_dataset'\n",
    "\n",
    "sample_df_grouped = fdl.utils.helpers.group_by(df=sample_df, group_by_col='srch_id')\n",
    "\n",
    "model.publish(\n",
    "    source=sample_df_grouped,\n",
    "    environment=fdl.EnvType.PRE_PRODUCTION,\n",
    "    dataset_name=DATASET_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Upload a model artifact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wryEH6LMvGvH"
   },
   "source": [
    "### 5.a Create a Model Wrapper Script\n",
    "\n",
    "Package.py is the interface between Fiddler’s backend and your model. This wrapper script helps Fiddler to understand how to load the model, how to run the model, and what its inputs and outputs are.\n",
    "\n",
    "You need to implement three parts:\n",
    "- init: Load the model, and any associated files such as feature transformers.\n",
    "- transform: If you use some pre-processing steps not part of the model file, transform the data into a format that the model recognizes.\n",
    "- predict: Make predictions using the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QZqb2ll-vGvH"
   },
   "outputs": [],
   "source": [
    "%%writefile model/package.py\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "PACKAGE_PATH = Path(__file__).parent\n",
    "\n",
    "class ModelPackage:\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "         Load the model file and any pre-processing files if needed.\n",
    "        \"\"\"\n",
    "        self.output_columns = ['score']\n",
    "\n",
    "        with open(PACKAGE_PATH / 'model.pkl', 'rb') as infile:\n",
    "            self.model = pickle.load(infile)\n",
    "\n",
    "    def transform(self, input_df):\n",
    "        \"\"\"\n",
    "        Accepts a pandas DataFrame object containing rows of raw feature vectors.\n",
    "        Use pre-processing file to transform the data if needed.\n",
    "        In this example we don't need to transform the data.\n",
    "        Outputs a pandas DataFrame object containing transformed data.\n",
    "        \"\"\"\n",
    "        return input_df\n",
    "\n",
    "    def predict(self, input_df):\n",
    "        \"\"\"\n",
    "        Accepts a pandas DataFrame object containing rows of raw feature vectors.\n",
    "        Outputs a pandas DataFrame object containing the model predictions whose column labels\n",
    "        must match the output column names in model info.\n",
    "        \"\"\"\n",
    "        transformed_df = self.transform(input_df)\n",
    "        pred = self.model.predict(transformed_df)\n",
    "        return pd.DataFrame(pred, columns=self.output_columns)\n",
    "\n",
    "def get_model():\n",
    "    return ModelPackage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ip1XQb_xvGvH"
   },
   "source": [
    "### 5.b Retriving the model files\n",
    "\n",
    "To explain a model's inner workigs we need to upload the model artifacts. We will retrive a pre-trained model from the Fiddler Repo that was trained with **lightgbm 2.3.0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-_cFPp_kvGvH"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/models/model_ranking.pkl\", \"model/model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uliL8RBMvGvI"
   },
   "source": [
    "### 5.c Upload the model files to Fiddler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wARXuDVvGvI"
   },
   "source": [
    "\n",
    "Now as a final step in the setup you can upload the model artifact files using `add_model_artifact`.\n",
    "   - The `model_dir` is the path for the folder containing the model file(s) and the `package.py` from ther last step.\n",
    "   - Since each model artifact uploaded to Fiddler gets deployed in its own container, the [deployment params](https://docs.fiddler.ai/reference/fdldeploymentparams) allow us to specify the compute needs and library set of the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vgdy1RQ-vGvI"
   },
   "outputs": [],
   "source": [
    "#Uploading Model files\n",
    "deployment_params = fdl.DeploymentParams(\n",
    "    image_uri=\"md-base/python/machine-learning:1.1.0\",\n",
    "    cpu=100,\n",
    "    memory=256,\n",
    "    replicas=1,\n",
    ")\n",
    "\n",
    "model.add_artifact(\n",
    "    model_dir=model_dir,\n",
    "    deployment_params=deployment_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6B8Vly1JvGvI"
   },
   "source": [
    "# 6. Publish Events For Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2Cznqy5vGvI"
   },
   "source": [
    "### 6.a Gather and prepare Production Events\n",
    "This is the production log file we are going to upload in Fiddler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "doqcM3ZevGvI"
   },
   "outputs": [],
   "source": [
    "df_logs = pd.read_csv('https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/data/v3/expedia_logs.csv')\n",
    "df_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Vgh6rDevGvI"
   },
   "outputs": [],
   "source": [
    "#timeshift the data to be current day\n",
    "df_logs['timestamp'] = df_logs['timestamp'] + (float(time.time()) - df_logs['timestamp'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DLRNuT5vGvJ"
   },
   "source": [
    "Again, let's group the data before sending it to Fiddler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cF_V6FZ-vGvJ"
   },
   "outputs": [],
   "source": [
    "df_logs_grouped = fdl.utils.helpers.group_by(df=df_logs, group_by_col='srch_id')\n",
    "df_logs_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYzNsNDNvGvJ"
   },
   "source": [
    "### 6.b Publish events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.publish(df_logs_grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNtMwctovGvO"
   },
   "source": [
    "# 7. Get insights\n",
    "\n",
    "\n",
    "**You're all done!**\n",
    "  \n",
    "You can now head to your Fiddler environment and start getting enhanced observability into your model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXyjlZNgvGvO"
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"https://raw.githubusercontent.com/fiddler-labs/fiddler-examples/main/quickstart/images/ranking_model_1.png\" />\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHvq62t4vGvO"
   },
   "source": [
    "--------\n",
    "**Questions?**  \n",
    "  \n",
    "Check out [our docs](https://docs.fiddler.ai/) for a more detailed explanation of what Fiddler has to offer.\n",
    "\n",
    "Join our [community Slack](http://fiddler-community.slack.com/) to ask any questions!\n",
    "\n",
    "If you're still looking for answers, fill out a ticket on [our support page](https://fiddlerlabs.zendesk.com/) and we'll get back to you shortly."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
