{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Custom Judge Evaluators\n",
    "\n",
    "This quickstart shows you how to create custom LLM-as-a-Judge evaluators using Fiddler's `CustomJudge` class.\n",
    "\n",
    "## What is CustomJudge?\n",
    "\n",
    "The `CustomJudge` evaluator lets you define arbitrary evaluation criteria by specifying:\n",
    "- A **prompt template** with `{{ placeholder }}` syntax for dynamic content\n",
    "- **Output fields** that define the structured response you expect from the LLM\n",
    "\n",
    "This is the most flexible evaluator in the Fiddler Evals SDK, enabling you to build domain-specific evaluation logic without writing custom code.\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "| Use Case | Example |\n",
    "|----------|--------|\n",
    "| **Classification** | Categorize text into predefined labels |\n",
    "| **Custom Rubrics** | Implement grading rubrics with specific criteria |\n",
    "| **Multi-Aspect Scoring** | Evaluate multiple dimensions (tone, accuracy, helpfulness) |\n",
    "| **Compliance Checking** | Verify responses meet specific guidelines |\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- A Fiddler account with API access\n",
    "- An LLM credential configured in **Settings > LLM Gateway**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "install-header",
   "metadata": {},
   "source": [
    "## 1. Install and Connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q fiddler-evals pandas\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from fiddler_evals import init\n",
    "from fiddler_evals.evaluators import CustomJudge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fiddler connection\n",
    "URL = ''  # e.g., 'https://your-org.fiddler.ai'\n",
    "TOKEN = ''  # From Settings > Credentials\n",
    "\n",
    "# LLM Gateway (from Settings > LLM Gateway)\n",
    "LLM_CREDENTIAL_NAME = 'Fiddler Credential'  # Change as needed for 3rd party providers\n",
    "LLM_MODEL_NAME = 'fiddler/llama3.1-8b'\n",
    "\n",
    "# Connect to Fiddler\n",
    "init(url=URL, token=TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-header",
   "metadata": {},
   "source": [
    "## 3. Sample Data\n",
    "\n",
    "We'll classify news article summaries into one of four topic categories:\n",
    "\n",
    "| Topic | Description |\n",
    "|-------|-------------|\n",
    "| **World** | Global news and international events |\n",
    "| **Sports** | Sports events and athletes |\n",
    "| **Business** | Business news (non-tech industries) |\n",
    "| **Sci/Tech** | Science and technology news |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample news articles for topic classification\n",
    "data = [\n",
    "    {\n",
    "        'text': 'Google announces new AI chip designed to accelerate machine learning workloads in data centers.',\n",
    "        'ground_truth': 'Sci/Tech',\n",
    "    },\n",
    "    {\n",
    "        'text': 'The Lakers defeated the Celtics 112-108 in overtime, with LeBron James scoring 35 points.',\n",
    "        'ground_truth': 'Sports',\n",
    "    },\n",
    "    {\n",
    "        'text': 'Oil prices surged 5% following OPEC decision to cut production by 2 million barrels per day.',\n",
    "        'ground_truth': 'Business',\n",
    "    },\n",
    "    {\n",
    "        'text': 'United Nations Security Council votes to impose new sanctions on North Korea over missile tests.',\n",
    "        'ground_truth': 'World',\n",
    "    },\n",
    "    {\n",
    "        'text': 'Apple stock rises 8% after reporting record iPhone sales in quarterly earnings.',\n",
    "        'ground_truth': 'Sci/Tech',\n",
    "    },\n",
    "    {\n",
    "        'text': 'Scientists discover high concentrations of water ice beneath the surface of Mars.',\n",
    "        'ground_truth': 'Sci/Tech',\n",
    "    },\n",
    "    {\n",
    "        'text': 'Wimbledon 2024: Djokovic advances to semifinals after defeating Alcaraz in five sets.',\n",
    "        'ground_truth': 'Sports',\n",
    "    },\n",
    "    {\n",
    "        'text': 'Federal Reserve raises interest rates by 0.25% citing persistent inflation concerns.',\n",
    "        'ground_truth': 'Business',\n",
    "    },\n",
    "    {\n",
    "        'text': 'European Union leaders agree on new climate targets to reduce emissions by 55% by 2030.',\n",
    "        'ground_truth': 'World',\n",
    "    },\n",
    "    {\n",
    "        'text': 'Microsoft acquires gaming company Activision Blizzard for $69 billion.',\n",
    "        'ground_truth': 'Sci/Tech',\n",
    "    },\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(f'Loaded {len(df)} articles')\n",
    "df['ground_truth'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-judge-header",
   "metadata": {},
   "source": [
    "## 4. Create a Simple CustomJudge\n",
    "\n",
    "Let's create a basic topic classifier using `CustomJudge`. We need to define:\n",
    "\n",
    "1. **prompt_template**: The evaluation prompt with `{{ placeholder }}` markers\n",
    "2. **output_fields**: Schema defining the expected outputs with their types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-judge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple topic classifier\n",
    "simple_judge = CustomJudge(\n",
    "    model=LLM_MODEL_NAME,\n",
    "    credential=LLM_CREDENTIAL_NAME,\n",
    "    prompt_template=\"\"\"\n",
    "        Determine the topic of the given news summary Pick one of: 'Sports', 'World', 'Sci/Tech', 'Business'.\n",
    "\n",
    "        News Summary: {{ news_summary }}\n",
    "    \"\"\",\n",
    "    output_fields={\n",
    "        'topic': {\n",
    "            'type': 'string',\n",
    "            'choices': ['Sports', 'World', 'Sci/Tech', 'Business'],\n",
    "        },\n",
    "        'reasoning': {\n",
    "            'type': 'string',\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "print('CustomJudge created successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-eval-header",
   "metadata": {},
   "source": [
    "## 5. Run Evaluation and Inspect Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    print(f'Evaluating article {len(results) + 1}/{len(df)}...', end='\\r')\n",
    "\n",
    "    # Get scores from CustomJudge\n",
    "    scores = simple_judge.score(inputs={'news_summary': row['text']})\n",
    "\n",
    "    # Convert scores list to dict for easy access\n",
    "    scores_dict = {s.name: s for s in scores}\n",
    "\n",
    "    results.append(\n",
    "        {\n",
    "            'ground_truth': row['ground_truth'],\n",
    "            'predicted_topic': scores_dict['topic'].label,\n",
    "            'reasoning': scores_dict['reasoning'].label,\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(f'\\nCompleted evaluation of {len(results)} articles!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (results_df['ground_truth'] == results_df['predicted_topic']).mean()\n",
    "print(f'Accuracy: {accuracy:.0%}')\n",
    "\n",
    "# Show confusion matrix\n",
    "print('\\nPrediction breakdown:')\n",
    "results_df.value_counts(subset=['ground_truth', 'predicted_topic']).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "misclassified",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at misclassified examples\n",
    "misclassified = results_df[results_df['ground_truth'] != results_df['predicted_topic']]\n",
    "\n",
    "print(f'Misclassified: {len(misclassified)} articles\\n')\n",
    "for _, row in misclassified.iterrows():\n",
    "    print(f'Expected: {row[\"ground_truth\"]} | Predicted: {row[\"predicted_topic\"]}')\n",
    "    print(f'Reasoning: {row[\"reasoning\"]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improve-header",
   "metadata": {},
   "source": [
    "## 6. Improve the Prompt\n",
    "\n",
    "Looking at the misclassifications, we often see confusion between **Sci/Tech** and **Business** (tech companies get classified as Business) or **Sports** and **Business** (sports business news).\n",
    "\n",
    "Let's add clearer category descriptions to help the LLM make better decisions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-judge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an improved topic classifier with better guidance\n",
    "improved_judge = CustomJudge(\n",
    "    model=LLM_MODEL_NAME,\n",
    "    credential=LLM_CREDENTIAL_NAME,\n",
    "    prompt_template=\"\"\"\n",
    "        Determine the topic of the given news summary.\n",
    "\n",
    "        Topic Guidelines:\n",
    "        - Sci/Tech: News about technology companies (Google, Apple, Microsoft, etc.), \n",
    "          software, hardware, scientific discoveries, research, health, and medicine\n",
    "        - Sports: News about sports events, athletes, teams, and competitions\n",
    "        - Business: News about companies or industries OUTSIDE of science, technology, or sports\n",
    "        - World: News about global events, politics, international affairs\n",
    "\n",
    "        News Summary: {{ news_summary }}\n",
    "    \"\"\",\n",
    "    output_fields={\n",
    "        'topic': {\n",
    "            'type': 'string',\n",
    "            'choices': ['Sports', 'World', 'Sci/Tech', 'Business'],\n",
    "            'description': 'The primary topic category for this news article',\n",
    "        },\n",
    "        'reasoning': {\n",
    "            'type': 'string',\n",
    "            'description': 'Brief explanation of why this topic was chosen',\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "print('Improved CustomJudge created!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-improved",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run evaluation with the improved judge\n",
    "improved_results = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    print(f'Evaluating article {len(improved_results) + 1}/{len(df)}...', end='\\r')\n",
    "\n",
    "    scores = improved_judge.score(inputs={'news_summary': row['text']})\n",
    "    scores_dict = {s.name: s for s in scores}\n",
    "\n",
    "    improved_results.append(\n",
    "        {\n",
    "            'ground_truth': row['ground_truth'],\n",
    "            'predicted_topic': scores_dict['topic'].label,\n",
    "            'reasoning': scores_dict['reasoning'].label,\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(f'\\nCompleted evaluation!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_df = pd.DataFrame(improved_results)\n",
    "\n",
    "# Compare accuracy\n",
    "original_accuracy = (results_df['ground_truth'] == results_df['predicted_topic']).mean()\n",
    "improved_accuracy = (\n",
    "    improved_df['ground_truth'] == improved_df['predicted_topic']\n",
    ").mean()\n",
    "\n",
    "print('Results Comparison')\n",
    "print('=' * 30)\n",
    "print(f'Simple prompt:   {original_accuracy:.0%}')\n",
    "print(f'Improved prompt: {improved_accuracy:.0%}')\n",
    "print(f'Improvement:     {(improved_accuracy - original_accuracy):+.0%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-breakdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the improved prediction breakdown\n",
    "print('Improved prediction breakdown:')\n",
    "improved_df.value_counts(subset=['ground_truth', 'predicted_topic']).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "## 7. Next Steps\n",
    "\n",
    "Now that you've learned how to create custom evaluators with `CustomJudge`, you can:\n",
    "\n",
    "- **Build domain-specific evaluators** tailored to your use case\n",
    "- **Iterate on prompts** to improve accuracy, as we did in Section 6\n",
    "- **Combine with built-in evaluators** like `Coherence`, `Faithfulness`, etc.\n",
    "- **Run experiments** to compare different prompts and models\n",
    "\n",
    "**Related Notebooks:**\n",
    "- [RAG Evaluators](./Fiddler_Quickstart_RAG_Part1_Evaluators.ipynb) - Built-in evaluators for RAG applications\n",
    "- [RAG Experiments](./Fiddler_Quickstart_RAG_Part2_Experiments.ipynb) - Run structured experiments with evaluators\n",
    "\n",
    "**Resources:**\n",
    "- [Fiddler Evals Documentation](https://docs.fiddler.ai/evaluations/overview)\n",
    "- [CustomJudge Reference](https://docs.fiddler.ai/evaluations/evaluators/custom-judge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8bff16",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
