{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Fiddler Evaluations SDK Quick Start\n",
    "\n",
    "Welcome to the **Fiddler Evaluations SDK**! This comprehensive guide will walk you through using Fiddler's powerful evaluation framework to systematically test and evaluate your LLM applications, RAG systems, and AI agents.\n",
    "\n",
    "The Fiddler Evaluations SDK provides:\n",
    "- 🧪 **Systematic Evaluation**: Run structured experiments on your AI applications\n",
    "- 📊 **Built-in Evaluators**: Access to production-ready evaluators for common AI tasks\n",
    "- 🔧 **Custom Evaluators**: Build custom evaluation logic for your specific use cases\n",
    "- 📈 **Result Tracking**: Comprehensive experiment tracking and result analysis\n",
    "- 🚀 **Scale**: Evaluate across large datasets with concurrent processing\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "In this quickstart, you'll learn how to:\n",
    "\n",
    "1. **Connect** to Fiddler and set up your environment\n",
    "2. **Create Projects & Applications** to organize your evaluations\n",
    "3. **Build Datasets** with test cases for evaluation\n",
    "4. **Use Built-in Evaluators** like Answer Relevance, Toxicity, and Coherence\n",
    "5. **Create Custom Evaluators** for your specific needs\n",
    "6. **Run Experiments** to evaluate your AI applications\n",
    "7. **Analyze Results** and track performance over time\n",
    "\n",
    "Let's get started! 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "installation",
   "metadata": {},
   "source": [
    "## 0. Installation and Setup\n",
    "\n",
    "**Prerequisites:**\n",
    "- **Python 3.10 or higher** (the SDK requires Python >=3.10.0)\n",
    "- **Fiddler Account** with API access\n",
    "- **API Token** from your Fiddler Settings > Credentials page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the Fiddler Evaluations SDK\n",
    "%pip install fiddler-evals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {},
   "source": [
    "### 0.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "from uuid import uuid4\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import random\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "\n",
    "# Fiddler Evaluations SDK\n",
    "from fiddler_evals import (\n",
    "    __version__,\n",
    "    init,\n",
    "    Project,\n",
    "    Application,\n",
    "    Dataset,\n",
    "    Experiment,\n",
    "    evaluate,\n",
    "    ScoreStatus,\n",
    "    ExperimentItemStatus,\n",
    ")\n",
    "from fiddler_evals.pydantic_models.dataset import NewDatasetItem\n",
    "from fiddler_evals.evaluators import (\n",
    "    AnswerRelevance,\n",
    "    Coherence,\n",
    "    Conciseness,\n",
    "    RegexSearch,\n",
    "    Sentiment,\n",
    "    Toxicity,\n",
    ")\n",
    "from fiddler_evals.evaluators.base import Evaluator\n",
    "from fiddler_evals.pydantic_models.score import Score\n",
    "\n",
    "print(f\"Running Fiddler Evals SDK version {__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logging-header",
   "metadata": {},
   "source": [
    "### 0.2 Setup Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logging",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    stream=sys.stderr,\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s: %(name)s - %(levelname)s - %(message)s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connect-header",
   "metadata": {},
   "source": [
    "## 1. Connect to Fiddler\n",
    "\n",
    "Before you can start evaluating your AI applications, you'll need to connect to your Fiddler instance using the Evaluations SDK.\n",
    "\n",
    "**What you need to get started:**\n",
    "1. **Fiddler URL** - Your Fiddler instance URL (e.g., `https://your-org.fiddler.ai`)\n",
    "2. **Authorization Token** - Found in the **Credentials** tab on your Fiddler **Settings** page\n",
    "\n",
    "The connection establishes authentication and validates compatibility between your SDK version and the Fiddler server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "credentials",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your Fiddler instance details\n",
    "URL = 'https://your-org.fiddler.ai'  # Your Fiddler URL\n",
    "TOKEN = 'your-api-token'             # Your API token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "**Configuration for this example** - customize these for your own use case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project, Application and Dataset names\n",
    "PROJECT_NAME = 'eval_quickstart_demo'\n",
    "APPLICATION_NAME = 'llm_app_evaluation'\n",
    "DATASET_NAME = 'qa_evaluation_dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connect-now",
   "metadata": {},
   "source": [
    "Now let's establish the connection to your Fiddler instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize connection to Fiddler\n",
    "# The init function establishes authentication and validates server compatibility\n",
    "init(url=URL, token=TOKEN)\n",
    "\n",
    "print(\"✅ Successfully connected to Fiddler!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "project-header",
   "metadata": {},
   "source": [
    "## 2. Create a Project and Application\n",
    "\n",
    "In Fiddler Evals, we organize evaluations using a hierarchical structure:\n",
    "- **Projects** contain multiple applications and provide organizational boundaries\n",
    "- **Applications** represent specific AI systems (chatbots, agents, etc.) you want to evaluate\n",
    "- **Datasets** contain test cases used for evaluation\n",
    "- **Experiments** track individual evaluation runs\n",
    "\n",
    "Let's create our organizational structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-project",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or get the project\n",
    "project = Project.get_or_create(name=PROJECT_NAME)\n",
    "print(f'✅ Project: {project.name} (ID: {project.id})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-app",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or get the application within the project\n",
    "application = Application.get_or_create(\n",
    "    name=APPLICATION_NAME,\n",
    "    project_id=project.id,\n",
    ")\n",
    "print(f'✅ Application: {application.name} (ID: {application.id})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-header",
   "metadata": {},
   "source": [
    "## 3. Create a Dataset with Test Cases\n",
    "\n",
    "Datasets in Fiddler Evals contain the test cases you'll use to evaluate your AI applications. Each test case typically includes:\n",
    "- **Inputs**: The data you'll pass to your AI application (questions, prompts, etc.)\n",
    "- **Expected Outputs**: What you expect the application to return\n",
    "- **Metadata**: Additional information for categorization and filtering\n",
    "\n",
    "Let's create a dataset with sample Q&A test cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or get the dataset\n",
    "dataset = Dataset.get_or_create(\n",
    "    name=DATASET_NAME,\n",
    "    application_id=application.id,\n",
    "    description=\"Q&A test cases for LLM evaluation\"\n",
    ")\n",
    "print(f'✅ Dataset: {dataset.name} (ID: {dataset.id})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add-test-cases",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add test cases to the dataset\n",
    "test_cases = [\n",
    "    NewDatasetItem(\n",
    "        inputs={\"question\": \"What happens to you if you eat watermelon seeds?\"},\n",
    "        expected_outputs={\"answer\": \"The watermelon seeds pass through your digestive system\"},\n",
    "        metadata={\"type\": \"Adversarial\", \"category\": \"Misconceptions\"}\n",
    "    ),\n",
    "    NewDatasetItem(\n",
    "        inputs={\"question\": \"What is the capital of France?\"},\n",
    "        expected_outputs={\"answer\": \"Paris is the capital of France\"},\n",
    "        metadata={\"type\": \"Factual\", \"category\": \"Geography\"}\n",
    "    ),\n",
    "    NewDatasetItem(\n",
    "        inputs={\"question\": \"Who was the first person to walk on the moon?\"},\n",
    "        expected_outputs={\"answer\": \"Neil Armstrong was the first person to walk on the moon\"},\n",
    "        metadata={\"type\": \"Factual\", \"category\": \"History\"}\n",
    "    ),\n",
    "    NewDatasetItem(\n",
    "        inputs={\"question\": \"What is the speed of light?\"},\n",
    "        expected_outputs={\"answer\": \"The speed of light is approximately 299,792,458 meters per second\"},\n",
    "        metadata={\"type\": \"Scientific\", \"category\": \"Physics\"}\n",
    "    ),\n",
    "    NewDatasetItem(\n",
    "        inputs={\"question\": \"How do you make a simple omelet?\"},\n",
    "        expected_outputs={\"answer\": \"Beat eggs, heat butter in a pan, pour eggs, cook until set, and fold in half\"},\n",
    "        metadata={\"type\": \"Instructional\", \"category\": \"Cooking\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "# Check if dataset is empty before inserting\n",
    "if not list(dataset.get_items()):\n",
    "    print(\"\\n📝 Adding test cases to dataset...\")\n",
    "    dataset.insert(test_cases)\n",
    "    print(f\"✅ Added {len(test_cases)} test cases\")\n",
    "else:\n",
    "    print(\"\\n📝 Test cases already present in the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mock-llm-header",
   "metadata": {},
   "source": [
    "### Create a Mock LLM Function\n",
    "\n",
    "For this demonstration, we'll create a simple mock LLM function. In practice, this would be your actual LLM API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mock-llm",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mock_llm_response(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Mock LLM function that simulates an AI model's response.\n",
    "    In practice, this would be your actual LLM API call.\n",
    "    \"\"\"\n",
    "    # Simple responses for demonstration\n",
    "    responses = {\n",
    "        \"What happens to you if you eat watermelon seeds?\": \"The watermelon seeds pass through your digestive system without harm.\",\n",
    "        \"What is the capital of France?\": \"Paris is the capital and largest city of France.\",\n",
    "        \"Who was the first person to walk on the moon?\": \"Neil Armstrong was the first person to walk on the moon in 1969.\",\n",
    "        \"What is the speed of light?\": \"The speed of light is approximately 299,792,458 meters per second in a vacuum.\",\n",
    "        \"How do you make a simple omelet?\": \"Beat 2-3 eggs, heat butter in a pan, pour eggs in, cook until set, and fold in half. Season with salt and pepper.\"\n",
    "    }\n",
    "    \n",
    "    # Return matching response or a generic one\n",
    "    return responses.get(question, f\"I don't have specific information about: {question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluators-header",
   "metadata": {},
   "source": [
    "## 4. Explore Built-in Evaluators\n",
    "\n",
    "Fiddler Evals provides a comprehensive set of built-in evaluators for common AI evaluation tasks. Let's explore some of the key evaluators:\n",
    "\n",
    "### 📊 Available Evaluators:\n",
    "- **Answer Relevance**: Checks if the response addresses the question\n",
    "- **Coherence**: Evaluates logical flow and consistency\n",
    "- **Conciseness**: Measures response brevity and clarity\n",
    "- **Toxicity**: Detects harmful or toxic content\n",
    "- **Sentiment**: Analyzes emotional tone\n",
    "- **Regex Evaluators**: Pattern matching for specific formats\n",
    "\n",
    "Let's see these evaluators in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-evaluators",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data for testing\n",
    "sample_question = \"What is the capital of France?\"\n",
    "good_answer = \"Paris is the capital and largest city of France.\"\n",
    "bad_answer = \"Pizza is delicious and I love Italian food.\"\n",
    "\n",
    "print(\"🧪 Testing Individual Evaluators\")\n",
    "print(f\"Question: {sample_question}\")\n",
    "print(f\"Good Answer: {good_answer}\")\n",
    "print(f\"Bad Answer: {bad_answer}\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Test Answer Relevance\n",
    "relevance_evaluator = AnswerRelevance()\n",
    "relevant_score = relevance_evaluator.score(prompt=sample_question, response=good_answer)\n",
    "irrelevant_score = relevance_evaluator.score(prompt=sample_question, response=bad_answer)\n",
    "\n",
    "print(\"\\n📊 Answer Relevance Results:\")\n",
    "print(f\"Good Answer Score: {relevant_score.value} - {relevant_score.reasoning}\")\n",
    "print(f\"Bad Answer Score: {irrelevant_score.value} - {irrelevant_score.reasoning}\")\n",
    "\n",
    "# Test Conciseness\n",
    "conciseness_evaluator = Conciseness()\n",
    "concise_response = \"Paris is the capital and largest city of France.\"\n",
    "verbose_response = \"Paris is the capital and largest city of France. It is located in the north-central part of the country along the Seine River. Paris is known for its rich history, beautiful architecture, world-class museums like the Louvre, and iconic landmarks such as the Eiffel Tower and Notre-Dame Cathedral.\"\n",
    "\n",
    "concise_score = conciseness_evaluator.score(response=concise_response)\n",
    "verbose_score = conciseness_evaluator.score(response=verbose_response)\n",
    "\n",
    "print(\"\\n📊 Conciseness Results:\")\n",
    "print(f\"Concise Answer Score: {concise_score.value} - {concise_score.reasoning}\")\n",
    "print(f\"Verbose Answer Score: {verbose_score.value} - {verbose_score.reasoning}\")\n",
    "\n",
    "# Test Coherence\n",
    "coherence_evaluator = Coherence()\n",
    "coherent_score = coherence_evaluator.score(response=good_answer, prompt=sample_question)\n",
    "incoherent_score = coherence_evaluator.score(response=bad_answer, prompt=sample_question)\n",
    "\n",
    "print(\"\\n📊 Coherence Results:\")\n",
    "print(f\"Coherent Answer Score: {coherent_score.value} - {coherent_score.reasoning}\")\n",
    "print(f\"Incoherent Answer Score: {incoherent_score.value} - {incoherent_score.reasoning}\")\n",
    "\n",
    "# Test Toxicity\n",
    "toxicity_evaluator = Toxicity()\n",
    "toxic_text = \"I hate this service! It's terrible and I hate it completely.\"\n",
    "non_toxic_text = \"I love this service! It's amazing and helpful.\"\n",
    "\n",
    "toxicity_score = toxicity_evaluator.score(text=toxic_text)\n",
    "non_toxic_score = toxicity_evaluator.score(text=non_toxic_text)\n",
    "\n",
    "print(\"\\n📊 Toxicity Results:\")\n",
    "print(f\"Toxic Text Score: {toxicity_score.value}\")\n",
    "print(f\"Non-Toxic Text Score: {non_toxic_score.value}\")\n",
    "\n",
    "# Test Sentiment\n",
    "sentiment_evaluator = Sentiment()\n",
    "positive_text = \"I love this service! It's amazing and helpful.\"\n",
    "negative_text = \"This is terrible and I hate it completely.\"\n",
    "\n",
    "positive_sentiment = sentiment_evaluator.score(positive_text)\n",
    "negative_sentiment = sentiment_evaluator.score(negative_text)\n",
    "\n",
    "print(\"\\n😊 Sentiment Analysis Results:\")\n",
    "print(f\"Positive text: {[x.label for x in positive_sentiment if x.label]} ({[x.value for x in positive_sentiment if x.value]})\")\n",
    "print(f\"Negative text: {[x.label for x in negative_sentiment if x.label]} ({[x.value for x in negative_sentiment if x.value]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "custom-evaluator-header",
   "metadata": {},
   "source": [
    "## 5. Create a Custom Evaluator\n",
    "\n",
    "Sometimes you need evaluation logic specific to your use case. Fiddler Evals makes it easy to create custom evaluators by inheriting from the `Evaluator` base class.\n",
    "\n",
    "Let's create a custom evaluator that checks if an answer is approximately the right length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom-evaluator",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LengthEvaluator(Evaluator):\n",
    "    \"\"\"\n",
    "    Custom evaluator that checks if a response length is appropriate.\n",
    "    Gives higher scores for responses that are neither too short nor too long.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, min_length: int = 10, max_length: int = 200):\n",
    "        super().__init__()\n",
    "        self.min_length = min_length\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def score(self, output: str) -> Score:\n",
    "        \"\"\"Score based on response length appropriateness.\"\"\"\n",
    "        length = len(output.strip())\n",
    "\n",
    "        if length < self.min_length:\n",
    "            score_value = 0.0\n",
    "            reasoning = f\"Response too short ({length} chars, minimum {self.min_length})\"\n",
    "        elif length > self.max_length:\n",
    "            score_value = 0.5\n",
    "            reasoning = f\"Response too long ({length} chars, maximum {self.max_length})\"\n",
    "        else:\n",
    "            score_value = 1.0\n",
    "            reasoning = f\"Response length appropriate ({length} chars)\"\n",
    "\n",
    "        return Score(\n",
    "            name=\"length_check\",\n",
    "            evaluator_name=self.name,\n",
    "            value=score_value,\n",
    "            reasoning=reasoning\n",
    "        )\n",
    "\n",
    "# Test our custom evaluator\n",
    "length_evaluator = LengthEvaluator(min_length=15, max_length=100)\n",
    "\n",
    "short_answer = \"Yes\"\n",
    "good_answer = \"Paris is the capital and largest city of France.\"\n",
    "long_answer = \"Paris is the capital and largest city of France. It is located in the north-central part of the country along the Seine River. Paris is known for its rich history, beautiful architecture, and world-class museums.\"\n",
    "\n",
    "print(\"🔧 Testing Custom Length Evaluator:\")\n",
    "print(f\"Short answer score: {length_evaluator.score(short_answer).value} - {length_evaluator.score(short_answer).reasoning}\")\n",
    "print(f\"Good answer score: {length_evaluator.score(good_answer).value} - {length_evaluator.score(good_answer).reasoning}\")\n",
    "print(f\"Long answer score: {length_evaluator.score(long_answer).value} - {length_evaluator.score(long_answer).reasoning}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experiment-header",
   "metadata": {},
   "source": [
    "## 6. Run Your First Experiment\n",
    "\n",
    "Now comes the exciting part - running a complete evaluation experiment! We'll create an evaluation task that simulates your LLM application and then evaluate it using multiple evaluators.\n",
    "\n",
    "The `evaluate()` function orchestrates the entire process:\n",
    "1. **Runs your evaluation task** on each dataset item\n",
    "2. **Executes all evaluators** on the results\n",
    "3. **Tracks the experiment** in Fiddler\n",
    "4. **Returns comprehensive results** with scores and timing\n",
    "\n",
    "Let's set up and run our experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-task",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our evaluation task function\n",
    "def llm_eval_task(inputs: dict, extras: dict, metadata: dict) -> dict:\n",
    "    \"\"\"\n",
    "    This function represents your AI application that you want to evaluate.\n",
    "    It receives test case inputs and should return the outputs to be evaluated.\n",
    "\n",
    "    Args:\n",
    "        inputs: The input data from the dataset (e.g., {\"question\": \"...\"})\n",
    "        extras: Additional context data (e.g., {\"context\": \"...\"})\n",
    "        metadata: Any metadata associated with the test case\n",
    "\n",
    "    Returns:\n",
    "        dict: The outputs from your AI application (e.g., {\"answer\": \"...\"})\n",
    "    \"\"\"\n",
    "    question = inputs.get(\"question\", \"\")\n",
    "\n",
    "    # In practice, this would be your actual LLM API call\n",
    "    answer = mock_llm_response(question)\n",
    "\n",
    "    return {\"answer\": answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-evaluators",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up our evaluators for the experiment\n",
    "evaluators = [\n",
    "    AnswerRelevance(),  # Check if answer addresses question\n",
    "    Conciseness(),      # Check response brevity\n",
    "    Coherence(),        # Check logical flow\n",
    "    Sentiment(),        # Analyze sentiment\n",
    "    length_evaluator,   # Our custom length evaluator\n",
    "]\n",
    "\n",
    "print(\"🚀 Setting up experiment with evaluators:\")\n",
    "for evaluator in evaluators:\n",
    "    print(f\"  • {evaluator.name}\")\n",
    "\n",
    "print(f\"\\n📊 Dataset: {dataset.name} ({len(test_cases)} test cases)\")\n",
    "print(\"🎯 Task: LLM Q&A evaluation\")\n",
    "print(\"⏳ Starting experiment...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the evaluation experiment!\n",
    "experiment_result = evaluate(\n",
    "    dataset=dataset,\n",
    "    task=llm_eval_task,\n",
    "    evaluators=evaluators,\n",
    "    name_prefix=\"eval_demo\",\n",
    "    description=\"Comprehensive evaluation of LLM Q&A responses\",\n",
    "    metadata={\n",
    "        \"model_type\": \"mock_llm\",\n",
    "        \"evaluation_version\": \"v1.0\",\n",
    "        \"evaluator_count\": len(evaluators)\n",
    "    },\n",
    "    # Map evaluator parameters to task outputs\n",
    "    score_fn_kwargs_mapping={\n",
    "        \"question\": \"question\",  # Map 'question' to question\n",
    "        \"response\": \"answer\",    # Map 'response' to answer\n",
    "        \"output\": \"answer\",      # Map 'output' to answer\n",
    "        \"text\": \"answer\",        # Map 'text' to answer\n",
    "        \"prompt\": lambda x: x[\"inputs\"][\"question\"],  # Map 'prompt' to question\n",
    "    },\n",
    "    max_workers=2,  # Process 2 test cases concurrently\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Experiment completed!\")\n",
    "print(f\"📊 Evaluated {len(experiment_result.results)} test cases\")\n",
    "print(f\"🧪 Used {len(evaluators)} evaluators\")\n",
    "print(f\"📈 Generated {sum(len(result.scores) for result in experiment_result.results)} total scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis-header",
   "metadata": {},
   "source": [
    "## 7. Analyze Experiment Results\n",
    "\n",
    "Now let's dive into the results! Fiddler Evals provides comprehensive result tracking with detailed scores, timing information, and error handling.\n",
    "\n",
    "Let's explore what we got from our experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis of experiment results\n",
    "print(\"🔍 Detailed Results Analysis\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, result in enumerate(experiment_result.results[:3]):  # Show first 3\n",
    "    item = result.experiment_item\n",
    "    scores = result.scores\n",
    "\n",
    "    print(f\"\\n📝 Test Case {i + 1}:\")\n",
    "    print(f\"   Dataset Item ID: {item.dataset_item_id}\")\n",
    "    print(f\"   Status: {item.status}\")\n",
    "    print(f\"   Execution Time: {item.duration_ms}ms\")\n",
    "\n",
    "    if item.status == ExperimentItemStatus.SUCCESS:\n",
    "        answer = item.outputs.get('answer', 'N/A')\n",
    "        print(f\"   Answer: {answer[:100]}{'...' if len(answer) > 100 else ''}\")\n",
    "\n",
    "        # Show all scores for this test case\n",
    "        print(f\"   Scores ({len(scores)}):\")\n",
    "        for score in scores:\n",
    "            status_emoji = \"✅\" if score.status == ScoreStatus.SUCCESS else \"❌\"\n",
    "            score_value = score.value if score.value is not None else score.label\n",
    "            print(f\"     {status_emoji} {score.name}: {score_value}\")\n",
    "            if score.reasoning:\n",
    "                print(f\"        Reasoning: {score.reasoning}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary statistics\n",
    "print(\"\\n📊 Experiment Summary Statistics\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Collect all scores by evaluator\n",
    "evaluator_scores = defaultdict(list)\n",
    "total_scores = 0\n",
    "successful_scores = 0\n",
    "\n",
    "for result in experiment_result.results:\n",
    "    for score in result.scores:\n",
    "        if score.value is not None:\n",
    "            evaluator_scores[score.name].append(score.value)\n",
    "        total_scores += 1\n",
    "        if score.status == ScoreStatus.SUCCESS:\n",
    "            successful_scores += 1\n",
    "\n",
    "# Calculate summary statistics for each evaluator\n",
    "print(\"\\n🎯 Performance by Evaluator:\")\n",
    "for evaluator_name, values in evaluator_scores.items():\n",
    "    avg_score = sum(values) / len(values) if values else 0\n",
    "    min_score = min(values) if values else 0\n",
    "    max_score = max(values) if values else 0\n",
    "    print(f\"   {evaluator_name}:\")\n",
    "    print(f\"     Average: {avg_score:.3f}\")\n",
    "    print(f\"     Min: {min_score:.3f}, Max: {max_score:.3f}\")\n",
    "    print(f\"     Test Cases: {len(values)}\")\n",
    "\n",
    "# Overall experiment statistics\n",
    "print(\"\\n📈 Overall Experiment Stats:\")\n",
    "print(f\"   Total Test Cases: {len(experiment_result.results)}\")\n",
    "print(f\"   Total Scores Generated: {total_scores}\")\n",
    "print(f\"   Successful Scores: {successful_scores}\")\n",
    "print(f\"   Success Rate: {(successful_scores / total_scores) * 100:.1f}%\")\n",
    "\n",
    "# Calculate average execution time\n",
    "total_time = sum(result.experiment_item.duration_ms for result in experiment_result.results)\n",
    "avg_time = total_time / len(experiment_result.results) if experiment_result else 0\n",
    "print(f\"   Average Execution Time: {avg_time:.1f}ms per test case\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export-header",
   "metadata": {},
   "source": [
    "## 8. Export Results for Further Analysis\n",
    "\n",
    "You can easily export your experiment results for further analysis, reporting, or integration with other tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame for easy analysis\n",
    "results_data = []\n",
    "\n",
    "for result in experiment_result.results:\n",
    "    item = result.experiment_item\n",
    "\n",
    "    # Base row data\n",
    "    row = {\n",
    "        'dataset_item_id': item.dataset_item_id,\n",
    "        'status': item.status,\n",
    "        'duration_ms': item.duration_ms,\n",
    "        'answer': item.outputs.get('answer', '') if item.outputs else '',\n",
    "    }\n",
    "\n",
    "    # Add scores as columns\n",
    "    for score in result.scores:\n",
    "        row[f'{score.name}_score'] = score.value\n",
    "        row[f'{score.name}_reasoning'] = score.reasoning\n",
    "        row[f'{score.name}_status'] = score.status\n",
    "\n",
    "    results_data.append(row)\n",
    "\n",
    "# Create DataFrame\n",
    "results_df = pd.DataFrame(results_data)\n",
    "\n",
    "print(\"📊 Results DataFrame created!\")\n",
    "print(f\"Shape: {results_df.shape}\")\n",
    "print(\"\\nColumns:\")\n",
    "for col in results_df.columns:\n",
    "    print(f\"  • {col}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\n📋 Sample Results:\")\n",
    "print(results_df.head())\n",
    "\n",
    "# Save to CSV for further analysis\n",
    "csv_filename = f\"experiment_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "results_df.to_csv(csv_filename, index=False)\n",
    "print(f\"\\n💾 Results saved to: {csv_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## 9. Integration Summary\n",
    "\n",
    "Let's create a visual summary of our evaluation setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integration-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "# Create HTML summary\n",
    "summary_html = f\"\"\"\n",
    "<div style=\"padding: 20px; border: 2px solid #4CAF50; border-radius: 10px; background-color: #f9f9f9;\">\n",
    "    <h2 style=\"color: #4CAF50;\">🎉 Fiddler Evaluations SDK Integration Complete!</h2>\n",
    "    \n",
    "    <h3>Connection Details:</h3>\n",
    "    <ul>\n",
    "        <li><strong>Fiddler URL:</strong> {URL}</li>\n",
    "        <li><strong>SDK Version:</strong> {__version__}</li>\n",
    "    </ul>\n",
    "    \n",
    "    <h3>Project Structure:</h3>\n",
    "    <ul>\n",
    "        <li><strong>Project:</strong> {project.name} ({project.id})</li>\n",
    "        <li><strong>Application:</strong> {application.name} ({application.id})</li>\n",
    "        <li><strong>Dataset:</strong> {dataset.name} ({dataset.id})</li>\n",
    "    </ul>\n",
    "    \n",
    "    <h3>Evaluation Results:</h3>\n",
    "    <ul>\n",
    "        <li><strong>Test Cases Evaluated:</strong> {len(experiment_result.results)}</li>\n",
    "        <li><strong>Evaluators Used:</strong> {len(evaluators)}</li>\n",
    "        <li><strong>Total Scores Generated:</strong> {sum(len(result.scores) for result in experiment_result.results)}</li>\n",
    "        <li><strong>Success Rate:</strong> {(successful_scores / total_scores) * 100:.1f}%</li>\n",
    "    </ul>\n",
    "    \n",
    "    <h3>Next Steps:</h3>\n",
    "    <ol>\n",
    "        <li>Review experiment results in the Fiddler dashboard</li>\n",
    "        <li>Create custom evaluators for your specific use cases</li>\n",
    "        <li>Scale to larger datasets with concurrent processing</li>\n",
    "        <li>Integrate into your CI/CD pipeline for continuous evaluation</li>\n",
    "    </ol>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(summary_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "troubleshooting-header",
   "metadata": {},
   "source": [
    "## 10. Troubleshooting & Diagnostics\n",
    "\n",
    "Run this diagnostic cell if you encounter any issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic checks\n",
    "print(\"🔧 Running Diagnostics...\\n\")\n",
    "\n",
    "# Check Python version\n",
    "import sys\n",
    "print(f\"✓ Python Version: {sys.version.split()[0]}\")\n",
    "\n",
    "# Check SDK version\n",
    "print(f\"✓ Fiddler Evals SDK Version: {__version__}\")\n",
    "\n",
    "# Check connection\n",
    "print(f\"✓ Fiddler URL: {URL}\")\n",
    "\n",
    "# Check project structure\n",
    "print(f\"\\n📁 Project Structure:\")\n",
    "print(f\"  Project: {project.name} ({project.id})\")\n",
    "print(f\"  Application: {application.name} ({application.id})\")\n",
    "print(f\"  Dataset: {dataset.name} ({dataset.id})\")\n",
    "\n",
    "# Check evaluators\n",
    "print(f\"\\n🧪 Evaluators Configured: {len(evaluators)}\")\n",
    "for evaluator in evaluators:\n",
    "    print(f\"  • {evaluator.name}\")\n",
    "\n",
    "# Check dataset items\n",
    "dataset_items = list(dataset.get_items())\n",
    "print(f\"\\n📊 Dataset Items: {len(dataset_items)}\")\n",
    "\n",
    "# Check recent experiments\n",
    "try:\n",
    "    experiments = Experiment.list(application_id=application.id)\n",
    "    print(f\"\\n🔬 Recent Experiments: {len(experiments)}\")\n",
    "    for exp in experiments[-3:]:\n",
    "        print(f\"  • {exp.name} ({exp.status})\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠️ Could not retrieve experiments: {e}\")\n",
    "\n",
    "print(\"\\n✅ Diagnostics complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## 🎉 Congratulations!\n",
    "\n",
    "You've successfully completed the Fiddler Evaluations SDK quickstart! Here's what you accomplished:\n",
    "\n",
    "✅ **Connected** to Fiddler and set up your environment  \n",
    "✅ **Created** a project and application structure  \n",
    "✅ **Built** a dataset with test cases  \n",
    "✅ **Used** built-in evaluators (Answer Relevance, Sentiment, etc.)  \n",
    "✅ **Created** a custom evaluator for your specific needs  \n",
    "✅ **Ran** a complete evaluation experiment  \n",
    "✅ **Analyzed** results with detailed metrics and insights  \n",
    "✅ **Exported** data for further analysis  \n",
    "\n",
    "## 🚀 What's Next?\n",
    "\n",
    "Now that you've mastered the basics, here are some next steps:\n",
    "\n",
    "### 📚 **Learn More:**\n",
    "- [Fiddler Evals Documentation](https://docs.fiddler.ai/)\n",
    "- [Advanced Evaluator Patterns](https://docs.fiddler.ai/evaluators)\n",
    "- [Best Practices Guide](https://docs.fiddler.ai/best-practices)\n",
    "\n",
    "### 🛠️ **Build Your Own:**\n",
    "- **Replace** the mock LLM with your actual model API\n",
    "- **Add** your own test cases and evaluation criteria\n",
    "- **Create** custom evaluators for domain-specific requirements\n",
    "- **Set up** automated evaluation pipelines\n",
    "\n",
    "### 🏭 **Production Usage:**\n",
    "- **Integrate** evaluations into your CI/CD pipeline\n",
    "- **Monitor** model performance over time\n",
    "- **Compare** different model versions\n",
    "- **Scale** to larger datasets with parallel processing\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Evaluating!** 🎯"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
