{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Fiddler Evaluations SDK Quick Start\n",
    "\n",
    "## Goal\n",
    "\n",
    "Welcome to the **Fiddler Evaluations SDK Quick Start**! This comprehensive guide will walk you through using Fiddler's powerful evaluation framework to systematically test and evaluate your LLM applications, RAG systems, and AI agents.\n",
    "\n",
    "### Evaluating LLM, RAG, and Agentic Applications\n",
    "\n",
    "The Fiddler Evaluations SDK provides:\n",
    "\n",
    "- 🧪 **Systematic Evaluation**: Run structured experiments on your AI applications\n",
    "- 📊 **Built-in Evaluators**: Access to production-ready evaluators for common AI tasks\n",
    "- 🔧 **Custom Evaluators**: Build custom evaluation logic for your specific use cases\n",
    "- 📈 **Result Tracking**: Comprehensive experiment tracking and result analysis\n",
    "- 🚀 **Scale**: Evaluate across large datasets with concurrent processing\n",
    "\n",
    "## About Fiddler\n",
    "\n",
    "Fiddler is the all-in-one AI Observability and Security platform for responsible AI. Monitoring and analytics capabilities provide a common language, centralized controls, and actionable insights to operationalize production predictive, generative, and agentic applications. An integral part of the platform, the Fiddler Trust Service provides quality and moderation controls for LLM applications. Powered by cost-effective, task-specific, and scalable Fiddler-developed trust models — including cloud and VPC deployments for secure environments — it delivers the fastest guardrails in the industry. Fortune 500 organizations utilize Fiddler to scale LLM and ML deployments, delivering high-performance AI, reducing costs, and ensuring responsible governance.\n",
    "\n",
    "In this quick start, you'll learn how to:\n",
    "\n",
    "1. **Connect** to Fiddler and set up your environment\n",
    "2. **Create Projects & Applications** to organize your evaluations\n",
    "3. **Build Datasets** with test cases for evaluation\n",
    "4. **Use Built-in Evaluators** like Answer Relevance, Toxicity, and Coherence\n",
    "5. **Create Custom Evaluators** for your specific needs\n",
    "6. **Run Experiments** to evaluate your AI applications\n",
    "7. **Analyze Results** and track performance over time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27eb132e",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "1. Connect to Fiddler\n",
    "2. Create a Project & Application\n",
    "3. Build Datasets\n",
    "4. Use Built-in Evaluators\n",
    "5. Create Custom Evaluators\n",
    "6. Run Experiments\n",
    "7. Analyze Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "installation",
   "metadata": {},
   "source": [
    "## 0. Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the Fiddler Evaluations SDK\n",
    "%pip install -q fiddler-evals\n",
    "\n",
    "# Core imports\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "\n",
    "# Fiddler Evaluations SDK\n",
    "from fiddler_evals import (\n",
    "    __version__,\n",
    "    init,\n",
    "    Project,\n",
    "    Application,\n",
    "    Dataset,\n",
    "    Experiment,\n",
    "    evaluate,\n",
    "    ScoreStatus,\n",
    "    ExperimentItemStatus,\n",
    ")\n",
    "from fiddler_evals.pydantic_models.dataset import NewDatasetItem\n",
    "from fiddler_evals.evaluators import (\n",
    "    AnswerRelevance,\n",
    "    Coherence,\n",
    "    Conciseness,\n",
    "    Sentiment,\n",
    "    Toxicity,\n",
    ")\n",
    "from fiddler_evals.evaluators.base import Evaluator\n",
    "from fiddler_evals.pydantic_models.score import Score\n",
    "\n",
    "print(f'Running Fiddler Evals SDK version {__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connect-header",
   "metadata": {},
   "source": [
    "## 1. Connect to Fiddler\n",
    "\n",
    "Before you can start evaluating your AI applications, you'll need to connect to your Fiddler instance using the Evaluations SDK.\n",
    "\n",
    "**What you need to get started:**\n",
    "1. **Fiddler URL** - Your Fiddler instance URL (e.g., `https://your-org.fiddler.ai`)\n",
    "2. **Authorization Token** - Found in the **Credentials** tab on your Fiddler **Settings** page\n",
    "\n",
    "The connection establishes authentication and validates compatibility between your SDK version and the Fiddler server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "credentials",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your Fiddler instance details\n",
    "URL = ''  # Make sure to include the full URL (including https:// e.g. 'https://your_company_name.fiddler.ai')\n",
    "TOKEN = ''  # Your Fiddler API token from Settings > Credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "**Configuration for this example** - customize these for your own use case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project, Application and Dataset names\n",
    "PROJECT_NAME = 'eval_demos'\n",
    "APPLICATION_NAME = 'llm_app_evaluation'\n",
    "DATASET_NAME = 'qa_evaluation_dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connect-now",
   "metadata": {},
   "source": [
    "Now let's establish the connection to your Fiddler instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize connection to Fiddler\n",
    "# The init function establishes authentication and validates server compatibility\n",
    "init(url=URL, token=TOKEN)\n",
    "\n",
    "print('✅ Successfully connected to Fiddler!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "project-header",
   "metadata": {},
   "source": [
    "## 2. Create a Project and Application\n",
    "\n",
    "In Fiddler Evals, we organize evaluations using a hierarchical structure:\n",
    "- **Projects** contain multiple applications and provide organizational boundaries\n",
    "- **Applications** represent specific AI systems (chatbots, agents, etc.) you want to evaluate\n",
    "- **Datasets** contain test cases used for evaluation\n",
    "- **Experiments** track individual evaluation runs\n",
    "\n",
    "Let's create our organizational structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-project",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or get the project\n",
    "project = Project.get_or_create(name=PROJECT_NAME)\n",
    "print(f'✅ Project: {project.name} (ID: {project.id})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-app",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or get the application within the project\n",
    "application = Application.get_or_create(\n",
    "    name=APPLICATION_NAME,\n",
    "    project_id=project.id,\n",
    ")\n",
    "print(f'✅ Application: {application.name} (ID: {application.id})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-header",
   "metadata": {},
   "source": [
    "## 3. Create a Dataset with Test Cases\n",
    "\n",
    "Datasets in Fiddler Evals contain the test cases you'll use to evaluate your AI applications. Each test case typically includes:\n",
    "- **Inputs**: The data you'll pass to your AI application (questions, prompts, etc.)\n",
    "- **Expected Outputs**: What you expect the application to return\n",
    "- **Metadata**: Additional information for categorization and filtering\n",
    "\n",
    "Let's create a dataset with sample Q&A test cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or get the dataset\n",
    "dataset = Dataset.get_or_create(\n",
    "    name=DATASET_NAME,\n",
    "    application_id=application.id,\n",
    "    description='Q&A test cases for LLM evaluation',\n",
    ")\n",
    "print(f'✅ Dataset: {dataset.name} (ID: {dataset.id})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add-test-cases",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add test cases to the dataset\n",
    "test_cases = [\n",
    "    NewDatasetItem(\n",
    "        inputs={'question': 'What happens to you if you eat watermelon seeds?'},\n",
    "        expected_outputs={\n",
    "            'answer': 'The watermelon seeds pass through your digestive system'\n",
    "        },\n",
    "        metadata={'type': 'Adversarial', 'category': 'Misconceptions'},\n",
    "    ),\n",
    "    NewDatasetItem(\n",
    "        inputs={'question': 'What is the capital of France?'},\n",
    "        expected_outputs={'answer': 'Paris is the capital of France'},\n",
    "        metadata={'type': 'Factual', 'category': 'Geography'},\n",
    "    ),\n",
    "    NewDatasetItem(\n",
    "        inputs={'question': 'Who was the first person to walk on the moon?'},\n",
    "        expected_outputs={\n",
    "            'answer': 'Neil Armstrong was the first person to walk on the moon'\n",
    "        },\n",
    "        metadata={'type': 'Factual', 'category': 'History'},\n",
    "    ),\n",
    "    NewDatasetItem(\n",
    "        inputs={'question': 'What is the speed of light?'},\n",
    "        expected_outputs={\n",
    "            'answer': 'The speed of light is approximately 299,792,458 meters per second'\n",
    "        },\n",
    "        metadata={'type': 'Scientific', 'category': 'Physics'},\n",
    "    ),\n",
    "    NewDatasetItem(\n",
    "        inputs={'question': 'How do you make a simple omelet?'},\n",
    "        expected_outputs={\n",
    "            'answer': 'Beat eggs, heat butter in a pan, pour eggs, cook until set, and fold in half'\n",
    "        },\n",
    "        metadata={'type': 'Instructional', 'category': 'Cooking'},\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Check if dataset is empty before inserting\n",
    "if not list(dataset.get_items()):\n",
    "    print('\\n📝 Adding test cases to dataset...')\n",
    "    dataset.insert(test_cases)\n",
    "    print(f'✅ Added {len(test_cases)} test cases')\n",
    "else:\n",
    "    print('\\n📝 Test cases already present in the dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluators-header",
   "metadata": {},
   "source": [
    "## 4. Explore Built-in Evaluators\n",
    "\n",
    "Fiddler Evals provides a comprehensive set of built-in evaluators for common AI evaluation tasks. Let's explore some of the key evaluators:\n",
    "\n",
    "### 📊 Available Evaluators:\n",
    "- **Answer Relevance**: Checks if the response addresses the question\n",
    "- **Coherence**: Evaluates logical flow and consistency\n",
    "- **Conciseness**: Measures response brevity and clarity\n",
    "- **Toxicity**: Detects harmful or toxic content\n",
    "- **Sentiment**: Analyzes emotional tone\n",
    "- **Regex Evaluators**: Pattern matching for specific formats\n",
    "\n",
    "Let's see these evaluators in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-evaluators",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data for testing\n",
    "sample_question = 'What is the capital of France?'\n",
    "good_answer = 'Paris is the capital and largest city of France.'\n",
    "bad_answer = 'Pizza is delicious and I love Italian food.'\n",
    "\n",
    "print('🧪 Testing Individual Evaluators')\n",
    "print(f'Question: {sample_question}')\n",
    "print(f'Good Answer: {good_answer}')\n",
    "print(f'Bad Answer: {bad_answer}')\n",
    "print('\\n' + '=' * 80)\n",
    "\n",
    "# Test Answer Relevance\n",
    "relevance_evaluator = AnswerRelevance()\n",
    "relevant_score = relevance_evaluator.score(prompt=sample_question, response=good_answer)\n",
    "irrelevant_score = relevance_evaluator.score(\n",
    "    prompt=sample_question, response=bad_answer\n",
    ")\n",
    "\n",
    "print('\\n📊 Answer Relevance Results:')\n",
    "print(f'Good Answer Score: {relevant_score.value} - {relevant_score.reasoning}')\n",
    "print(f'Bad Answer Score: {irrelevant_score.value} - {irrelevant_score.reasoning}')\n",
    "\n",
    "# Test Conciseness\n",
    "conciseness_evaluator = Conciseness()\n",
    "concise_response = 'Paris is the capital and largest city of France.'\n",
    "verbose_response = 'Paris is the capital and largest city of France. It is located in the north-central part of the country along the Seine River. Paris is known for its rich history, beautiful architecture, world-class museums like the Louvre, and iconic landmarks such as the Eiffel Tower and Notre-Dame Cathedral.'\n",
    "\n",
    "concise_score = conciseness_evaluator.score(response=concise_response)\n",
    "verbose_score = conciseness_evaluator.score(response=verbose_response)\n",
    "\n",
    "print('\\n📊 Conciseness Results:')\n",
    "print(f'Concise Answer Score: {concise_score.value} - {concise_score.reasoning}')\n",
    "print(f'Verbose Answer Score: {verbose_score.value} - {verbose_score.reasoning}')\n",
    "\n",
    "# Test Coherence\n",
    "coherence_evaluator = Coherence()\n",
    "coherent_score = coherence_evaluator.score(response=good_answer, prompt=sample_question)\n",
    "incoherent_score = coherence_evaluator.score(\n",
    "    response=bad_answer, prompt=sample_question\n",
    ")\n",
    "\n",
    "print('\\n📊 Coherence Results:')\n",
    "print(f'Coherent Answer Score: {coherent_score.value} - {coherent_score.reasoning}')\n",
    "print(\n",
    "    f'Incoherent Answer Score: {incoherent_score.value} - {incoherent_score.reasoning}'\n",
    ")\n",
    "\n",
    "# Test Toxicity\n",
    "toxicity_evaluator = Toxicity()\n",
    "toxic_text = \"I hate this service! It's terrible and I hate it completely.\"\n",
    "non_toxic_text = \"I love this service! It's amazing and helpful.\"\n",
    "\n",
    "toxicity_score = toxicity_evaluator.score(text=toxic_text)\n",
    "non_toxic_score = toxicity_evaluator.score(text=non_toxic_text)\n",
    "\n",
    "print('\\n📊 Toxicity Results:')\n",
    "print(f'Toxic Text Score: {toxicity_score.value}')\n",
    "print(f'Non-Toxic Text Score: {non_toxic_score.value}')\n",
    "\n",
    "# Test Sentiment\n",
    "sentiment_evaluator = Sentiment()\n",
    "positive_text = \"I love this service! It's amazing and helpful.\"\n",
    "negative_text = 'This is terrible and I hate it completely.'\n",
    "\n",
    "positive_sentiment = sentiment_evaluator.score(positive_text)\n",
    "negative_sentiment = sentiment_evaluator.score(negative_text)\n",
    "\n",
    "print('\\n😊 Sentiment Analysis Results:')\n",
    "print(\n",
    "    f'Positive text: {[x.label for x in positive_sentiment if x.label]} ({[x.value for x in positive_sentiment if x.value]})'\n",
    ")\n",
    "print(\n",
    "    f'Negative text: {[x.label for x in negative_sentiment if x.label]} ({[x.value for x in negative_sentiment if x.value]})'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "custom-evaluator-header",
   "metadata": {},
   "source": [
    "## 5. Create a Custom Evaluator\n",
    "\n",
    "Sometimes you need evaluation logic specific to your use case. Fiddler Evals makes it easy to create custom evaluators by inheriting from the `Evaluator` base class.\n",
    "\n",
    "Let's create a custom evaluator that checks if an answer is approximately the right length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom-evaluator",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LengthEvaluator(Evaluator):\n",
    "    \"\"\"\n",
    "    Custom evaluator that checks if a response length is appropriate.\n",
    "    Gives higher scores for responses that are neither too short nor too long.\n",
    "\n",
    "    This demonstrates how to create domain-specific evaluators for your use case.\n",
    "    For example, you might want answers to be detailed but not verbose.\n",
    "\n",
    "    Args:\n",
    "        min_length: Minimum acceptable character count (default: 10)\n",
    "                   Values below this indicate insufficient detail (e.g., \"Yes\" or \"No\")\n",
    "        max_length: Maximum acceptable character count (default: 200)\n",
    "                   Values above this indicate excessive verbosity\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, min_length: int = 10, max_length: int = 200):\n",
    "        super().__init__()\n",
    "        self.min_length = min_length\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def score(self, output: str) -> Score:\n",
    "        \"\"\"Score based on response length appropriateness.\"\"\"\n",
    "        length = len(output.strip())\n",
    "\n",
    "        if length < self.min_length:\n",
    "            score_value = 0.0\n",
    "            reasoning = (\n",
    "                f'Response too short ({length} chars, minimum {self.min_length})'\n",
    "            )\n",
    "        elif length > self.max_length:\n",
    "            score_value = 0.5\n",
    "            reasoning = f'Response too long ({length} chars, maximum {self.max_length})'\n",
    "        else:\n",
    "            score_value = 1.0\n",
    "            reasoning = f'Response length appropriate ({length} chars)'\n",
    "\n",
    "        return Score(\n",
    "            name='length_check',\n",
    "            evaluator_name=self.name,\n",
    "            value=score_value,\n",
    "            reasoning=reasoning,\n",
    "        )\n",
    "\n",
    "\n",
    "# Test our custom evaluator\n",
    "length_evaluator = LengthEvaluator(min_length=15, max_length=100)\n",
    "\n",
    "short_answer = 'Yes'\n",
    "good_answer = 'Paris is the capital and largest city of France.'\n",
    "long_answer = 'Paris is the capital and largest city of France. It is located in the north-central part of the country along the Seine River. Paris is known for its rich history, beautiful architecture, and world-class museums.'\n",
    "\n",
    "print('🔧 Testing Custom Length Evaluator:')\n",
    "print(\n",
    "    f'Short answer score: {length_evaluator.score(short_answer).value} - {length_evaluator.score(short_answer).reasoning}'\n",
    ")\n",
    "print(\n",
    "    f'Good answer score: {length_evaluator.score(good_answer).value} - {length_evaluator.score(good_answer).reasoning}'\n",
    ")\n",
    "print(\n",
    "    f'Long answer score: {length_evaluator.score(long_answer).value} - {length_evaluator.score(long_answer).reasoning}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experiment-header",
   "metadata": {},
   "source": [
    "## 6. Run Your First Experiment\n",
    "\n",
    "Now comes the exciting part - running a complete evaluation experiment! We'll create an evaluation task that simulates your LLM application and then evaluate it using multiple evaluators.\n",
    "\n",
    "The `evaluate()` function orchestrates the entire process:\n",
    "1. **Runs your evaluation task** on each dataset item\n",
    "2. **Executes all evaluators** on the results\n",
    "3. **Tracks the experiment** in Fiddler\n",
    "4. **Returns comprehensive results** with scores and timing\n",
    "\n",
    "Let's set up and run our experiment:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1b97e4",
   "metadata": {},
   "source": [
    "### Create a Mock LLM Function\n",
    "\n",
    "For this demonstration, we'll create a simple mock LLM function. In practice, this would be your actual LLM API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bead40e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mock_llm_response(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Mock LLM function that simulates an AI model's response.\n",
    "    In practice, this would be your actual LLM API call.\n",
    "\n",
    "    Note: Some responses below are intentionally incorrect to demonstrate\n",
    "    how Fiddler's evaluators detect quality issues in LLM outputs.\n",
    "    \"\"\"\n",
    "    # Simple responses for demonstration\n",
    "    # NOTE: Some answers are intentionally wrong to show evaluator detection\n",
    "    responses = {\n",
    "        'What happens to you if you eat watermelon seeds?': 'The watermelon seeds pass through your digestive system without harm.',\n",
    "        'What is the capital of France?': 'Grenoble is the capital and largest city of France.',  # Intentionally WRONG - should be Paris\n",
    "        'Who was the first person to walk on the moon?': 'Neil Armstrong was the first person to walk on the moon in 1969.',\n",
    "        'What is the speed of light?': 'The speed of light is approximately 299,792,458 miles per second in a vacuum.',  # Intentionally uses miles instead of meters\n",
    "        'How do you make a simple omelet?': 'Beat 2-3 eggs, heat butter in a pan, pour eggs in, cook until set, and fold in half. Season with salt and pepper.',\n",
    "    }\n",
    "\n",
    "    # Return matching response or a generic one\n",
    "    return responses.get(\n",
    "        question, f\"I don't have specific information about: {question}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7fe733",
   "metadata": {},
   "source": [
    "### Create a Mock LLM Task\n",
    "\n",
    "For this demonstration, we'll create a simple mock LLM application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-task",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our evaluation task function\n",
    "def llm_eval_task(inputs: dict, extras: dict, metadata: dict) -> dict:\n",
    "    \"\"\"\n",
    "    This function represents your AI application that you want to evaluate.\n",
    "    It receives test case inputs and should return the outputs to be evaluated.\n",
    "\n",
    "    Args:\n",
    "        inputs: The input data from the dataset (e.g., {\"question\": \"...\"})\n",
    "        extras: Additional context data (e.g., {\"context\": \"...\"})\n",
    "        metadata: Any metadata associated with the test case\n",
    "\n",
    "    Returns:\n",
    "        dict: The outputs from your AI application (e.g., {\"answer\": \"...\"})\n",
    "    \"\"\"\n",
    "    question = inputs.get('question', '')\n",
    "\n",
    "    # In practice, this would be your actual LLM API call\n",
    "    answer = mock_llm_response(question)\n",
    "\n",
    "    return {'answer': answer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1316cd0f",
   "metadata": {},
   "source": [
    "### Select the Evaluators\n",
    "\n",
    "Establish the individual evaluators to use in this evaluation experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-evaluators",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up our evaluators for the experiment\n",
    "evaluators = [\n",
    "    AnswerRelevance(),  # Check if answer addresses question\n",
    "    Conciseness(),  # Check response brevity\n",
    "    Coherence(),  # Check logical flow\n",
    "    Sentiment(),  # Analyze sentiment\n",
    "    length_evaluator,  # Our custom length evaluator\n",
    "]\n",
    "\n",
    "print('🚀 Setting up experiment with evaluators:')\n",
    "for evaluator in evaluators:\n",
    "    print(f'  • {evaluator.name}')\n",
    "\n",
    "print(f'\\n📊 Dataset: {dataset.name} ({len(test_cases)} test cases)')\n",
    "print('🎯 Task: LLM Q&A evaluation')\n",
    "print('⏳ Starting experiment...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcea4a53",
   "metadata": {},
   "source": [
    "### Run the Evaluation and View Experiment Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the evaluation experiment!\n",
    "experiment_result = evaluate(\n",
    "    dataset=dataset,\n",
    "    task=llm_eval_task,\n",
    "    evaluators=evaluators,\n",
    "    name_prefix='eval_demo',\n",
    "    description='Comprehensive evaluation of LLM Q&A responses',\n",
    "    metadata={\n",
    "        'model_type': 'mock_llm',\n",
    "        'evaluation_version': 'v1.1',\n",
    "        'evaluator_count': len(evaluators),\n",
    "    },\n",
    "    # Map evaluator parameters to task outputs\n",
    "    score_fn_kwargs_mapping={\n",
    "        'question': 'question',  # Map 'question' to question\n",
    "        'response': 'answer',  # Map 'response' to answer\n",
    "        'output': 'answer',  # Map 'output' to answer\n",
    "        'text': 'answer',  # Map 'text' to answer\n",
    "        'prompt': lambda x: x['inputs']['question'],  # Map 'prompt' to question\n",
    "    },\n",
    "    max_workers=2,  # Process 2 test cases concurrently\n",
    ")\n",
    "\n",
    "print('\\n✅ Experiment completed!')\n",
    "print(f'📊 Evaluated {len(experiment_result.results)} test cases')\n",
    "print(f'🧪 Used {len(evaluators)} evaluators')\n",
    "print(\n",
    "    f'📈 Generated {sum(len(result.scores) for result in experiment_result.results)} total scores'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis-header",
   "metadata": {},
   "source": [
    "## 8. Analyze Experiment Results (Programmatic Access)\n",
    "\n",
    "Now let's dive into the results! Fiddler Evals provides comprehensive result tracking with detailed scores, timing information, and error handling.\n",
    "\n",
    "Let's explore what we got from our experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis of experiment results\n",
    "print('🔍 Detailed Results Analysis')\n",
    "print('=' * 80)\n",
    "\n",
    "for i, result in enumerate(experiment_result.results):\n",
    "    item = result.experiment_item\n",
    "    scores = result.scores\n",
    "\n",
    "    print(f'\\n📝 Test Case {i + 1}:')\n",
    "    print(f'   Dataset Item ID: {item.dataset_item_id}')\n",
    "    print(f'   Status: {item.status}')\n",
    "    print(f'   Execution Time: {item.duration_ms}ms')\n",
    "\n",
    "    if item.status == ExperimentItemStatus.SUCCESS:\n",
    "        answer = item.outputs.get('answer', 'N/A')\n",
    "        print(f'   Answer: {answer[:100]}{\"...\" if len(answer) > 100 else \"\"}')\n",
    "\n",
    "        # Show all scores for this test case\n",
    "        print(f'   Scores ({len(scores)}):')\n",
    "        for score in scores:\n",
    "            status_emoji = '✅' if score.status == ScoreStatus.SUCCESS else '❌'\n",
    "            score_value = score.value if score.value is not None else score.label\n",
    "            print(f'     {status_emoji} {score.name}: {score_value}')\n",
    "            if score.reasoning:\n",
    "                print(f'        Reasoning: {score.reasoning}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export-header",
   "metadata": {},
   "source": [
    "## 9. Export Results for Further Analysis\n",
    "\n",
    "You can easily export your experiment results for further analysis, reporting, or integration with other tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame for easy analysis\n",
    "results_data = []\n",
    "\n",
    "for result in experiment_result.results:\n",
    "    item = result.experiment_item\n",
    "\n",
    "    # Base row data\n",
    "    row = {\n",
    "        'dataset_item_id': item.dataset_item_id,\n",
    "        'status': item.status,\n",
    "        'duration_ms': item.duration_ms,\n",
    "        'answer': item.outputs.get('answer', '') if item.outputs else '',\n",
    "    }\n",
    "\n",
    "    # Add scores as columns\n",
    "    for score in result.scores:\n",
    "        row[f'{score.name}_score'] = score.value\n",
    "        row[f'{score.name}_reasoning'] = score.reasoning\n",
    "        row[f'{score.name}_status'] = score.status\n",
    "\n",
    "    results_data.append(row)\n",
    "\n",
    "# Create DataFrame\n",
    "results_df = pd.DataFrame(results_data)\n",
    "\n",
    "print('📊 Results DataFrame created!')\n",
    "print(f'Shape: {results_df.shape}')\n",
    "print('\\nColumns:')\n",
    "for col in results_df.columns:\n",
    "    print(f'  • {col}')\n",
    "\n",
    "# Display first few rows\n",
    "print('\\n📋 Sample Results:')\n",
    "print(results_df.head())\n",
    "\n",
    "# Save to CSV for further analysis\n",
    "csv_filename = f'experiment_results_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "results_df.to_csv(csv_filename, index=False)\n",
    "print(f'\\n💾 Results saved to: {csv_filename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ackg1x6xgf",
   "metadata": {},
   "source": [
    "## 7. View Results in Fiddler UI\n",
    "\n",
    "Your experiment has been saved to Fiddler! Now let's see how to view and analyze your results in the Fiddler web interface.\n",
    "\n",
    "### 📊 Accessing Your Experiment\n",
    "\n",
    "1. **Open Fiddler**: Navigate to your Fiddler instance (the URL you used to connect)\n",
    "2. **Find Your Project**: Go to **Projects** in the left sidebar → Select `eval_demos`\n",
    "3. **Open Your Application**: Click on the `llm_app_evaluation` application\n",
    "4. **View Experiments**: Click the **Experiments** tab to see all evaluation runs\n",
    "5. **Select Your Experiment**: Find the experiment that starts with `eval_demo_` (sorted by most recent)\n",
    "\n",
    "### 🔍 What You'll See in the UI\n",
    "\n",
    "The Fiddler UI provides a comprehensive view of your evaluation results:\n",
    "\n",
    "- **Summary Metrics**: Aggregate scores across all evaluators\n",
    "- **Test Case Details**: Individual results for each question/answer pair\n",
    "- **Evaluator Scores**: Detailed breakdown of each evaluator's assessment\n",
    "- **Performance Metrics**: Execution time and status for each test case\n",
    "- **Filtering & Search**: Filter by metadata (type, category) to find specific test cases\n",
    "- **Export Options**: Download results as CSV or JSON for further analysis\n",
    "\n",
    "### 💡 Next Steps in the UI\n",
    "\n",
    "- **Compare Experiments**: Select multiple experiments to see side-by-side comparisons\n",
    "- **Analyze Patterns**: Look for categories with consistently low scores\n",
    "- **Track Progress**: Run the same evaluation after making improvements to see score changes\n",
    "- **Share Results**: Export reports or share experiment URLs with your team\n",
    "\n",
    "The experiment data displayed below matches what you'll see in the Fiddler UI!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "troubleshooting-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Advanced Topics\n",
    "\n",
    "The following sections cover advanced usage patterns. These are optional for getting started but useful as you scale your evaluation workflows.\n",
    "\n",
    "## 10. Troubleshooting & Diagnostics\n",
    "\n",
    "Run this diagnostic cell if you encounter any issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic checks\n",
    "print('🔧 System Diagnostics\\n')\n",
    "print('=' * 80)\n",
    "\n",
    "# Check Python version\n",
    "print(f'\\n✓ Python Version: {sys.version.split()[0]}')\n",
    "\n",
    "# Check SDK version\n",
    "print(f'✓ Fiddler Evals SDK Version: {__version__}')\n",
    "\n",
    "# Check connection\n",
    "print(f'✓ Connected to: {URL}')\n",
    "\n",
    "# Check project structure\n",
    "print('\\n📁 Project Structure:')\n",
    "print(f'  └─ Project: {project.name} ({project.id})')\n",
    "print(f'     └─ Application: {application.name} ({application.id})')\n",
    "print(f'        └─ Dataset: {dataset.name} ({dataset.id})')\n",
    "\n",
    "# Check evaluators configured\n",
    "print(f'\\n🧪 Evaluators Configured: {len(evaluators)}')\n",
    "for evaluator in evaluators:\n",
    "    print(f'  • {evaluator.name}')\n",
    "\n",
    "# Check dataset items\n",
    "dataset_items = list(dataset.get_items())\n",
    "print(f'\\n📊 Dataset: {len(dataset_items)} test cases')\n",
    "\n",
    "# Check recent experiments\n",
    "try:\n",
    "    experiments = list(Experiment.list(application_id=application.id))\n",
    "    print(f'\\n🔬 Total Experiments: {len(experiments)}')\n",
    "    if experiments:\n",
    "        print(f'   Most Recent:')\n",
    "        for exp in experiments[:3]:\n",
    "            print(f'   • {exp.name} - Status: {exp.status}')\n",
    "except Exception as e:\n",
    "    print(f'\\n⚠️ Could not retrieve experiments: {e}')\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('✅ Diagnostics Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## 🎉 Congratulations!\n",
    "\n",
    "You've successfully completed the Fiddler Evaluations SDK quickstart! Here's what you accomplished:\n",
    "\n",
    "✅ **Connected** to Fiddler and set up your environment  \n",
    "✅ **Created** a project and application structure  \n",
    "✅ **Built** a dataset with test cases  \n",
    "✅ **Used** built-in evaluators (Answer Relevance, Sentiment, etc.)  \n",
    "✅ **Created** a custom evaluator for your specific needs  \n",
    "✅ **Ran** a complete evaluation experiment  \n",
    "✅ **Analyzed** results with detailed metrics and insights  \n",
    "✅ **Exported** data for further analysis  \n",
    "\n",
    "## 🚀 What's Next?\n",
    "\n",
    "Now that you've mastered the basics, here are some next steps:\n",
    "\n",
    "### 📚 **Learn More:**\n",
    "- [Getting Started with Fiddler Evals](https://docs.fiddler.ai/first-steps/getting-started-with-fiddler-evals)\n",
    "- [Evals SDK Technical Reference](https://docs.fiddler.ai/technical-reference/fiddler-evals-sdk)\n",
    "- [Evals SDK Advanced Guide](https://docs.fiddler.ai/evaluators)\n",
    "\n",
    "### 🛠️ **Build Your Own:**\n",
    "- **Replace** the mock LLM with your actual model API\n",
    "- **Add** your own test cases and evaluation criteria\n",
    "- **Create** custom evaluators for domain-specific requirements\n",
    "- **Set up** automated evaluation pipelines\n",
    "\n",
    "### 🏭 **Production Usage:**\n",
    "- **Integrate** evaluations into your CI/CD pipeline\n",
    "- **Monitor** model performance over time\n",
    "- **Compare** different model versions\n",
    "- **Scale** to larger datasets with parallel processing\n",
    "\n",
    "**Happy Evaluating!** 🎯\n",
    "\n",
    "---\n",
    "\n",
    "**Questions?**  \n",
    "  \n",
    "Check out [our docs](https://docs.fiddler.ai/) for a more detailed explanation of what Fiddler has to offer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
