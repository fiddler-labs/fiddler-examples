{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fiddler Guardrails Free Trial Quick Start Guide\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q matplotlib numpy pandas requests scikit-learn\n",
    "\n",
    "import time\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from sklearn.metrics import (\n",
    "    PrecisionRecallDisplay,\n",
    "    RocCurveDisplay,\n",
    "    classification_report,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIDDLER_GUARDRAILS_TOKEN = \"\"\n",
    "GUARDRAILS_BASE_URL = \"https://guardrails.cloud.fiddler.ai/v3/guardrails\"\n",
    "\n",
    "FAITHFULNESS_GUARDRAILS_URL = f\"{GUARDRAILS_BASE_URL}/ftl-response-faithfulness\"\n",
    "\n",
    "GUARDRAILS_HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {FIDDLER_GUARDRAILS_TOKEN}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load RAGTruth QA Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download from GitHub and set columns we'll use for looking at performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ragtruth_qa_from_github(sample_size: int = None) -> pd.DataFrame:\n",
    "    \"\"\"Read RAGTruth from GitHub and filter to the Q&A examples in the test set.\n",
    "\n",
    "    Args:\n",
    "        sample_size (int, optional): How many records to return. If specified then a balanced sample will returned.\n",
    "            Defaults to None, which means all records will be returned\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: RAGTruth dataset filtered to Q&A examples in the test set, optionally downsampled.\n",
    "    \"\"\"\n",
    "    ragtruth_source_info = pd.read_json(\n",
    "        \"https://github.com/ParticleMedia/RAGTruth/raw/refs/heads/main/dataset/source_info.jsonl\",\n",
    "        lines=True,\n",
    "    )\n",
    "    ragtruth_response = pd.read_json(\n",
    "        \"https://github.com/ParticleMedia/RAGTruth/raw/refs/heads/main/dataset/response.jsonl\",\n",
    "        lines=True,\n",
    "    )\n",
    "    df = pd.merge(\n",
    "        ragtruth_source_info,\n",
    "        ragtruth_response,\n",
    "        left_on=\"source_id\",\n",
    "        right_on=\"source_id\",\n",
    "    )\n",
    "    df_qa = df[(df[\"task_type\"] == \"QA\") & (df[\"split\"] == \"test\")]\n",
    "\n",
    "    if sample_size is None:\n",
    "        return df_qa\n",
    "    else:\n",
    "        df_qa_pos = df_qa[df_qa[\"labels\"].apply(len) == 0].sample(\n",
    "            sample_size // 2, random_state=19\n",
    "        )\n",
    "        df_qa_neg = df_qa[df_qa[\"labels\"].apply(len) != 0].sample(\n",
    "            sample_size // 2, random_state=19\n",
    "        )\n",
    "        return pd.concat([df_qa_pos, df_qa_neg])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ragtruth_qa = read_ragtruth_qa_from_github(sample_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ragtruth_qa[\"n_input_words\"] = (\n",
    "    ragtruth_qa[\"source_info\"].astype(str).str.split().str.len()\n",
    "    + ragtruth_qa[\"response\"].astype(str).str.split().str.len()\n",
    ")  # a rough estimate of word count\n",
    "\n",
    "ragtruth_qa[\"is_faithful\"] = (ragtruth_qa[\"labels\"].apply(len) == 0).astype(int)\n",
    "display(ragtruth_qa[\"is_faithful\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Code for Guardrails and Evaluation Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_faithfulness_response(\n",
    "    context: str, response: str, sleep_seconds: float = 0.3\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Invoke the guardrail for Faithfulness, returning the faithfulness score and latency time\n",
    "\n",
    "    Args:\n",
    "        context (str): Guardrails context field\n",
    "        response (str): Guardrails response field\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]: Faithfulness score and latency in seconds.\n",
    "    \"\"\"\n",
    "    time.sleep(sleep_seconds)\n",
    "    data = {\n",
    "        \"context\": context,\n",
    "        \"response\": response,\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            FAITHFULNESS_GUARDRAILS_URL,\n",
    "            headers=GUARDRAILS_HEADERS,\n",
    "            json={\"data\": data},\n",
    "            timeout=30,\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None, response.elapsed.total_seconds()\n",
    "\n",
    "    return response.json()[\"fdl_faithful_score\"], response.elapsed.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faithful_display_name_map = {0: \"hallucination\", 1: \"faithful\"}\n",
    "pos_label = \"faithful\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_classification_results(df: pd.DataFrame, ds_name: str):\n",
    "    global faithful_display_name_map, pos_label\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    pr = PrecisionRecallDisplay.from_predictions(\n",
    "        df[\"is_faithful\"].map(faithful_display_name_map),\n",
    "        y_pred=df[\"fdl_faithful_score\"],\n",
    "        ax=axes[2],\n",
    "        pos_label=pos_label,\n",
    "    )\n",
    "    pr.ax_.set_title(f\"Precision-Recall Curve: {ds_name}\")\n",
    "\n",
    "    roc = RocCurveDisplay.from_predictions(\n",
    "        df[\"is_faithful\"].map(faithful_display_name_map),\n",
    "        y_pred=df[\"fdl_faithful_score\"],\n",
    "        ax=axes[1],\n",
    "        plot_chance_level=True,\n",
    "        pos_label=pos_label,\n",
    "    )\n",
    "    roc.ax_.set_title(f\"ROC Curve: {ds_name}\")\n",
    "\n",
    "    axes[0].hist(\n",
    "        df[df[\"is_faithful\"] == 1][\"fdl_faithful_score\"],\n",
    "        color=\"cornflowerblue\",\n",
    "        label=\"Faithful\",\n",
    "        bins=np.arange(0, 1.05, 0.1),\n",
    "    )\n",
    "    axes[0].hist(\n",
    "        df[df[\"is_faithful\"] == 0][\"fdl_faithful_score\"],\n",
    "        color=\"orange\",\n",
    "        label=\"Hallucination\",\n",
    "        histtype=\"step\",\n",
    "        linewidth=1.5,\n",
    "        bins=np.arange(0, 1.05, 0.1),\n",
    "    )\n",
    "    axes[0].set_xlabel(\"Score\")\n",
    "    axes[0].set_ylabel(\"Frequency\", axes=axes[0])\n",
    "    axes[0].set_title(f\"Faithfulness Scores for {ds_name}\")\n",
    "    axes[0].legend()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latency(df: pd.DataFrame, ds_name: str):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    axes[0].hist(df[\"fdl_faithful_latency\"])\n",
    "    axes[0].set_title(\n",
    "        f\"Latency for {ds_name}: p95={int(1000 * df['fdl_faithful_latency'].quantile(0.95).round(3))}ms\"\n",
    "    )\n",
    "    axes[0].set_xlabel(\"Seconds\")\n",
    "    axes[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "    axes[1].set_title(\n",
    "        f\"Median: {int((df['n_input_words'] / df['fdl_faithful_latency']).median())} Words per Second\"\n",
    "    )\n",
    "    axes[1].scatter(x=df[\"n_input_words\"], y=df[\"fdl_faithful_latency\"])\n",
    "    axes[1].set_xlabel(\"# of Input Words\")\n",
    "    axes[1].set_ylabel(\"Seconds\")\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Dataset Through Guardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ragtruth_qa[[\"fdl_faithful_score\", \"fdl_faithful_latency\"]] = ragtruth_qa.apply(\n",
    "    lambda row: get_faithfulness_response(str(row[\"source_info\"]), row[\"response\"]),\n",
    "    axis=1,\n",
    "    result_type=\"expand\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show(plot_classification_results(ragtruth_qa, \"RAGTruth Q&A\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    classification_report(\n",
    "        ragtruth_qa[\"is_faithful\"].map(faithful_display_name_map),\n",
    "        (ragtruth_qa[\"fdl_faithful_score\"] > 0.6)\n",
    "        .astype(int)\n",
    "        .map(faithful_display_name_map),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_latency_fig = plot_latency(ragtruth_qa, \"RAGTruth Q&A\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
