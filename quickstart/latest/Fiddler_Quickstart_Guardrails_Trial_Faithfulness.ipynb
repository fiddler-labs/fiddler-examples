{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fiddler Free Guardrails Quick Start Guide - Faithfulness\n",
    "\n",
    "## Goal\n",
    "\n",
    "This guide demonstrates how to use Fiddler Guardrails to detect and measure the faithfulness of a Large Language Model's response to a given context.\n",
    "\n",
    "## About Fiddler Guardrails\n",
    "\n",
    "Fiddler Guardrails provides enterprise-grade protection against critical LLM risks in production environments. This solution actively moderates and mitigates harmful content in both prompts and responses, including hallucinations, toxicity, safety violations, prompt injection attacks, and jailbreaking attempts. The solution is powered by proprietary, fine-tuned, task-specific Fiddler Trust Models, specifically engineered for real-time content analysis.\n",
    "\n",
    "## About Fiddler\n",
    "\n",
    "Fiddler is the all-in-one AI Observability and Security platform for responsible AI. Monitoring and analytics capabilities provide a common language, centralized controls, and actionable insights to operationalize production ML models, GenAI, AI agents, and LLM applications with trust. An integral part of the platform, the Fiddler Trust Service provides quality and moderation controls for LLM applications. Powered by cost-effective, task-specific, and scalable Fiddler-developed trust models — including cloud and VPC deployments for secure environments — it delivers the fastest guardrails in the industry. Fortune 500 organizations utilize Fiddler to scale LLM and ML deployments, delivering high-performance AI, reducing costs, and ensuring responsible governance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fiddler Free Guardrails\n",
    "\n",
    "* [Sign Up for an API Key](https://docs.fiddler.ai/tutorials-and-quick-starts/fiddler-guardrails-free-trial/guardrails-quick-start-guide) (needed to run this notebook)\n",
    "* [Guardrails Documentation](https://docs.fiddler.ai/technical-reference/fiddler-guardrails-free-trial-documentation)\n",
    "* [Guardrails FAQ](https://docs.fiddler.ai/tutorials-and-quick-starts/fiddler-guardrails-free-trial/guardrails-faq)\n",
    "\n",
    "---\n",
    "\n",
    "## Faithfulness Model\n",
    "\n",
    "This Fiddler Trust Model detects hallucinations by evaluating the accuracy and reliability of facts presented in AI-generated text responses in retrieval-augmented generation (RAG) contexts.\n",
    "\n",
    "The model requires two inputs:\n",
    "\n",
    "1. Response: the text generated by your generative application\n",
    "1. Context Documents: the reference text that the application response must remain faithful to\n",
    "\n",
    "The output is a single score (float) representing the factual consistency between the response and the provided context.\n",
    "\n",
    "---\n",
    "\n",
    "## Notebook Summary\n",
    "\n",
    "In this notebook we load a subset of RAGTruth Q&A tasks, a hallucination corpus within a RAG setting. We send these examples to the FTL Response Faithfulness Guardrail. We report on classification metrics and latency.\n",
    "\n",
    "This notebook should complete within 1 minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q matplotlib numpy pandas requests scikit-learn\n",
    "\n",
    "import time\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from sklearn.metrics import (\n",
    "    PrecisionRecallDisplay,\n",
    "    RocCurveDisplay,\n",
    "    classification_report,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIDDLER_GUARDRAILS_TOKEN = \"\"\n",
    "\n",
    "GUARDRAILS_BASE_URL = \"https://guardrails.cloud.fiddler.ai/v3/guardrails\"\n",
    "\n",
    "FAITHFULNESS_GUARDRAILS_URL = f\"{GUARDRAILS_BASE_URL}/ftl-response-faithfulness\"\n",
    "\n",
    "GUARDRAILS_HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {FIDDLER_GUARDRAILS_TOKEN}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "assert (\n",
    "    FIDDLER_GUARDRAILS_TOKEN != \"\"\n",
    "), \"Set :FIDDLER_GUARDRAILS_TOKEN above; sign-up at https://docs.fiddler.ai/tutorials-and-quick-starts/fiddler-guardrails-free-trial/guardrails-quick-start-guide\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load RAGTruth QA Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Download and Sample\n",
    "\n",
    "Download from GitHub and set columns we'll use for looking at performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ragtruth_qa_from_github(sample_size: int = None) -> pd.DataFrame:\n",
    "    \"\"\"Read RAGTruth from GitHub and filter to the Q&A examples in the test set.\n",
    "\n",
    "    Args:\n",
    "        sample_size (int, optional): How many records to return. If specified then a balanced sample will returned.\n",
    "            Defaults to None, which means all records will be returned\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: RAGTruth dataset filtered to Q&A examples in the test set, optionally downsampled.\n",
    "    \"\"\"\n",
    "    ragtruth_source_info = pd.read_json(\n",
    "        \"https://github.com/ParticleMedia/RAGTruth/raw/refs/heads/main/dataset/source_info.jsonl\",\n",
    "        lines=True,\n",
    "    )\n",
    "    ragtruth_response = pd.read_json(\n",
    "        \"https://github.com/ParticleMedia/RAGTruth/raw/refs/heads/main/dataset/response.jsonl\",\n",
    "        lines=True,\n",
    "    )\n",
    "    df = pd.merge(\n",
    "        ragtruth_source_info,\n",
    "        ragtruth_response,\n",
    "        left_on=\"source_id\",\n",
    "        right_on=\"source_id\",\n",
    "    )\n",
    "    df_qa = df[\n",
    "        (df[\"task_type\"] == \"QA\") & (df[\"split\"] == \"test\") & (df[\"quality\"] == \"good\")\n",
    "    ]\n",
    "\n",
    "    if sample_size is None:\n",
    "        return df_qa\n",
    "    else:\n",
    "        df_qa_pos = df_qa[df_qa[\"labels\"].apply(len) == 0].sample(\n",
    "            sample_size // 2, random_state=19\n",
    "        )\n",
    "        df_qa_neg = df_qa[df_qa[\"labels\"].apply(len) != 0].sample(\n",
    "            sample_size // 2, random_state=19\n",
    "        )\n",
    "        return pd.concat([df_qa_pos, df_qa_neg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ragtruth_qa = read_ragtruth_qa_from_github(\n",
    "    sample_size=20\n",
    ")  # Note: the Fiddler Free Guardrails is limited to 70 requests in 1 hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Preview the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ragtruth_qa[\"n_input_words\"] = (\n",
    "    ragtruth_qa[\"source_info\"].astype(str).str.split().str.len()\n",
    "    + ragtruth_qa[\"response\"].astype(str).str.split().str.len()\n",
    ")  # a rough estimate of word count\n",
    "\n",
    "ragtruth_qa[\"is_faithful\"] = (ragtruth_qa[\"labels\"].apply(len) == 0).astype(int)\n",
    "display(ragtruth_qa[\"is_faithful\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    ragtruth_qa[[\"is_faithful\", \"source_info\", \"response\"]]\n",
    "    .sample(frac=1, random_state=19)\n",
    "    .head(4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Code for Guardrails and Evaluation Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_faithfulness_response(\n",
    "    context: str, response: str, sleep_seconds: float = 0.5\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Invoke the guardrail for Faithfulness, returning the faithfulness score and latency time\n",
    "\n",
    "    Args:\n",
    "        context (str): Guardrails context field\n",
    "        response (str): Guardrails response field\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]: Faithfulness score and latency in seconds.\n",
    "    \"\"\"\n",
    "    time.sleep(sleep_seconds)\n",
    "    data = {\n",
    "        \"context\": context,\n",
    "        \"response\": response,\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            FAITHFULNESS_GUARDRAILS_URL,\n",
    "            headers=GUARDRAILS_HEADERS,\n",
    "            json={\"data\": data},\n",
    "            timeout=30,\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None, response.elapsed.total_seconds()\n",
    "\n",
    "    return response.json()[\"fdl_faithful_score\"], response.elapsed.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faithful_display_name_map = {0: \"hallucination\", 1: \"faithful\"}\n",
    "pos_label = \"faithful\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_classification_results(df: pd.DataFrame, ds_name: str):\n",
    "    global faithful_display_name_map, pos_label\n",
    "\n",
    "    df_clean = df.dropna(subset=[\"fdl_faithful_score\"])\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    fig.suptitle(ds_name)\n",
    "    pr = PrecisionRecallDisplay.from_predictions(\n",
    "        df_clean[\"is_faithful\"].map(faithful_display_name_map),\n",
    "        y_pred=df_clean[\"fdl_faithful_score\"],\n",
    "        ax=axes[2],\n",
    "        pos_label=pos_label,\n",
    "    )\n",
    "    pr.ax_.set_title(f\"Precision-Recall Curve\")\n",
    "\n",
    "    roc = RocCurveDisplay.from_predictions(\n",
    "        df_clean[\"is_faithful\"].map(faithful_display_name_map),\n",
    "        y_pred=df_clean[\"fdl_faithful_score\"],\n",
    "        ax=axes[1],\n",
    "        plot_chance_level=True,\n",
    "        pos_label=pos_label,\n",
    "    )\n",
    "    roc.ax_.set_title(f\"ROC Curve\")\n",
    "\n",
    "    axes[0].hist(\n",
    "        df_clean[df_clean[\"is_faithful\"] == 1][\"fdl_faithful_score\"],\n",
    "        color=\"cornflowerblue\",\n",
    "        label=\"Faithful\",\n",
    "        bins=np.arange(0, 1.05, 0.1),\n",
    "    )\n",
    "    axes[0].hist(\n",
    "        df_clean[df_clean[\"is_faithful\"] == 0][\"fdl_faithful_score\"],\n",
    "        color=\"orange\",\n",
    "        label=\"Hallucination\",\n",
    "        histtype=\"step\",\n",
    "        linewidth=1.5,\n",
    "        bins=np.arange(0, 1.05, 0.1),\n",
    "    )\n",
    "    axes[0].set_xlabel(\"Score\")\n",
    "    axes[0].set_ylabel(\"Frequency\", axes=axes[0])\n",
    "    axes[0].set_title(f\"Faithfulness Scores\")\n",
    "    axes[0].legend()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latency(df: pd.DataFrame, ds_name: str):\n",
    "    df_clean = df.dropna(subset=[\"fdl_faithful_score\"])\n",
    "    p95_latency = int(1000 * df_clean[\"fdl_faithful_latency\"].quantile(0.95).round(3))\n",
    "    p50_words_per_sec = int(\n",
    "        (df_clean[\"n_input_words\"] / df_clean[\"fdl_faithful_latency\"]).median()\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.suptitle(f\"{ds_name} Latency\")\n",
    "\n",
    "    ax.hist(df_clean[\"fdl_faithful_latency\"])\n",
    "    ax.set_title(f\"p95={p95_latency}ms; median {p50_words_per_sec} words per second\")\n",
    "    ax.set_xlabel(\"Seconds\")\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Dataset Through Guardrails\n",
    "\n",
    "We will sleep between requests to avoid rate-limit restrictions on the free trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ragtruth_qa[[\"fdl_faithful_score\", \"fdl_faithful_latency\"]] = ragtruth_qa.apply(\n",
    "    lambda row: get_faithfulness_response(str(row[\"source_info\"]), row[\"response\"]),\n",
    "    axis=1,\n",
    "    result_type=\"expand\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show(plot_classification_results(ragtruth_qa, \"RAGTruth Q&A\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    classification_report(\n",
    "        ragtruth_qa[\"is_faithful\"].map(faithful_display_name_map),\n",
    "        (ragtruth_qa[\"fdl_faithful_score\"] > 0.6)\n",
    "        # This is a good default threshold but you can change to match your use-case\n",
    "        .astype(int).map(faithful_display_name_map),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_latency_fig = plot_latency(ragtruth_qa, \"RAGTruth Q&A\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
