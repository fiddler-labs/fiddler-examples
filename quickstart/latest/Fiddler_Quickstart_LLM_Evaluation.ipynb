{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fiddler LLM Evaluation Quick Start Guide\n",
    "\n",
    "Fiddler is the pioneer in enterprise AI Observability, offering a unified platform enabling all model stakeholders to monitor model performance and investigate the source of model degradation. Fiddler's AI Observability platform supports traditional ML models and Generative AI applications. Fiddler can assist teams during the evaluation phase of selecting LLM models before developing an application. This guide explains how to compare LLM outputs from different models, such as GPT3.5 and Claude, to help determine the most suitable model for your language model application.\n",
    "\n",
    "---\n",
    "\n",
    "You can start using Fiddler ***in minutes*** by following these 6 quick steps:\n",
    "\n",
    "1. Connect to Fiddler\n",
    "2. Create or Retrieve a Fiddler Project\n",
    "3. Load Data Samples\n",
    "4. Enable Specific Fiddler LLM Enrichments\n",
    "5. Provide Information About the LLM Project\n",
    "6. Publish Datasets for Model Comparison\n",
    "\n",
    "Get insights!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T19:02:01.749620Z",
     "start_time": "2023-07-27T19:01:56.285723Z"
    },
    "id": "Pg-BdRJ4w3LM"
   },
   "outputs": [],
   "source": [
    "%pip install -q fiddler-client\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import fiddler as fdl\n",
    "\n",
    "print(f\"Running Fiddler Python client version {fdl.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connect to Fiddler\n",
    "\n",
    "Before you can add information about your LLM datasets with Fiddler, you'll need to connect using the Fiddler Python client.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**We need a couple pieces of information to get started.**\n",
    "1. The URL you're using to connect to Fiddler\n",
    "2. Your authorization token\n",
    "\n",
    "Your authorization token can be found by navigating to the **Credentials** tab on the **Settings** page of your Fiddler environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T19:02:01.753366Z",
     "start_time": "2023-07-27T19:02:01.751323Z"
    },
    "id": "05hPBZHr1eBv"
   },
   "outputs": [],
   "source": [
    "URL = ''  # Make sure to include the full URL (including https:// e.g. 'https://your_company_name.fiddler.ai').\n",
    "TOKEN = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants for this example notebook, change as needed to create your own versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = 'quickstart_examples'  # If the project already exists, the notebook will create the model under the existing project.\n",
    "MODEL_NAME = 'fiddler_llm_evaluation'\n",
    "\n",
    "GPT_NAME = 'gpt3.5_dataset'\n",
    "CLAUDE_NAME = 'claude_dataset'\n",
    "\n",
    "# Sample data hosted on GitHub\n",
    "PATH_TO_SAMPLE_GPT_CSV = 'https://media.githubusercontent.com/media/fiddler-labs/fiddler-examples/main/quickstart/data/v3/chatbot_production_data.csv'\n",
    "PATH_TO_SAMPLE_CLAUDE_CSV "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now just run the following code block to connect to the Fiddler API!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T19:02:58.459534Z",
     "start_time": "2023-07-27T19:02:57.976152Z"
    },
    "id": "g6ONUHliBAsH"
   },
   "outputs": [],
   "source": [
    "fdl.init(url=URL, token=TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a Fiddler Project\n",
    "\n",
    "Once you connect, you can create a new project by specifying a unique project name for the name parameter with either the Project.create() or the Project.get_or_create() methods. If the project already exists, the get_or_create() method will instead return the existing project which is helpful when running this notebook multiple times and when using an existing project to house Fiddler examples. \n",
    "\n",
    "*Note: get_or_create() requires Fiddler Python client 3.7+.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T18:54:19.978019Z",
     "start_time": "2023-07-27T18:54:19.874012Z"
    },
    "id": "raH7eU2Koaai",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "project = fdl.Project(name=PROJECT_NAME).get_or_create()\n",
    "\n",
    "# Check if the project has an ID to distinguish new vs existing\n",
    "print(f'{\"Created new\" if project.id is None else \"Retrieved existing\"} project with id = {project.id} and name = {project.name}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Dataset Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_df = pd.read_csv(PATH_TO_SAMPLE_GPT_CSV)\n",
    "claude_df = pd.read_csv(PATH_TO_SAMPLE_CLAUDE_CSV)\n",
    "\n",
    "gpt_df\n",
    "claude_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Enable Fiddler LLM Enrichments\n",
    "\n",
    "After picking a sample of our chatbot's prompts and responses, we can request that Fiddler execute a series of enrichment services that can \"score\" our prompts and responses for a variety of insights.  These enrichment services can detect AI safety issues like PII leakage, hallucinations, toxicity, and more.  We can also opt-in for enrichment services like embedding generation which will allow us to track prompt and response outliers and drift. A full description of these enrichments can be found [here](https://docs.fiddler.ai/platform-guide/llm-monitoring/enrichments-private-preview).\n",
    "\n",
    "---\n",
    "Define a list of Fiddler AI backend enrichments for various aspects of the model's input and output, including text embeddings, sentiment analysis, and PII detection. Each enrichment is represented by an appropriate Fiddler API enrichment object, such as TextEmbedding or Enrichment, with associated configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kJmCV_40oaan"
   },
   "outputs": [],
   "source": [
    "fiddler_backend_enrichments = [\n",
    "    # Generate text embeddings for the prompt (question) column\n",
    "    fdl.TextEmbedding(\n",
    "        name='Prompt TextEmbedding',\n",
    "        source_column='question',\n",
    "        column='Enrichment Prompt Embedding',\n",
    "        n_tags=10,\n",
    "    ),\n",
    "    # Generate text embeddings for the response column\n",
    "    fdl.TextEmbedding(\n",
    "        name='Response TextEmbedding',\n",
    "        source_column='response',\n",
    "        column='Enrichment Response Embedding',\n",
    "        n_tags=10,\n",
    "    ),\n",
    "    # Generate text embeddings for the source documents (rag documents) column\n",
    "    fdl.TextEmbedding(\n",
    "        name='Source Docs TextEmbedding',\n",
    "        source_column='source_docs',\n",
    "        column='Enrichment Source Docs Embedding',\n",
    "        n_tags=10,\n",
    "    ),\n",
    "    # Enrichment to assess response faithfulness using source docs and the response\n",
    "    fdl.Enrichment(\n",
    "        name='Faithfulness',\n",
    "        enrichment='ftl_response_faithfulness',\n",
    "        columns=['source_docs', 'response'],\n",
    "        config={'context_field': 'source_docs', 'response_field': 'response'},\n",
    "    ),\n",
    "    # Perform sentiment analysis on the question and response columns\n",
    "    fdl.Enrichment(\n",
    "        name='Enrichment QA Sentiment',\n",
    "        enrichment='sentiment',\n",
    "        columns=['question', 'response'],\n",
    "    ),\n",
    "    # Detect personally identifiable information (PII) in the question column\n",
    "    fdl.Enrichment(\n",
    "        name='Rag PII', enrichment='pii', columns=['question'], allow_list=['fiddler']\n",
    "    ),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.  Provide Information About the LLM Project\n",
    "\n",
    "Now it's time to onboard information about our LLM dataset to Fiddler.  We do this by defining a `ModelSpec` object.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "The `ModelSpec` object will contain some **information about how your LLM datasets are structured**.\n",
    "  \n",
    "*Just include:*\n",
    "1. The **input/output** columns.  These are just the raw inputs and outputs tracked in our LLM dataset.\n",
    "2. Any **metadata** columns. Make sure to include the 'model' column we generated earlier. \n",
    "3. The **custom features** which contain the configuration of the enrichments we opted for.\n",
    "\n",
    "We'll also want to set the **task** to LLM, since these datasets are generated from LLMs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spec = fdl.ModelSpec(\n",
    "    inputs=['question', 'response', 'source_docs'],\n",
    "    metadata=['session_id', 'comment', 'timestamp', 'feedback', 'model'],\n",
    "    custom_features=fiddler_backend_enrichments,\n",
    ")\n",
    "\n",
    "model_task = fdl.ModelTask.LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set this up in Fiddler by configuring a Model object to represent your LLM evaluation project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_project = fdl.Model.from_data(\n",
    "    source=gpt_df,\n",
    "    name=MODEL_NAME,\n",
    "    project_id=project.id,\n",
    "    spec=model_spec,\n",
    "    task=model_task,\n",
    "    max_cardinality=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now call the create method to create it in Fiddler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_project.create()\n",
    "print(\n",
    "    f'New model created with id = {llm_.id} and name = {llm_project.name}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Publish Data for Comparison\n",
    "\n",
    "Information about the LLM datasets is now onboarded to Fiddler. It's time to actually start adding the data itself to the preproduction environment for comparison!\n",
    "\n",
    "  \n",
    "Let's load in some sample data (prompts and responses) from our GPT and Claude datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publish_job_gpt = llm_project.publish(\n",
    "    source=gpt_df,\n",
    "    environment=fdl.EnvType.PRE_PRODUCTION,\n",
    "    dataset_name=GPT_NAME,\n",
    ")\n",
    "\n",
    "# Print the Job ID for tracking\n",
    "print(f'Initiated pre-production environment data upload with Job ID = {publish_job_gpt.id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, load the second dataset for comparison with the first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0HA3zidgoaao"
   },
   "outputs": [],
   "source": [
    "publish_job_claude = llm_project.publish(\n",
    "    source=claude_df,\n",
    "    environment=fdl.EnvType.PRE_PRODUCTION,\n",
    "    dataset_name=CLAUDE_NAME,\n",
    ")\n",
    "\n",
    "# Print the Job ID for tracking\n",
    "print(f'Initiated pre-production environment data upload with Job ID = {publish_job_claude.id}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get insights\n",
    "\n",
    "**You're all done!**\n",
    "\n",
    "You can now head to your Fiddler environment and start getting enhanced insights into the multiple datasets you are evaluating.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What's Next?**\n",
    "\n",
    "Try the [ML Monitoring - Quick Start Guide](https://docs.fiddler.ai/quickstart-notebooks/quick-start)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Questions?**  \n",
    "  \n",
    "Check out [our docs](https://docs.fiddler.ai/) for a more detailed explanation of what Fiddler has to offer.\n",
    "\n",
    "Join our [community Slack](http://fiddler-community.slack.com/) to ask any questions!\n",
    "\n",
    "If you're still looking for answers, fill out a ticket on [our support page](https://fiddlerlabs.zendesk.com/) and we'll get back to you shortly."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
