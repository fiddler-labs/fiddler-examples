{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8accbd85",
   "metadata": {},
   "source": [
    "# Fiddler LLM-as-a-Judge Quick Start Guide - Prompt Specs\n",
    "\n",
    "## Goal\n",
    "\n",
    "This guide demonstrates how to create a custom LLM-as-a-Judge eval; test it on a small amount of data; and then utilize it in the Fiddler platform.\n",
    "\n",
    "## About Fiddler\n",
    "\n",
    "Fiddler is the all-in-one AI Observability and Security platform for responsible AI. Monitoring and analytics capabilities provide a common language, centralized controls, and actionable insights to operationalize production ML models, GenAI, AI agents, and LLM applications with trust. An integral part of the platform, the Fiddler Trust Service provides quality and moderation controls for LLM applications. Powered by cost-effective, task-specific, and scalable Fiddler-developed trust models — including cloud and VPC deployments for secure environments — it delivers the fastest guardrails in the industry. Fortune 500 organizations utilize Fiddler to scale LLM and ML deployments, delivering high-performance AI, reducing costs, and ensuring responsible governance.\n",
    "\n",
    "---\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "You can start using Fiddler's LLM-as-a-Judge ***in minutes*** by following these quick steps.\n",
    "1. Load a Data Sample. \n",
    "1. Create and validate a Prompt Spec.\n",
    "1. Measure performance in the Evaluations Playground.\n",
    "1. Add field descriptions to improve accuracy.\n",
    "1. Create a corresponding GenAI enrichment in our Fiddler Project.\n",
    "1. Publish Production Events.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f610e348",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q fiddler-client # only needed if creating an enrichment in the Fiddler platform\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import fiddler as fdl\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2d277c",
   "metadata": {},
   "source": [
    "## Setup Your Environment\n",
    "\n",
    "Please set `FIDDLER_TOKEN` and `FIDDLER_BASE_URL` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6cbbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIDDLER_TOKEN = \"\"\n",
    "FIDDLER_BASE_URL = \"https://my_company.fiddler.ai\"  # Make sure to include the full URL (including https:// e.g. 'https://your_company_name.fiddler.ai').\n",
    "\n",
    "PROMPT_SPEC_URL = f\"{FIDDLER_BASE_URL}/v3/llm-as-a-judge/prompt-spec\"\n",
    "\n",
    "FIDDLER_HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {FIDDLER_TOKEN}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "assert FIDDLER_TOKEN != \"\", \"Please set your Fiddler API token\"\n",
    "assert FIDDLER_BASE_URL != \"https://my_company.fiddler.ai\", (\n",
    "    \"Please set your Fiddler API URL\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1abea62",
   "metadata": {},
   "source": [
    "## 1. Download and Sample Data\n",
    "\n",
    "We'll use [AG News set](https://huggingface.co/datasets/fancyzhx/ag_news). It has the text of a news summary and a categorical `label` column for corresponding topic. We'll use Fiddler's LLM-as-a-Judge solution to predict this label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38a3505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data, limiting to 20 randomly selected rows\n",
    "df_ag_news = pd.read_parquet(\n",
    "    \"hf://datasets/fancyzhx/ag_news/data/test-00000-of-00001.parquet\"\n",
    ").sample(20, random_state=25)\n",
    "\n",
    "# Create a new column that maps the label index to the topic name\n",
    "df_ag_news[\"original_topic\"] = df_ag_news[\"label\"].map(\n",
    "    {0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Sci/Tech\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5759df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the count of each unique topic\n",
    "df_ag_news[\"original_topic\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5c134a",
   "metadata": {},
   "source": [
    "## 2. Make a Simple Prompt Spec\n",
    "\n",
    "The Prompt Spec is a simple definition of the input and output fields for the LLM-as-a-Judge task. Under the hood we will turn this simple specification into an appropriate prompt.\n",
    "\n",
    "For our Prompt Spec we define a single input field and two output fields:\n",
    "1. Input `news_summary`: note that this is a more descriptive name than original column name `text`. Descriptive name can improve model performance.\n",
    "1. Output `topic`: we expect this will match the given `label`\n",
    "1. Output `reasoning`: this is a free-text field in which the model will include why it chose the `topic`. Including this field will cause longer execution times, but will help us during prompt-tuning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1e5f9a",
   "metadata": {},
   "source": [
    "### 2.1 Define and Validate the Prompt Spec \n",
    "\n",
    "Here we use the `validate` endpoint to check that our spec is valid. You could skip directly to `predict` in Section 2.2, as that action performs a validation check as well.\n",
    "\n",
    "> **Note**: by setting `choices` on the output field, we restrict the prediction to be one of these values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677c6ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_spec_basic = {\n",
    "    \"input_fields\": {\"news_summary\": {\"type\": \"string\"}},\n",
    "    \"output_fields\": {\n",
    "        \"topic\": {\n",
    "            \"type\": \"string\",\n",
    "            \"choices\": df_ag_news[\"original_topic\"].unique().tolist(),\n",
    "        },\n",
    "        \"reasoning\": {\"type\": \"string\"},\n",
    "    },\n",
    "}\n",
    "print(json.dumps(prompt_spec_basic, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d306b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_response = requests.post(\n",
    "    f\"{PROMPT_SPEC_URL}/validate\",\n",
    "    headers=FIDDLER_HEADERS,\n",
    "    json={\"prompt_spec\": prompt_spec_basic},\n",
    ")\n",
    "validate_response.raise_for_status()\n",
    "print(\"Status Code:\", validate_response.status_code)\n",
    "print(json.dumps(validate_response.json(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b42ca0c",
   "metadata": {},
   "source": [
    "### 2.2 Test with Ad-Hoc Data\n",
    "\n",
    "We send in one request to the `predict` endpoint. In the next section we'll send in our entire dataframe. \n",
    "\n",
    "> **Note**: This could take 10-15 minutes to run the first time, while the backend spins up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279eb08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(prompt_spec, input_data):\n",
    "    predict_response = requests.post(\n",
    "        f\"{PROMPT_SPEC_URL}/predict\",\n",
    "        headers=FIDDLER_HEADERS,\n",
    "        json={\"prompt_spec\": prompt_spec, \"input_data\": input_data},\n",
    "    )\n",
    "    if predict_response.status_code != 200:\n",
    "        print(f\"Error ({predict_response.status_code}): {predict_response.text}\")\n",
    "        return {\"topic\": None, \"reasoning\": None}\n",
    "    return predict_response.json()[\"prediction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc80698",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    json.dumps(\n",
    "        get_prediction(\n",
    "            prompt_spec_basic, {\"news_summary\": \"Wimbledon 2025 is under way!\"}\n",
    "        ),\n",
    "        indent=2,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330b05b7",
   "metadata": {},
   "source": [
    "### 2.3 Test with a DataFrame\n",
    "\n",
    "Now that you're familiar with the `predict` action, let's use it to evaluate our dataframe. This will take about 30 seconds to run.\n",
    "\n",
    "> **Note**: This endpoint is meant for evaluation purposes only and should not be used on production loads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57031624",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ag_news[[\"topic\", \"reasoning\"]] = df_ag_news.apply(\n",
    "    lambda row: get_prediction(prompt_spec_basic, {\"news_summary\": row[\"text\"]}),\n",
    "    axis=1,\n",
    "    result_type=\"expand\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69aa2d4",
   "metadata": {},
   "source": [
    "### 2.4 Inspect Results\n",
    "\n",
    "We see that several `Sci/Tech` articles were misclassified as `World`. The `reasoning` field helps identify trends. We'll use this to update our prompt spec in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff78f6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (df_ag_news[\"original_topic\"] == df_ag_news[\"topic\"]).mean()\n",
    "print(f\"Accuracy: {accuracy:.0%}\")\n",
    "\n",
    "df_ag_news.value_counts(subset=[\"original_topic\", \"topic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3245c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in df_ag_news[\n",
    "    (df_ag_news[\"original_topic\"] == \"Sci/Tech\") & (df_ag_news[\"topic\"] != \"Sci/Tech\")\n",
    "][\"reasoning\"]:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4664b50d",
   "metadata": {},
   "source": [
    "## 3. Make a Richer Prompt Spec\n",
    "\n",
    "In Section 2, we noted that descriptive field names can help improve model performance. You can also add a task instruction and field descriptions. Here, we will add a description to `topic` to help with classifying `Sci/Tech` articles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355d5d8c",
   "metadata": {},
   "source": [
    "### 3.1 Update the spec with a label \"hint\"\n",
    "\n",
    "We will re-run one of the `Sci/Tech` entries that was originally classified as `Business`. We see it now gets our desired label using the new prompt spec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78c6b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_spec_rich = {\n",
    "    \"instruction\": \"Determine the topic of the given news summary.\",\n",
    "    \"input_fields\": {\n",
    "        \"news_summary\": {\n",
    "            \"type\": \"string\",\n",
    "        }\n",
    "    },\n",
    "    \"output_fields\": {\n",
    "        \"topic\": {\n",
    "            \"type\": \"string\",\n",
    "            \"choices\": df_ag_news[\"original_topic\"].unique().tolist(),\n",
    "            \"description\": \"\"\"Use topic 'Sci/Tech' if the news summary is about a company or business in the tech industry, or if the news summary is about a scientific discovery or research, including health and medicine.\n",
    "            Use topic 'Sports' if the news summary is about a sports event or athlete.\n",
    "            Use topic 'Business' if the news summary is about a company or industry outside of science, technology, or sports.\n",
    "            Use topic 'World' if the news summary is about a global event or issue.\n",
    "            \"\"\",\n",
    "        },\n",
    "        \"reasoning\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The reasoning behind the predicted topic.\",\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a45277",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    json.dumps(\n",
    "        get_prediction(prompt_spec_rich, {\"news_summary\": df_ag_news.loc[267, \"text\"]}),\n",
    "        indent=2,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1ab2b7",
   "metadata": {},
   "source": [
    "### 3.2 Re-evaluate with the new Prompt\n",
    "\n",
    "We see that accuracy has improved over our initial results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ebe94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ag_news[[\"topic\", \"reasoning\"]] = df_ag_news.apply(\n",
    "    lambda row: get_prediction(prompt_spec_rich, {\"news_summary\": row[\"text\"]}),\n",
    "    axis=1,\n",
    "    result_type=\"expand\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228e5cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (df_ag_news[\"original_topic\"] == df_ag_news[\"topic\"]).mean()\n",
    "print(f\"Accuracy: {accuracy:.0%}\")\n",
    "\n",
    "df_ag_news.value_counts(subset=[\"original_topic\", \"topic\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dbb994",
   "metadata": {},
   "source": [
    "## 4. Create a Fiddler GenAI Enrichment\n",
    "\n",
    "Use the Fiddler client to create an Enrichment using our rich prompt spec. We'll then publish data to the platform.\n",
    "\n",
    "> **Note:** you can go back and remove the `reasoning` field if you do not want to include it in production monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec78ef56",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdl.init(url=FIDDLER_BASE_URL, token=FIDDLER_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f67c4e3",
   "metadata": {},
   "source": [
    "### 4.1 Create a Fiddler Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd31a3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"quickstart_examples\"  # If the project already exists, the notebook will create the model under the existing project.\n",
    "MODEL_NAME = \"fiddler_news_classifier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e96b9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "project = fdl.Project.get_or_create(name=PROJECT_NAME)\n",
    "\n",
    "print(f\"Using project with id = {project.id} and name = {project.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29afb4ed",
   "metadata": {},
   "source": [
    "### 4.2 Manipulate DataFrame\n",
    "\n",
    "Recall we used `news_summary` in our prompt. Let's make our dataframe match this and add some metadata.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a27e9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "platform_df = df_ag_news.rename(columns={\"text\": \"news_summary\"})\n",
    "platform_df[\"id\"] = platform_df.index\n",
    "platform_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e3c245",
   "metadata": {},
   "source": [
    "### 4.3 Add the Prediction as a Fiddler GenAI Enrichment\n",
    "\n",
    "* `name` will be used as part of the generated column name; set it to something meaningful for your use case.\n",
    "* `enrichment` must always be `llm_as_a_judge`.\n",
    "* `columns` matches all the input columns your prompt spec uses.\n",
    "* `config` must set the prompt spec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894d9679",
   "metadata": {},
   "outputs": [],
   "source": [
    "fiddler_llm_enrichments = [\n",
    "    fdl.Enrichment(\n",
    "        name=\"news_topic\",\n",
    "        enrichment=\"llm_as_a_judge\",\n",
    "        columns=[\"news_summary\"],\n",
    "        config={\"prompt_spec\": prompt_spec_rich},\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f016ac27",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spec = fdl.ModelSpec(\n",
    "    inputs=[\"news_summary\"],\n",
    "    metadata=[\"id\", \"original_topic\"],\n",
    "    custom_features=fiddler_llm_enrichments,\n",
    ")\n",
    "llm_application = fdl.Model.from_data(\n",
    "    source=platform_df,\n",
    "    name=MODEL_NAME,\n",
    "    project_id=project.id,\n",
    "    spec=model_spec,\n",
    "    task=fdl.ModelTask.LLM,\n",
    "    max_cardinality=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbf96bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_application.create()\n",
    "print(\n",
    "    f\"New model created with id = {llm_application.id} and name = {llm_application.name}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912d16a4",
   "metadata": {},
   "source": [
    "### 4.4 Publish Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4200571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll use this later to download the data\n",
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14f9371",
   "metadata": {},
   "outputs": [],
   "source": [
    "production_publish_job = llm_application.publish(platform_df)\n",
    "\n",
    "print(f\"Initiated production data upload with Job ID = {production_publish_job.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d488546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait for the job to complete\n",
    "production_publish_job.wait(interval=20)\n",
    "\n",
    "if production_publish_job.status == \"SUCCESS\":\n",
    "    print(\"\\n\\nProduction publish job completed successfully\")\n",
    "else:\n",
    "    print(\"\\n\\nProduction publish job failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e29620",
   "metadata": {},
   "source": [
    "### 4.5 Download Processed Data\n",
    "\n",
    "Our prediction will add two columns: `FDL news_topic (topic)` and `FDL news_topic (reasoning)`. \n",
    "\n",
    "> **Note**: The column names follow the pattern: `FDL {enrichment name} ({prompt spec output column})`, using values as set in section 4.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1041d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_application.download_data(\n",
    "    output_dir=\"test_download\",\n",
    "    env_type=fdl.EnvType.PRODUCTION,\n",
    "    start_time=start_time,\n",
    "    end_time=datetime.now(),\n",
    "    columns=[\n",
    "        \"id\",\n",
    "        \"news_summary\",\n",
    "        \"original_topic\",\n",
    "        \"FDL news_topic (topic)\",\n",
    "        \"FDL news_topic (reasoning)\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2314ff52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the original data and the results of LLM-as-a-Judge\n",
    "fdl_data = pd.read_parquet(\"test_download/output.parquet\")\n",
    "fdl_data.sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3877e19",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- Explore the Fiddler UI to see your model and enrichments\n",
    "- Try different prompt specs with your own data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-base-py3-11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
