{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fiddler LangGraph SDK: Custom Instrumentation with Decorators and Context Managers\n",
    "\n",
    "This notebook demonstrates the **decorator-based** and **manual** instrumentation modes introduced in Fiddler LangGraph SDK v1.4.0. These modes give you fine-grained control over trace structure and metadata — ideal for custom application logic, non-LangGraph components, and advanced observability patterns.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Decorator basics** — instrument any function with `@trace()`\n",
    "2. **Span metadata** — use `get_current_span()` to add LLM-specific attributes\n",
    "3. **Span types** — `generation`, `tool`, `chain` with type-specific helper methods\n",
    "4. **Manual instrumentation** — `start_as_current_span()` and `start_span()` context managers\n",
    "5. **Combining approaches** — mix decorators and manual spans in one trace\n",
    "6. **Async support** — `@trace()` works with async functions automatically\n",
    "\n",
    "## When to Use Custom Instrumentation\n",
    "\n",
    "| Approach | Best for |\n",
    "|----------|----------|\n",
    "| **Auto-instrumentation** (`LangGraphInstrumentor`) | LangGraph agents — zero-code setup |\n",
    "| **Decorator** (`@trace()`) | Custom Python functions, non-framework code, quick setup |\n",
    "| **Manual** (`start_as_current_span()`) | Loops, conditional logic, dynamic span names, maximum control |\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Python 3.10+\n",
    "- OpenAI API key\n",
    "- Fiddler instance with API key and application ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install fiddler-langgraph openai python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Create a `.env` file in your working directory with your credentials:\n",
    "\n",
    "```text\n",
    "FIDDLER_URL=https://your-instance.fiddler.ai\n",
    "FIDDLER_APPLICATION_ID=your-genai-application-id\n",
    "FIDDLER_API_KEY=your-fiddler-access-token\n",
    "OPENAI_API_KEY=your-openai-api-key\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "required_vars = [\"FIDDLER_API_KEY\", \"FIDDLER_APPLICATION_ID\", \"FIDDLER_URL\", \"OPENAI_API_KEY\"]\n",
    "missing = [v for v in required_vars if not os.getenv(v)]\n",
    "\n",
    "if missing:\n",
    "    raise EnvironmentError(f\"Missing environment variables: {missing}\")\n",
    "\n",
    "print(f\"Fiddler URL: {os.getenv('FIDDLER_URL')}\")\n",
    "print(f\"Application ID: {os.getenv('FIDDLER_APPLICATION_ID')}\")\n",
    "print(\"All credentials configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Fiddler Client\n",
    "\n",
    "Creating a `FiddlerClient` automatically registers it as the **global singleton**. The `@trace()` decorator and `get_client()` use this singleton by default, so you only need to create the client once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiddler_langgraph import FiddlerClient\n",
    "\n",
    "fdl_client = FiddlerClient(\n",
    "    api_key=os.getenv(\"FIDDLER_API_KEY\"),\n",
    "    application_id=os.getenv(\"FIDDLER_APPLICATION_ID\"),\n",
    "    url=os.getenv(\"FIDDLER_URL\"),\n",
    "    console_tracer=True,  # Print spans to console for debugging\n",
    ")\n",
    "\n",
    "print(f\"FiddlerClient initialized. Console tracing enabled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI client for LLM calls throughout this notebook\n",
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Basic `@trace()` Decorator\n",
    "\n",
    "The `@trace()` decorator instruments any Python function — sync or async — with a single line. By default it captures the function's arguments as input and return value as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiddler_langgraph import trace\n",
    "\n",
    "\n",
    "@trace()\n",
    "def greet(name: str) -> str:\n",
    "    \"\"\"A simple function to demonstrate @trace() basics.\"\"\"\n",
    "    return f\"Hello, {name}! Welcome to Fiddler.\"\n",
    "\n",
    "\n",
    "# Call the function — a span is created automatically\n",
    "result = greet(\"Alice\")\n",
    "print(result)\n",
    "\n",
    "# The span captures:\n",
    "#   - name: \"greet\" (from the function name)\n",
    "#   - input: {\"name\": \"Alice\"}\n",
    "#   - output: \"Hello, Alice! Welcome to Fiddler.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlling Input/Output Capture\n",
    "\n",
    "Use `capture_input` and `capture_output` to control what gets recorded. This is useful when arguments contain sensitive data or when the return value is large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@trace(capture_input=False, capture_output=False)\n",
    "def process_sensitive_data(user_email: str, ssn: str) -> dict:\n",
    "    \"\"\"Process sensitive data without capturing PII in traces.\"\"\"\n",
    "    return {\"status\": \"processed\", \"user\": user_email}\n",
    "\n",
    "\n",
    "result = process_sensitive_data(\"alice@example.com\", \"123-45-6789\")\n",
    "print(result)\n",
    "# The span is created but input/output are not recorded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Span Names\n",
    "\n",
    "By default, the span name is the function name. Use the `name` parameter to override it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@trace(name=\"data_validation\")\n",
    "def validate(data: dict) -> bool:\n",
    "    \"\"\"Validate input data. Span will be named 'data_validation', not 'validate'.\"\"\"\n",
    "    return \"query\" in data and len(data[\"query\"]) > 0\n",
    "\n",
    "\n",
    "is_valid = validate({\"query\": \"What is the weather in Tokyo?\"})\n",
    "print(f\"Valid: {is_valid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Adding Metadata with `get_current_span()`\n",
    "\n",
    "Inside a `@trace()`-decorated function, call `get_current_span()` to access the active span and set additional attributes. This is how you add LLM-specific metadata like model name, token usage, and custom attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiddler_langgraph import get_current_span\n",
    "\n",
    "\n",
    "@trace(as_type=\"generation\", capture_input=False, capture_output=False)\n",
    "def ask_llm(question: str) -> str:\n",
    "    \"\"\"Call OpenAI and capture LLM metadata via get_current_span().\"\"\"\n",
    "    span = get_current_span(as_type=\"generation\")\n",
    "\n",
    "    # Set LLM metadata before the call\n",
    "    if span:\n",
    "        span.set_model(\"gpt-4o-mini\")\n",
    "        span.set_system(\"openai\")\n",
    "        span.set_user_prompt(question)\n",
    "\n",
    "    # Make the OpenAI API call\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": question}],\n",
    "        temperature=0.3,\n",
    "    )\n",
    "\n",
    "    answer = response.choices[0].message.content\n",
    "\n",
    "    # Set output metadata after the call\n",
    "    if span:\n",
    "        span.set_completion(answer)\n",
    "        if response.usage:\n",
    "            span.set_usage(\n",
    "                input_tokens=response.usage.prompt_tokens,\n",
    "                output_tokens=response.usage.completion_tokens,\n",
    "                total_tokens=response.usage.total_tokens,\n",
    "            )\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n",
    "answer = ask_llm(\"What is the capital of France?\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Attributes\n",
    "\n",
    "Use `set_attribute()` to attach any custom key-value pair to a span. Custom attributes appear as `fiddler.span.user.<key>` in Fiddler and can be used for filtering and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@trace(as_type=\"generation\", capture_input=False, capture_output=False)\n",
    "def ask_llm_with_context(question: str, department: str) -> str:\n",
    "    \"\"\"LLM call with custom business attributes.\"\"\"\n",
    "    span = get_current_span(as_type=\"generation\")\n",
    "\n",
    "    if span:\n",
    "        span.set_model(\"gpt-4o-mini\")\n",
    "        span.set_system(\"openai\")\n",
    "        span.set_user_prompt(question)\n",
    "        # Custom attributes for business context\n",
    "        span.set_attribute(\"department\", department)\n",
    "        span.set_attribute(\"priority\", \"high\")\n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": question}],\n",
    "        temperature=0.3,\n",
    "    )\n",
    "\n",
    "    answer = response.choices[0].message.content\n",
    "\n",
    "    if span:\n",
    "        span.set_completion(answer)\n",
    "        if response.usage:\n",
    "            span.set_usage(\n",
    "                input_tokens=response.usage.prompt_tokens,\n",
    "                output_tokens=response.usage.completion_tokens,\n",
    "            )\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n",
    "answer = ask_llm_with_context(\"Summarize the benefits of AI observability.\", department=\"engineering\")\n",
    "print(f\"Answer: {answer[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Span Types and Helper Methods\n",
    "\n",
    "The `as_type` parameter determines the span's semantic type and which helper methods are available:\n",
    "\n",
    "| Span Type | Wrapper Class | Purpose | Key Helpers |\n",
    "|-----------|--------------|---------|-------------|\n",
    "| `generation` | `FiddlerGeneration` | LLM calls | `set_model()`, `set_user_prompt()`, `set_completion()`, `set_usage()` |\n",
    "| `tool` | `FiddlerTool` | Tool/function executions | `set_tool_name()`, `set_tool_input()`, `set_tool_output()` |\n",
    "| `chain` | `FiddlerChain` | Workflow orchestration | `set_input()`, `set_output()` |\n",
    "| `span` | `FiddlerSpan` | General-purpose (default) | `set_input()`, `set_output()`, `set_attribute()` |\n",
    "\n",
    "All types share common helpers: `set_agent_name()`, `set_conversation_id()`, `set_attribute()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool Spans\n",
    "\n",
    "Use `as_type='tool'` for functions that perform tool-like operations (API calls, database queries, searches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated knowledge base\n",
    "HOTEL_KNOWLEDGE = {\n",
    "    \"Tokyo\": \"Mandarin Oriental Tokyo — luxury hotel in Nihonbashi with views of Mount Fuji.\",\n",
    "    \"Paris\": \"The Ritz Paris — historic hotel on Place Vendome with Michelin-starred dining.\",\n",
    "    \"London\": \"The Savoy — legendary hotel on the Strand, famous for afternoon tea.\",\n",
    "}\n",
    "\n",
    "\n",
    "@trace(as_type=\"tool\", name=\"search_hotel\")\n",
    "def search_hotel(city: str) -> str:\n",
    "    \"\"\"Search the knowledge base for hotel information.\"\"\"\n",
    "    span = get_current_span(as_type=\"tool\")\n",
    "    if span:\n",
    "        span.set_tool_name(\"search_hotel\")\n",
    "        span.set_tool_input({\"city\": city})\n",
    "\n",
    "    result = HOTEL_KNOWLEDGE.get(city, f\"No hotel information found for {city}.\")\n",
    "\n",
    "    if span:\n",
    "        span.set_tool_output(result)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "hotel_info = search_hotel(\"Tokyo\")\n",
    "print(f\"Hotel info: {hotel_info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain Spans with Nesting\n",
    "\n",
    "Use `as_type='chain'` for orchestration functions. When decorated functions call each other, the `@trace()` decorator automatically establishes parent-child relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "@trace(as_type=\"generation\", capture_input=False, capture_output=False, model=\"gpt-4o-mini\", system=\"openai\")\n",
    "def recommend_hotel(city: str, context: str) -> str:\n",
    "    \"\"\"Use LLM to generate a hotel recommendation based on search results.\"\"\"\n",
    "    span = get_current_span(as_type=\"generation\")\n",
    "\n",
    "    prompt = f\"Based on this hotel info: {context}\\nWrite a one-sentence recommendation for a traveler visiting {city}.\"\n",
    "\n",
    "    if span:\n",
    "        span.set_user_prompt(prompt)\n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.5,\n",
    "    )\n",
    "\n",
    "    answer = response.choices[0].message.content\n",
    "\n",
    "    if span:\n",
    "        span.set_completion(answer)\n",
    "        if response.usage:\n",
    "            span.set_usage(\n",
    "                input_tokens=response.usage.prompt_tokens,\n",
    "                output_tokens=response.usage.completion_tokens,\n",
    "            )\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n",
    "@trace(as_type=\"chain\", name=\"travel_advisor\")\n",
    "def travel_advisor(city: str) -> dict:\n",
    "    \"\"\"Orchestrate a hotel search and LLM recommendation.\n",
    "\n",
    "    This creates a trace with three spans:\n",
    "      travel_advisor (chain)\n",
    "        -> search_hotel (tool)\n",
    "        -> recommend_hotel (generation)\n",
    "    \"\"\"\n",
    "    span = get_current_span(as_type=\"chain\")\n",
    "    if span:\n",
    "        span.set_agent_name(\"travel_advisor\")\n",
    "        span.set_input({\"city\": city})\n",
    "\n",
    "    # Step 1: Search for hotel info (tool span)\n",
    "    hotel_info = search_hotel(city)\n",
    "\n",
    "    # Step 2: Generate recommendation (generation span)\n",
    "    recommendation = recommend_hotel(city, hotel_info)\n",
    "\n",
    "    result = {\"city\": city, \"hotel_info\": hotel_info, \"recommendation\": recommendation}\n",
    "\n",
    "    if span:\n",
    "        span.set_output(result)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Run the chain — observe the parent-child span hierarchy\n",
    "advice = travel_advisor(\"Paris\")\n",
    "print(f\"City: {advice['city']}\")\n",
    "print(f\"Hotel: {advice['hotel_info']}\")\n",
    "print(f\"Recommendation: {advice['recommendation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Manual Instrumentation with Context Managers\n",
    "\n",
    "Manual instrumentation gives you full control over span boundaries. Use it when you need to:\n",
    "- Instrument loops where each iteration should be a separate span\n",
    "- Create spans with dynamic names\n",
    "- Control exactly when a span starts and ends\n",
    "\n",
    "### `start_as_current_span()` — Recommended\n",
    "\n",
    "The context manager automatically ends the span and records exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "\n",
    "def multi_city_search(cities: list[str]) -> dict:\n",
    "    \"\"\"Search hotels in multiple cities using manual instrumentation.\n",
    "\n",
    "    Each city gets its own tool span, all nested under one chain span.\n",
    "    This pattern is ideal for loops where each iteration should be tracked.\n",
    "    \"\"\"\n",
    "    session_id = str(uuid.uuid4())\n",
    "    results = {}\n",
    "\n",
    "    with fdl_client.start_as_current_span(\"multi_city_search\", as_type=\"chain\") as chain:\n",
    "        chain.set_agent_name(\"hotel_search\")\n",
    "        chain.set_conversation_id(session_id)\n",
    "        chain.set_input({\"cities\": cities})\n",
    "\n",
    "        for city in cities:\n",
    "            # Each iteration creates a child tool span\n",
    "            with fdl_client.start_as_current_span(f\"search_{city.lower()}\", as_type=\"tool\") as tool:\n",
    "                tool.set_tool_name(\"search_hotel\")\n",
    "                tool.set_tool_input({\"city\": city})\n",
    "\n",
    "                info = HOTEL_KNOWLEDGE.get(city, f\"No information for {city}.\")\n",
    "                results[city] = info\n",
    "\n",
    "                tool.set_tool_output(info)\n",
    "\n",
    "        chain.set_output(results)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "results = multi_city_search([\"Tokyo\", \"Paris\", \"London\"])\n",
    "for city, info in results.items():\n",
    "    print(f\"{city}: {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `start_span()` — Explicit Control\n",
    "\n",
    "Use `start_span()` when you need to end the span at a specific point rather than at the end of a `with` block. You **must** call `span.end()` manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_search(city: str, include_recommendation: bool) -> dict:\n",
    "    \"\"\"Demonstrate start_span() for explicit lifecycle control.\"\"\"\n",
    "    # Start span manually\n",
    "    span = fdl_client.start_span(\"conditional_search\", as_type=\"chain\")\n",
    "    span.set_input({\"city\": city, \"include_recommendation\": include_recommendation})\n",
    "\n",
    "    result = {\"city\": city}\n",
    "\n",
    "    try:\n",
    "        # Always search for hotel info\n",
    "        with fdl_client.start_as_current_span(\"hotel_lookup\", as_type=\"tool\") as tool:\n",
    "            tool.set_tool_name(\"search_hotel\")\n",
    "            tool.set_tool_input({\"city\": city})\n",
    "            info = HOTEL_KNOWLEDGE.get(city, f\"No information for {city}.\")\n",
    "            result[\"hotel_info\"] = info\n",
    "            tool.set_tool_output(info)\n",
    "\n",
    "        # Conditionally add an LLM recommendation\n",
    "        if include_recommendation:\n",
    "            with fdl_client.start_as_current_span(\"generate_recommendation\", as_type=\"generation\") as gen:\n",
    "                gen.set_model(\"gpt-4o-mini\")\n",
    "                gen.set_system(\"openai\")\n",
    "                prompt = f\"In one sentence, why should someone visit {city}?\"\n",
    "                gen.set_user_prompt(prompt)\n",
    "\n",
    "                response = openai_client.chat.completions.create(\n",
    "                    model=\"gpt-4o-mini\",\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    temperature=0.5,\n",
    "                )\n",
    "                recommendation = response.choices[0].message.content\n",
    "                result[\"recommendation\"] = recommendation\n",
    "                gen.set_completion(recommendation)\n",
    "\n",
    "        span.set_output(result)\n",
    "    finally:\n",
    "        # Always end the span\n",
    "        span.end()\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# With recommendation\n",
    "result = conditional_search(\"London\", include_recommendation=True)\n",
    "print(f\"City: {result['city']}\")\n",
    "print(f\"Hotel: {result['hotel_info']}\")\n",
    "print(f\"Recommendation: {result.get('recommendation', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Combining Decorators and Manual Spans\n",
    "\n",
    "You can mix decorator and manual instrumentation in the same trace. Decorated functions automatically become children of any active manual span, and manual spans inside decorated functions become children of the decorator's span.\n",
    "\n",
    "This example builds a simple agent loop that:\n",
    "1. Uses `@trace()` for the LLM call and tool functions\n",
    "2. Uses manual spans for the orchestration loop (where iteration count is dynamic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool functions using decorators\n",
    "@trace(as_type=\"tool\", name=\"book_flight\")\n",
    "def book_flight(origin: str, destination: str) -> str:\n",
    "    \"\"\"Book a flight between two cities.\"\"\"\n",
    "    span = get_current_span(as_type=\"tool\")\n",
    "    if span:\n",
    "        span.set_tool_name(\"book_flight\")\n",
    "        span.set_tool_input({\"origin\": origin, \"destination\": destination})\n",
    "\n",
    "    result = f\"Flight booked: {origin} -> {destination}, confirmation #FL{uuid.uuid4().hex[:6].upper()}\"\n",
    "\n",
    "    if span:\n",
    "        span.set_tool_output(result)\n",
    "    return result\n",
    "\n",
    "\n",
    "# LLM call using decorator\n",
    "@trace(as_type=\"generation\", capture_input=False, capture_output=False, model=\"gpt-4o-mini\", system=\"openai\")\n",
    "def plan_trip(user_request: str) -> dict:\n",
    "    \"\"\"Ask the LLM to plan a trip and return structured output.\"\"\"\n",
    "    span = get_current_span(as_type=\"generation\")\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are a travel planner. Given a request, respond with a JSON object \"\n",
    "        \"containing: {\\\"action\\\": \\\"book_flight\\\" or \\\"search_hotel\\\" or \\\"done\\\", \"\n",
    "        \"\\\"origin\\\": \\\"city\\\", \\\"destination\\\": \\\"city\\\", \\\"message\\\": \\\"summary\\\"}. \"\n",
    "        \"Respond with ONLY the JSON object, no markdown.\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_request},\n",
    "    ]\n",
    "\n",
    "    if span:\n",
    "        span.set_user_prompt(user_request)\n",
    "        span.set_messages(messages)\n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "\n",
    "    content = response.choices[0].message.content\n",
    "\n",
    "    if span:\n",
    "        span.set_completion(content)\n",
    "        if response.usage:\n",
    "            span.set_usage(\n",
    "                input_tokens=response.usage.prompt_tokens,\n",
    "                output_tokens=response.usage.completion_tokens,\n",
    "            )\n",
    "\n",
    "    try:\n",
    "        return json.loads(content)\n",
    "    except json.JSONDecodeError:\n",
    "        return {\"action\": \"done\", \"message\": content}\n",
    "\n",
    "\n",
    "# Orchestration using manual spans (dynamic loop)\n",
    "def travel_agent(user_request: str) -> str:\n",
    "    \"\"\"Simple agent loop mixing decorated functions with manual spans.\"\"\"\n",
    "    session_id = str(uuid.uuid4())\n",
    "\n",
    "    with fdl_client.start_as_current_span(\"travel_agent\", as_type=\"chain\") as root:\n",
    "        root.set_agent_name(\"travel_agent\")\n",
    "        root.set_conversation_id(session_id)\n",
    "        root.set_input({\"request\": user_request})\n",
    "\n",
    "        # Agent loop — manual span for each iteration\n",
    "        max_iterations = 3\n",
    "        results = []\n",
    "\n",
    "        for i in range(max_iterations):\n",
    "            with fdl_client.start_as_current_span(f\"iteration_{i}\", as_type=\"chain\") as step:\n",
    "                step.set_attribute(\"iteration\", i)\n",
    "\n",
    "                # Decorated function called inside manual span — becomes a child\n",
    "                plan = plan_trip(user_request)\n",
    "                action = plan.get(\"action\", \"done\")\n",
    "\n",
    "                if action == \"book_flight\":\n",
    "                    result = book_flight(\n",
    "                        origin=plan.get(\"origin\", \"Unknown\"),\n",
    "                        destination=plan.get(\"destination\", \"Unknown\"),\n",
    "                    )\n",
    "                    results.append(result)\n",
    "                elif action == \"search_hotel\":\n",
    "                    result = search_hotel(plan.get(\"destination\", \"Unknown\"))\n",
    "                    results.append(result)\n",
    "                else:\n",
    "                    results.append(plan.get(\"message\", \"Done.\"))\n",
    "                    break\n",
    "\n",
    "                step.set_output({\"action\": action, \"result\": results[-1]})\n",
    "\n",
    "        final_output = \"\\n\".join(results)\n",
    "        root.set_output(final_output)\n",
    "\n",
    "    return final_output\n",
    "\n",
    "\n",
    "output = travel_agent(\"I need to fly from Boston to Tokyo\")\n",
    "print(f\"Agent output:\\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Async Support\n",
    "\n",
    "The `@trace()` decorator automatically detects async functions. No special syntax is needed — just decorate your `async def` as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "@trace(as_type=\"generation\", capture_input=False, capture_output=False, model=\"gpt-4o-mini\", system=\"openai\")\n",
    "async def async_ask_llm(question: str) -> str:\n",
    "    \"\"\"Async LLM call — @trace() handles async automatically.\"\"\"\n",
    "    span = get_current_span(as_type=\"generation\")\n",
    "    if span:\n",
    "        span.set_user_prompt(question)\n",
    "\n",
    "    # Using synchronous OpenAI client in async context for simplicity.\n",
    "    # In production, use the AsyncOpenAI client.\n",
    "    response = await asyncio.to_thread(\n",
    "        openai_client.chat.completions.create,\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": question}],\n",
    "        temperature=0.3,\n",
    "    )\n",
    "\n",
    "    answer = response.choices[0].message.content\n",
    "    if span:\n",
    "        span.set_completion(answer)\n",
    "        if response.usage:\n",
    "            span.set_usage(\n",
    "                input_tokens=response.usage.prompt_tokens,\n",
    "                output_tokens=response.usage.completion_tokens,\n",
    "            )\n",
    "    return answer\n",
    "\n",
    "\n",
    "@trace(as_type=\"chain\", name=\"parallel_questions\")\n",
    "async def ask_multiple(questions: list[str]) -> list[str]:\n",
    "    \"\"\"Run multiple LLM calls concurrently — each gets its own generation span.\"\"\"\n",
    "    span = get_current_span(as_type=\"chain\")\n",
    "    if span:\n",
    "        span.set_input({\"questions\": questions})\n",
    "\n",
    "    answers = await asyncio.gather(*[async_ask_llm(q) for q in questions])\n",
    "\n",
    "    if span:\n",
    "        span.set_output({\"answers\": list(answers)})\n",
    "    return list(answers)\n",
    "\n",
    "\n",
    "# Run async functions\n",
    "questions = [\n",
    "    \"What is the tallest building in Tokyo?\",\n",
    "    \"What is the most famous museum in Paris?\",\n",
    "    \"What is the best time to visit London?\",\n",
    "]\n",
    "answers = await ask_multiple(questions)\n",
    "\n",
    "for q, a in zip(questions, answers):\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"A: {a[:150]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Context Isolation and `is_fiddler_span()`\n",
    "\n",
    "Each `FiddlerClient` operates in its own isolated OpenTelemetry context. This means Fiddler traces never interfere with other OTel-instrumented libraries in your application.\n",
    "\n",
    "Use `is_fiddler_span()` to verify whether a span belongs to Fiddler's tracer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiddler_langgraph import get_client\n",
    "from fiddler_langgraph.core.utils import is_fiddler_span\n",
    "\n",
    "# Verify the global client singleton\n",
    "client = get_client()\n",
    "print(f\"Global client retrieved: {client is not None}\")\n",
    "print(f\"Same instance as fdl_client: {client is fdl_client}\")\n",
    "\n",
    "# Verify span ownership inside a trace\n",
    "with fdl_client.start_as_current_span(\"ownership_check\", as_type=\"span\") as span:\n",
    "    print(f\"Is Fiddler span: {is_fiddler_span(span)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cleanup\n",
    "\n",
    "Flush remaining spans and shut down the client. In production, `FiddlerClient` registers an `atexit` handler that does this automatically, but for notebooks it is best to call `shutdown()` explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdl_client.force_flush(timeout_millis=5000)\n",
    "fdl_client.shutdown()\n",
    "print(\"All spans flushed and client shut down.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook covered the custom instrumentation modes introduced in Fiddler LangGraph SDK v1.4.0:\n",
    "\n",
    "| Concept | What You Learned |\n",
    "|---------|------------------|\n",
    "| `@trace()` decorator | Instrument any function with one line; automatic I/O capture |\n",
    "| `get_current_span()` | Access the active span to set LLM metadata, token usage, and custom attributes |\n",
    "| Span types | `generation`, `tool`, `chain` with type-specific helper methods |\n",
    "| `start_as_current_span()` | Manual context manager with automatic lifecycle management |\n",
    "| `start_span()` | Explicit span control when you need to end spans conditionally |\n",
    "| Mixing approaches | Decorators and manual spans compose naturally in the same trace |\n",
    "| Async support | `@trace()` works with async functions automatically |\n",
    "| Context isolation | Fiddler traces never interfere with other OTel tracers |\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Auto-instrumentation**: Use `LangGraphInstrumentor` for zero-code LangGraph agent monitoring — see the [Quick Start Guide](https://docs.fiddler.ai/developers/quick-starts/langgraph-sdk-quick-start)\n",
    "- **Advanced patterns**: Production configuration, multi-agent systems, and performance optimization — see the [Advanced Guide](https://docs.fiddler.ai/developers/tutorials/llm-monitoring/langgraph-sdk-advanced)\n",
    "- **SDK API reference**: Full API documentation for all classes and functions — see the [SDK API Reference](https://docs.fiddler.ai/sdk-api/langgraph/)\n",
    "- **Integration guide**: Comprehensive overview of all three instrumentation approaches — see the [LangGraph SDK Integration](https://docs.fiddler.ai/integrations/agentic-ai/langgraph-sdk)\n",
    "\n",
    "---\n",
    "\n",
    "**Support:** [support@fiddler.ai](mailto:support@fiddler.ai) | [Fiddler Documentation](https://docs.fiddler.ai)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 4,
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
