{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# RAG Evaluations - Part 1: Evaluators\n",
    "\n",
    "This quickstart shows you how to evaluate RAG (Retrieval-Augmented Generation) applications using Fiddler's evaluators.\n",
    "\n",
    "## RAG Health: 4 Evaluators Working Together\n",
    "\n",
    "Fiddler provides 4 evaluators that together measure RAG health:\n",
    "\n",
    "| Evaluator | Question It Answers |\n",
    "|-----------|--------------------|\n",
    "| **Context Relevance** | Are the retrieved documents relevant to the query? |\n",
    "| **Faithfulness** | Is the response grounded in the context (no hallucinations)? |\n",
    "| **Answer Relevance** | Does the response address the user's question? |\n",
    "| **Coherence** | Is the response logically structured and easy to follow? |\n",
    "\n",
    "Each evaluator catches different problems. Used together, they give you a complete picture of RAG quality.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- A Fiddler account with API access\n",
    "- An LLM credential configured in **Settings > LLM Gateway**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "install-header",
   "metadata": {},
   "source": [
    "## 1. Install and Connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q ipywidgets fiddler-evals pandas\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from fiddler_evals import init\n",
    "from fiddler_evals.evaluators import (\n",
    "    AnswerRelevance,\n",
    "    Coherence,\n",
    "    ContextRelevance,\n",
    "    RAGFaithfulness,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fiddler connection\n",
    "URL = ''  # e.g., 'https://your-org.fiddler.ai'\n",
    "TOKEN = ''  # From Settings > Credentials\n",
    "\n",
    "# LLM Gateway (from Settings > LLM Gateway)\n",
    "LLM_CREDENTIAL_NAME = 'Fiddler Credential'  # Change as needed for 3rd party providers\n",
    "LLM_MODEL_NAME = 'fiddler/llama3.1-8b'\n",
    "\n",
    "# Connect to Fiddler\n",
    "init(url=URL, token=TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-header",
   "metadata": {},
   "source": [
    "## 3. Sample RAG Data\n",
    "\n",
    "We'll evaluate 5 RAG interactions, each demonstrating a different quality scenario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_data = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            # Good RAG - everything works correctly\n",
    "            'scenario': 'good_rag',\n",
    "            'user_query': 'What is the capital of France?',\n",
    "            'retrieved_documents': [\n",
    "                'Paris is the capital and largest city of France.',\n",
    "                'France is located in Western Europe.',\n",
    "            ],\n",
    "            'rag_response': 'The capital of France is Paris.',\n",
    "        },\n",
    "        {\n",
    "            # Irrelevant context - retrieved docs don't match the query\n",
    "            'scenario': 'irrelevant_context',\n",
    "            'user_query': 'How do I reset my password?',\n",
    "            'retrieved_documents': [\n",
    "                'To make pasta, boil water and add salt.',\n",
    "                'Italian cuisine features many pasta dishes.',\n",
    "            ],\n",
    "            'rag_response': 'To reset your password, go to the login page and click Forgot Password.',\n",
    "        },\n",
    "        {\n",
    "            # Hallucination - response includes facts not in the context\n",
    "            'scenario': 'hallucination',\n",
    "            'user_query': 'What are the business hours?',\n",
    "            'retrieved_documents': [\n",
    "                'We are open Monday-Thursday: 9AM to 5PM. Fridays we close at noon.',\n",
    "                'We are closed on federal holidays.',\n",
    "            ],\n",
    "            'rag_response': 'Our business hours are Monday through Friday, 9 AM to 5 PM.',\n",
    "        },\n",
    "        {\n",
    "            # Off-topic answer - response doesn't address the question\n",
    "            'scenario': 'off_topic_answer',\n",
    "            'user_query': 'What is your return policy?',\n",
    "            'retrieved_documents': [\n",
    "                'Returns are accepted within 30 days of purchase.',\n",
    "                'Items must be unused and in original packaging.',\n",
    "            ],\n",
    "            'rag_response': 'We offer free shipping on orders over $50. Delivery takes 3-5 business days.',\n",
    "        },\n",
    "        {\n",
    "            # Incoherent response - jumbled, illogical text\n",
    "            'scenario': 'incoherent_response',\n",
    "            'user_query': 'How do I contact support?',\n",
    "            'retrieved_documents': [\n",
    "                'Contact support at support@example.com.',\n",
    "                'Phone support is available at 1-800-555-0123.',\n",
    "            ],\n",
    "            'rag_response': 'Support contact the email. Phone 1-800 is yes. Help desk Monday purple elephant.',\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "rag_data[['scenario', 'user_query', 'rag_response']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-header",
   "metadata": {},
   "source": [
    "## 4. Run All Evaluators\n",
    "\n",
    "We'll run all 4 RAG Health evaluators on each test case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluators",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluators\n",
    "context_relevance = ContextRelevance(\n",
    "    model=LLM_MODEL_NAME, credential=LLM_CREDENTIAL_NAME\n",
    ")\n",
    "faithfulness = RAGFaithfulness(model=LLM_MODEL_NAME, credential=LLM_CREDENTIAL_NAME)\n",
    "answer_relevance = AnswerRelevance(model=LLM_MODEL_NAME, credential=LLM_CREDENTIAL_NAME)\n",
    "coherence = Coherence(model=LLM_MODEL_NAME, credential=LLM_CREDENTIAL_NAME)\n",
    "\n",
    "# Evaluate each row\n",
    "results = []\n",
    "for _, row in rag_data.iterrows():\n",
    "    print(f'Evaluating: {row[\"scenario\"]}...')\n",
    "\n",
    "    result = {'scenario': row['scenario']}\n",
    "\n",
    "    # Context Relevance - Are the retrieved docs relevant to the query?\n",
    "    cr = context_relevance.score(\n",
    "        user_query=row['user_query'],\n",
    "        retrieved_documents=row['retrieved_documents'],\n",
    "    )\n",
    "    result['context_relevance'] = cr.label\n",
    "    result['context_relevance_score'] = cr.value\n",
    "\n",
    "    # Faithfulness - Is the response grounded in the context?\n",
    "    f = faithfulness.score(\n",
    "        user_query=row['user_query'],\n",
    "        rag_response=row['rag_response'],\n",
    "        retrieved_documents=row['retrieved_documents'],\n",
    "    )\n",
    "    result['faithfulness'] = f.label\n",
    "    result['faithfulness_score'] = f.value\n",
    "\n",
    "    # Answer Relevance - Does the response address the question?\n",
    "    ar = answer_relevance.score(\n",
    "        user_query=row['user_query'],\n",
    "        rag_response=row['rag_response'],\n",
    "        retrieved_documents=row['retrieved_documents'],\n",
    "    )\n",
    "    result['answer_relevance'] = ar.label\n",
    "    result['answer_relevance_score'] = ar.value\n",
    "\n",
    "    # Coherence - Is the response well-structured?\n",
    "    c = coherence.score(\n",
    "        prompt=row['user_query'],\n",
    "        response=row['rag_response'],\n",
    "    )\n",
    "    result['coherence'] = c.label\n",
    "    result['coherence_score'] = c.value\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-header",
   "metadata": {},
   "source": [
    "## 5. View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c5eec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpretation",
   "metadata": {},
   "source": [
    "## 6. Interpreting the Results\n",
    "\n",
    "Each evaluator catches different problems:\n",
    "\n",
    "| Scenario | Problem | Evaluator That Catches It |\n",
    "|----------|---------|---------------------------|\n",
    "| `good_rag` | None - everything works | All scores high |\n",
    "| `irrelevant_context` | Retrieved docs don't match query | **Context Relevance** scores low |\n",
    "| `hallucination` | Response includes facts not in context | **Faithfulness** scores low |\n",
    "| `off_topic_answer` | Response doesn't address the question | **Answer Relevance** scores low |\n",
    "| `incoherent_response` | Response is jumbled and illogical | **Coherence** scores low |\n",
    "\n",
    "This is why using all 4 evaluators together gives you a complete picture of RAG health - each one catches a different failure mode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you understand the evaluators, see **[Part 2: Experiments](./Fiddler_Quickstart_RAG_Part2_Experiments.ipynb)** to:\n",
    "- Run structured evaluation experiments\n",
    "- Track results over time in Fiddler\n",
    "- Validate evaluators against golden labeled data\n",
    "\n",
    "**Resources:**\n",
    "- [Fiddler Evals Documentation](https://docs.fiddler.ai/evaluations/overview)\n",
    "- [Evaluator Reference](https://docs.fiddler.ai/evaluations/evaluators)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4ec017",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
