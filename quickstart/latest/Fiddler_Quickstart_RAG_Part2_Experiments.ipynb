{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# RAG Evaluations - Part 2: Experiments\n",
    "\n",
    "This quickstart shows you how to run structured RAG evaluation experiments and track results in Fiddler.\n",
    "\n",
    "**What you'll learn:**\n",
    "- Create a Dataset with labeled test cases\n",
    "- Run an Experiment with multiple evaluators\n",
    "- Validate evaluators against golden labels\n",
    "- View and analyze results in the Fiddler UI\n",
    "\n",
    "For details on individual evaluators, see **[Part 1: Evaluators](./Fiddler_Quickstart_RAG_Part1_Evaluators.ipynb)**.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- A Fiddler account with API access\n",
    "- An LLM credential configured in **Settings > LLM Gateway**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "install-header",
   "metadata": {},
   "source": [
    "## 1. Install and Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q fiddler-evals pandas ipywidgets\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from fiddler_evals import (\n",
    "    Application,\n",
    "    Dataset,\n",
    "    Project,\n",
    "    evaluate,\n",
    "    init,\n",
    ")\n",
    "from fiddler_evals.evaluators import (\n",
    "    AnswerRelevance,\n",
    "    Coherence,\n",
    "    ContextRelevance,\n",
    "    RAGFaithfulness,\n",
    ")\n",
    "from fiddler_evals.pydantic_models.experiment import ExperimentItemResult\n",
    "from fiddler_evals.pydantic_models.score import Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fiddler connection\n",
    "URL = ''  # e.g., 'https://your-org.fiddler.ai'\n",
    "TOKEN = ''  # From Settings > Credentials\n",
    "\n",
    "# LLM Gateway (from Settings > LLM Gateway)\n",
    "LLM_CREDENTIAL_NAME = 'Fiddler Credential'  # Change as needed for 3rd party providers\n",
    "LLM_MODEL_NAME = 'fiddler/llama3.1-8b'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connect-header",
   "metadata": {},
   "source": [
    "## 3. Connect and Create Resources\n",
    "\n",
    "Experiments are organized in a hierarchy: **Project > Application > Dataset > Experiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Fiddler\n",
    "init(url=URL, token=TOKEN)\n",
    "\n",
    "# Create project, application, and dataset\n",
    "project = Project.get_or_create(name='quickstart')\n",
    "application = Application.get_or_create(name='rag-evaluation', project_id=project.id)\n",
    "dataset = Dataset.get_or_create(name='rag-test-cases-1', application_id=application.id)\n",
    "\n",
    "print(f'Project: {project.name}')\n",
    "print(f'Application: {application.name}')\n",
    "print(f'Dataset: {dataset.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-header",
   "metadata": {},
   "source": [
    "## 4. Create RAG Test Data\n",
    "\n",
    "We'll create 5 test cases with `expected_quality` labels (good/bad) to validate our evaluators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_data = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            # Good RAG - everything works correctly\n",
    "            'scenario': 'good_rag',\n",
    "            'expected_quality': 'good',\n",
    "            'user_query': 'What is the capital of France?',\n",
    "            'retrieved_documents': [\n",
    "                'Paris is the capital and largest city of France.',\n",
    "                'France is located in Western Europe.',\n",
    "            ],\n",
    "            'rag_response': 'The capital of France is Paris.',\n",
    "        },\n",
    "        {\n",
    "            # Irrelevant context - retrieved docs don't match the query\n",
    "            'scenario': 'irrelevant_context',\n",
    "            'expected_quality': 'bad',\n",
    "            'user_query': 'How do I reset my password?',\n",
    "            'retrieved_documents': [\n",
    "                'To make pasta, boil water and add salt.',\n",
    "                'Italian cuisine features many pasta dishes.',\n",
    "            ],\n",
    "            'rag_response': 'To reset your password, go to the login page and click Forgot Password.',\n",
    "        },\n",
    "        {\n",
    "            # Hallucination - response includes facts not in the context\n",
    "            'scenario': 'hallucination',\n",
    "            'expected_quality': 'bad',\n",
    "            'user_query': 'What are the business hours?',\n",
    "            'retrieved_documents': [\n",
    "                'Our office is located at 123 Main Street.',\n",
    "                'We are closed on federal holidays.',\n",
    "            ],\n",
    "            'rag_response': 'Our business hours are Monday through Friday, 9 AM to 5 PM.',\n",
    "        },\n",
    "        {\n",
    "            # Off-topic answer - response doesn't address the question\n",
    "            'scenario': 'off_topic_answer',\n",
    "            'expected_quality': 'bad',\n",
    "            'user_query': 'What is your return policy?',\n",
    "            'retrieved_documents': [\n",
    "                'Returns are accepted within 30 days of purchase.',\n",
    "                'Items must be unused and in original packaging.',\n",
    "            ],\n",
    "            'rag_response': 'We offer free shipping on orders over $50. Delivery takes 3-5 business days.',\n",
    "        },\n",
    "        {\n",
    "            # Incoherent response - jumbled, illogical text\n",
    "            'scenario': 'incoherent_response',\n",
    "            'expected_quality': 'bad',\n",
    "            'user_query': 'How do I contact support?',\n",
    "            'retrieved_documents': [\n",
    "                'Contact support at support@example.com.',\n",
    "                'Phone support is available at 1-800-555-0123.',\n",
    "            ],\n",
    "            'rag_response': 'Support contact the email. Phone 1-800 is yes. Help desk Monday purple elephant.',\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "rag_data[['scenario', 'expected_quality', 'user_query', 'rag_response']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insert-header",
   "metadata": {},
   "source": [
    "## 5. Insert Data and Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if dataset already has items (avoid duplicates on re-run)\n",
    "existing_items = list(dataset.get_items())\n",
    "if not existing_items:\n",
    "    dataset.insert_from_pandas(\n",
    "        df=rag_data,\n",
    "        input_columns=['user_query', 'retrieved_documents', 'rag_response'],\n",
    "        expected_output_columns=['expected_quality'],\n",
    "        metadata_columns=['scenario'],\n",
    "    )\n",
    "    print(f'Inserted {len(rag_data)} test cases')\n",
    "else:\n",
    "    print(f'Dataset already has {len(existing_items)} items, skipping insert')\n",
    "    print(\n",
    "        'To start fresh, delete the dataset from the Fiddler UI or use a new dataset name'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define task function\n",
    "def rag_task(inputs: dict, extras: dict, metadata: dict) -> dict:\n",
    "    \"\"\"Return the pre-recorded RAG response for evaluation.\n",
    "\n",
    "    In production, replace this with your actual RAG pipeline call.\n",
    "    \"\"\"\n",
    "    return {'rag_response': inputs['rag_response']}\n",
    "\n",
    "\n",
    "# Configure evaluators (in order of RAG pipeline stages)\n",
    "evaluators = [\n",
    "    ContextRelevance(model=LLM_MODEL_NAME, credential=LLM_CREDENTIAL_NAME),\n",
    "    RAGFaithfulness(model=LLM_MODEL_NAME, credential=LLM_CREDENTIAL_NAME),\n",
    "    AnswerRelevance(model=LLM_MODEL_NAME, credential=LLM_CREDENTIAL_NAME),\n",
    "    Coherence(model=LLM_MODEL_NAME, credential=LLM_CREDENTIAL_NAME),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18ebc32",
   "metadata": {},
   "source": [
    "### Evaluator Input Parameters\n",
    "Each evaluator requires specific inputs. Here's what each one needs:\n",
    "| Evaluator | Required Inputs |\n",
    "|-----------|-----------------|\n",
    "| ContextRelevance | `user_query`, `retrieved_documents` |\n",
    "| RAGFaithfulness | `user_query`, `rag_response`, `retrieved_documents` |\n",
    "| AnswerRelevance | `user_query`, `rag_response`, `retrieved_documents` (optional) |\n",
    "| Coherence | `prompt`, `response` |\n",
    "\n",
    "\n",
    "**Tip:** You can inspect any evaluator's signature:\n",
    "```py\n",
    "import inspect\n",
    "\n",
    "for e in evaluators:\n",
    "    print(e.name, inspect.signature(e.score)) \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157ff98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map evaluator parameters to our data structure\n",
    "score_fn_kwargs_mapping = {\n",
    "    'user_query': lambda x: x['inputs']['user_query'],\n",
    "    'retrieved_documents': lambda x: x['inputs']['retrieved_documents'],\n",
    "    'rag_response': 'rag_response',\n",
    "    'prompt': lambda x: x['inputs']['user_query'],\n",
    "    'response': 'rag_response',\n",
    "}\n",
    "\n",
    "# Run the experiment\n",
    "result = evaluate(\n",
    "    dataset=dataset,\n",
    "    task=rag_task,\n",
    "    evaluators=evaluators,\n",
    "    score_fn_kwargs_mapping=score_fn_kwargs_mapping,\n",
    "    description='RAG quality evaluation with golden label validation',\n",
    ")\n",
    "\n",
    "print(f'Experiment complete: {result.experiment.name}')\n",
    "print(f'Evaluated {len(result.results)} test cases')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ui-header",
   "metadata": {},
   "source": [
    "## 6. Validate Evaluators Against Golden Labels\n",
    "\n",
    "Our test data includes `expected_quality` labels (good/bad). Let's check if the evaluators correctly identified the quality of each response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validate-compute",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_results = []\n",
    "for item_result in result.results:\n",
    "    expected = item_result.dataset_item.expected_outputs.get('expected_quality')\n",
    "\n",
    "    # Predict 'bad' if any evaluator flagged a problem\n",
    "    bad_labels = {'no', 'low', 'False'}\n",
    "    has_problem = any(s.label in bad_labels for s in item_result.scores)\n",
    "    predicted = 'bad' if has_problem else 'good'\n",
    "\n",
    "    validation_results.append(\n",
    "        ExperimentItemResult(\n",
    "            experiment_item=item_result.experiment_item,\n",
    "            dataset_item=item_result.dataset_item,\n",
    "            scores=[\n",
    "                Score(\n",
    "                    name='predicted_quality',\n",
    "                    evaluator_name='Validator',\n",
    "                    value=1.0 if predicted == 'good' else 0.0,\n",
    "                    label=predicted,\n",
    "                    reasoning=f'Expected: {expected}',\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Add validation scores to experiment\n",
    "result.experiment.add_results(validation_results)\n",
    "print('Validation scores added to experiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validate-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy: how often did evaluators correctly predict quality?\n",
    "correct = 0\n",
    "for v in validation_results:\n",
    "    expected = v.dataset_item.expected_outputs.get('expected_quality')\n",
    "    predicted = v.scores[0].label\n",
    "    if expected == predicted:\n",
    "        correct += 1\n",
    "total = len(validation_results)\n",
    "print(f'Evaluator Accuracy: {correct}/{total} ({100 * correct / total:.0f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "view-results-header",
   "metadata": {},
   "source": [
    "## 7. View Results\n",
    "\n",
    "Your experiment and validation scores are now in Fiddler.\n",
    "\n",
    "**In the Fiddler UI you can:**\n",
    "- View scores for each test case\n",
    "- Filter by scenario or score values\n",
    "- Compare multiple experiments side-by-side\n",
    "- Export results for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "view-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct link to experiment\n",
    "print(f'View in Fiddler: {URL}/evals/experiments/{result.experiment.id}')\n",
    "\n",
    "# Export to DataFrame for local analysis\\n\n",
    "rows = []\n",
    "for r in result.results:\n",
    "    # Get expected quality from dataset\\n\n",
    "    expected = r.dataset_item.expected_outputs.get('expected_quality')\n",
    "\n",
    "    # Get predicted from validation results (computed in Section 6)\\n\n",
    "    predicted = next(\n",
    "        (\n",
    "            v.scores[0].label\n",
    "            for v in validation_results\n",
    "            if v.dataset_item.id == r.dataset_item.id\n",
    "        ),\n",
    "        None,\n",
    "    )\n",
    "    row = {\n",
    "        'scenario': r.dataset_item.metadata.get('scenario'),\n",
    "        'expected': expected,\n",
    "        'predicted': predicted,\n",
    "    }\n",
    "    for score in r.scores:\n",
    "        row[score.evaluator_name] = score.value\n",
    "    rows.append(row)\n",
    "\n",
    "results_df = pd.DataFrame(rows)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- **Use your own labeled data**: Replace the sample data with your golden test cases\n",
    "- **Tune the threshold**: Adjust the score threshold based on your quality requirements\n",
    "- **Compare experiments**: Run multiple experiments to compare model versions or prompts\n",
    "- **Automate evaluations**: Integrate into CI/CD pipelines for continuous monitoring\n",
    "\n",
    "**Resources:**\n",
    "- [Part 1: Evaluators](./Fiddler_Quickstart_RAG_Part1_Evaluators.ipynb) - Details on individual evaluators\n",
    "- [Fiddler Evals Documentation](https://docs.fiddler.ai/evaluations/overview)\n",
    "- [Evaluator Reference](https://docs.fiddler.ai/evaluations/evaluators)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
